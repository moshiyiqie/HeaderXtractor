
Quantum Computing | A treatise
Prabhat Kumar
October 24, 1996
Abstract
Quantum computing has witnessed a surge of activity recently owing
to some very exciting discoveries on both the theoretical and practical
fronts. In this overview, we sketch an account of the developments in
this scientifically intriguing field, starting in the early 80's when the first
questions about the computability of quantum processes were raised,
and which led to the formal definitions of a Quantum Computer and a
Quantum Complexity Theory. Peter Shor's recent remarkable discovery of quantum algorithms to solve the problems of integer factoring
and discrete log computing, which are believed to be extremely hard to
solve efficiently on classical computers, is a compelling demonstration
of the suspected superiority of quantum computing over the classical
model that is in use today. We discuss one of his algorithms and the
implications it has for classical cryptography. We discuss some of the
latest work in this field which has brought us yet closer to achieving a
physical realization of a quantum computer. Whenever it happens, if it
happens, it would be yet another revolution in the field of computing,
and maybe the biggest one to date.
1 Birth of Quantum Computing

Within the Letter of the Law: open-textured planning
Kathryn E. Sanders
Department of Computer Science
University of Maryland
College Park, MD 20742
sanders@cs.umd.edu
Abstract
Most case-based reasoning systems have used a
single "best" or "most similar" case as the basis
for a solution. For many problems, however,
there is no single exact solution. Rather, there
is a range of acceptable answers. We use cases
not only as a basis for a solution, but also to
indicate the boundaries within which a solution
can be found. We solve problems by choosing
some point within those boundaries.
In this paper, I discuss this use of cases with
illustrations from chiron, a system I have implemented in the domain of personal income
tax planning.
1 Introduction

A Comparison of Content-Based Image/Video Retrieval
Systems
Fatma Ozcan
December 15, 1997
Abstract
Recently, content-based retrieval of images and video has become a hot research area. The reason for this
is the need for effective and efficient techniques, that meet user requirements, to access large volumes of digital
images and video data. In [GR 95], previous approaches to content-based retrieval is said to have taken two
directions. In the first approach, image contents are modeled as a set of attributes extracted manually and
managed within the framework of conventional database-management systems. The second approach depends
on an integrated feature-extraction/object-recognition subsystem to overcome the limitations of attribute-based retrieval. However, it has been also stated that [GR 95] recent content-based image retrieval systems
recognized the need for synergy between these two approaches.
There has been numerous efforts to build content-based image retrieval systems. These include, Chabot
[OS 95], SCORE [ATY 95, ATY 96], CONIVAS [AD 96], Photobook [PPS 93], CANDID [KCH 95], JACOB
[ALDV 96, LA 96], VisualSEEk [SC 96] and QBIC [FSN+ 95]. In the first place, these systems differ in their
querying capabilities. Images could be retrieved through the use of color, texture, sketch, shape, volume,
spatial constraints, browsing, motion, text and domain concepts. Current approaches to content-based image
retrieval also differ in terms of image features extracted, their level of abstraction and the degree of domain
independence. In this paper, a comparative survey on these systems is conducted to figure out design tradeoffs
and different approaches to content-based image retrieval. Furthermore, a taxonomy of content-based retrieval
systems, as well as a generic architecture, is provided.
1 Introduction

Efficiently Supporting Ad Hoc Queries in Large Datasets of Time
Sequences
Flip Korn
Dept. of Computer Science
University of Maryland
College Park, MD 20742
flip@cs.umd.edu
H. V. Jagadish
AT&T Laboratories
Florham Park, NJ 07932
jag@research.att.com
Christos Faloutsos
Dept. of Computer Science and
Inst. for Systems Research
University of Maryland
College Park, MD 20742
christos@cs.umd.edu
Abstract
Ad hoc querying is difficult on very large datasets, since it
is usually not possible to have the entire dataset on disk.
While compression can be used to decrease the size of the
dataset, compressed data is notoriously difficult to index
or access.
In this paper we consider a very large dataset comprising multiple distinct time sequences. Each point in the
sequence is a numerical value. We show how to compress
such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the
data is uncompressed. Experiments on large, real world
datasets (AT&T customer calling patterns) show that the
proposed method achieves an average of less than 5% error
in any data value after compressing to a mere 2.5% of the
original space (i.e., a 40:1 compression ratio), with these
numbers not very sensitive to dataset size. Experiments
on aggregate queries achieved a 0.5% reconstruction error
with a space requirement under 2%.
1 Introduction

How to Be a Good
Graduate Student
Advisor
Marie desJardins
marie@erg.sri.com
March 1994
Abstract
This paper attempts to raise some issues that
are important for graduate students to be successful and to get as much out of the process
as possible, and for advisors who wish to help
their students be successful. The intent is not
to provide prescriptive advice no formulas for
finishing a thesis or twelve-step programs for becoming a better advisor are given but to raise
awareness on both sides of the advisor-student
relationship as to what the expectations are and
should be for this relationship, what a graduate
student should expect to accomplish, common
problems, and where to go if the advisor is not
forthcoming.
1 Introduction

To appear, Computer Aided Design, 1995 or 1996
GENERATING REDESIGN SUGGESTIONS TO REDUCE SETUP COST:
A STEP TOWARDS AUTOMATED REDESIGN
Diganta Das
Mechanical Engr. Dept. and
Institute for Systems Research,
University of Maryland
College Park, MD 20742
diganta@cs.umd.edu
Satyandra K. Gupta
The Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213
skgupta@isl1.ri.cmu.edu
Dana S. Nau
Dept. of Computer Science and
Institute for Systems Research,
University of Maryland
College Park, MD 20742
nau@cs.umd.edu
Abstract
All mechanical designs pass through a series of formal and informal redesign steps, involving the analysis of functionality, manufacturability, cost and other life-cycle factors. The
speed and efficacy of these steps has a major influence on the lead time of the product from
conceptualization to launching. In this paper we propose a methodology for automatically
generating redesign suggestions for reducing setup costs for machined parts.
Given an interpretation of the design as a collection of machinable features, our approach
is to generate alternate machining features by making geometric changes to the original
features, and add them to the feature set of the original part to create an extended feature
set. The designer may provide restrictions on the design indicating the type and extent of
modifications allowed on certain faces and volumes, in which case all redesign suggestions
generated by our approach honor those restrictions.
By taking combinations of features from the extended feature set generated above, we can
generate modified versions of the original design that still satisfy the designer's intent. By
considering precedence constraints and approach directions for the machining operations as
well as simple fixturability constraints, we can estimate the setup time that will be required
for each design. Any modified design whose setup time is less than that of the original
design can be presented to the designer as a possible way to modify the original design.
+PAGE+

Transport and Display Mechanisms For Multimedia
Conferencing Across Packet-Switched Networks *
K. Jeffay, D.L. Stone, F.D. Smith
University of North Carolina at Chapel Hill
Department of Computer Science
Chapel Hill, NC 27599-3175 USA
-jeffay,stone,smithfd-@cs.unc.edu
September, 1993
Abstract: A transport protocol that supports real-time communication of
audio/video frames across campus-area packet switched networks is presented.
It is a best effort protocol that attempts to ameliorate the effect of jitter, load
variation, and packet loss, to provide low latency, synchronized audio and video
communications. This goal is realized through four transport and display
mechanisms, and a real-time implementation of these mechanisms that
integrates operating system services ( e.g., scheduling and resource allocation,
and device management) with network communication services ( e.g., transport
protocols), and with application code ( e.g., display routines). The four
mechanisms are: a facility for varying the synchronization between audio and
video to achieve continuous audio in the face of jitter, a network congestion
monitoring mechanism that is used to control audio/video latency, a queueing
mechanism at the sender that is used to maximize frame throughput without
unnecessarily increasing latency, and a forward error correction mechanism for
transmitting audio frames multiple times to ameliorate the effects of packet loss
in the network. The effectiveness of these techniques is demonstrated by
measuring the performance of the protocol when transmitting audio and video
across congested networks.
Published in: Computer Networks and ISDN Systems,
Vol. 26, No. 10, (July 1994) pp. 1281-1304.
* This work supported in parts by grants from the National Science Foundation (numbers CCR-9110938 and ICI
9015443), and by the Digital Equipment Corporation and the IBM Corporation.
+PAGE+

To appear in 1996 ACM SIGMETRICS (Extended Version: UMass TR 95-98, Nov. 1995)
Supporting Stored Video: Reducing Rate Variability and End-to-End Resource
Requirements through Optimal Smoothing
James D. Salehi, Zhi-Li Zhang, James F. Kurose, and Don Towsley
Department of Computer Science, University of Massachusetts,   Amherst MA 01003, USA
Abstract
VBR compressed video is known to exhibit significant, multiple-
time-scale bit rate variability. In this paper, we consider the transmission of stored video from a server to a client across a high speed
network, and explore how the client buffer space can be used most
effectively toward reducing the variability of the transmitted bit
rate.
We present two basic results. First, we present an optimal
smoothing algorithm for achieving the greatest possible reduction
in rate variability when transmitting stored video to a client with
given buffer size. We provide a formal proof of optimality, and
demonstrate the performance of the algorithm on a set of long
MPEG-1 encoded video traces. Second, we evaluate the impact
of optimal smoothing on the network resources needed for video
transport, under two network service models: Deterministic Guar-
anteed service [1, 11] and Renegotiated CBR (RCBR) service [9, 8].
Under both models, we find the impact of optimal smoothing to be
dramatic.
1 Introduction

Alleviating Priority Inversion and Non-determinism
in Real-time CORBA ORB Core Architectures
Douglas C. Schmidt, Sumedh Mungee, and Aniruddha Gokhale
fschmidt,sumedh,gokhaleg@cs.wustl.edu
Department of Computer Science, Washington University
St. Louis, MO 63130, USA
This paper has been submitted to the 4 th IEEE Real-time
Technology and Applications Symposium (RTAS), Denver,
Colorado, June 3-5, 1998.
Abstract
There is increasing demand to extend CORBA to support
applications with stringent real-time requirements. However, conventional CORBA Object Request Brokers (ORBs)
exhibit substantial priority inversion and non-determinism,
which makes them unsuitable for applications with deterministic real-time requirements. This paper focuses on software
architectures that help to alleviate priority inversion and non-determinism in real-time CORBA ORBs. It also illustrates empirically why conventional ORBs do not yet support real-time
quality of service.
Keywords: Real-time CORBA Object Request Broker, QoS-enabled OO Middleware, Performance Measurements
1 Introduction

Reducing False Sharing on Shared Memory Multiprocessors
through Compile Time Data Transformations
Tor E. Jeremiassen
AT&T Bell Laboratories
600 Mountain Ave.
Murray Hill, New Jersey 07974
tor@research.att.com
Susan J. Eggers
Department of Computer Science and Engineering
University of Washington
Seattle, Washington 98195
eggers@cs.washington.edu
Abstract
We have developed compiler algorithms that analyze explicitly parallel programs and restructure their shared data to
reduce the number of false sharing misses. The algorithms
analyze per-process shared data accesses, pinpoint the data
structures that are susceptible to false sharing and choose
an appropriate transformation to reduce it. The transformations either group data that is accessed by the same processor or separate individual data items that are shared.
This paper evaluates that technique. We show through
simulation that our analysis successfully identifies the data
structures that are responsible for most false sharing misses,
and then transforms them without unduly decreasing spatial
locality. The reduction in false sharing positively impacts
both execution time and program scalability when executed
on a KSR2. Both factors combine to increase the maximum
achievable speedup for all programs, more than doubling
it for several. Despite being able to only approximate actual inter-processor memory accesses, the compiler-directed
transformations always outperform programmer efforts to
eliminate false sharing.
1 Introduction

To appear in Proceedings of the Twenty Third ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, St. Petersburg
Beach, Florida, January 21-24, 1996. c fl1996 ACM (see notice below).
Is it a Tree, a DAG, or a Cyclic Graph?
A Shape Analysis for Heap-Directed Pointers in C
Rakesh Ghiya and Laurie J. Hendren
School of Computer Science, McGill University
Montreal, Quebec, CANADA H3A 2A7
fghiya,hendreng@cs.mcgill.ca
Abstract
This paper reports on the design and implementation of a practical shape analysis for C. The purpose of the analysis is to aid in the disambiguation of
heap-allocated data structures by estimating the shape
(Tree, DAG, or Cyclic Graph) of the data structure accessible from each heap-directed pointer. This shape
information can be used to improve dependence testing and in parallelization, and to guide the choice of
more complex heap analyses.
The method has been implemented as a context-sensitive interprocedural analysis in the McCAT compiler. Experimental results and observations are given
for 16 benchmark programs. These results show that
the analysis gives accurate and useful results for an
important group of applications.
1 Introduction and Related

Compiler Support for Maintaining Cache
Coherence Using Data Prefetching ?
(Extended Abstract)
Hock-Beng Lim 1 , Lynn Choi 2 and Pen-Chung Yew 3
1 Center for Supercomputing R & D, Univ. of Illinois,   Urbana, IL 61801
2 Microprocessor Group, Intel Corporation,   Santa Clara, CA 95095
3 Dept. of Computer Science, Univ. of Minnesota,   Minneapolis, MN 55455
1 Introduction and Motivation

INTEGRATING FINE-GRAINED MESSAGE PASSING IN CACHE COHERENT
SHARED MEMORY MULTIPROCESSORS (*)
David K. Poulsen
Kuck and Associates, Inc.
1906 Fox Drive
Champaign, IL 61821
217-356-2288
poulsen@kai.com
and
Pen-Chung Yew
Department of Computer Science
University of Minnesota
4-192 EE/CS Building
200 Union Street SE
Minneapolis, MN 55455
yew@cs.umn.edu
(*) This work was supported in part by the National Science Foundation under grant nos. NSF MIP 93-07910 and
NSF MIP 89-20891, the Department of Energy under grant no. DOE DE FG02-85ER25001, the National Security
Agency, and an IBM Resident Study Fellowship.
This work was performed while the authors were with the Center for Supercomputing Research and Development,
University of Illinois at Urbana-Champaign, Urbana, IL 61801.
+PAGE+

Constraint-Based Animations
Allan Heydon Greg Nelson
Digital Systems Research Center
130 Lytton Ave., Palo Alto, CA 94301
fheydon,gnelsong@pa.dec.com
Copyright c fl1995, Digital Equipment Corporation. All right reserved.
1. Introduction

Dynamic Resource Management for Continuous
Media Traffic over ATM Networks
Rose P. Tsang 1 , Paisal Keattithananant
Taisheng Chang 3 , Jenwei Hsieh 3 , David H.C. Du 2
Distributed Multimedia Center 3
Computer Science Department
University of Minnesota
Minneapolis, MN 55455
Abstract
Real-time continuous media traffic, such as digital video and audio, is expected
to comprise a large percentage of the network load on future high speed packet
switch networks such as ATM. A major feature which distinguishes high speed
networks from traditional slower speed networks is the large amount of data the
network must manage. For efficient network usage, traffic control mechanisms are
essential. Currently, most mechanisms for traffic control (such as flow control)
have centered on the support of Available Bit Rate (ABR), i.e., non real-time,
traffic. With regard to ATM, for ABR traffic, two major types of schemes which
have been proposed are rate-control and credit-control schemes. Neither of these
schemes are directly applicable to Real-time Variable Bit Rate (VBR) traffic such
as continuous media traffic. Traffic control for continuous media traffic is an inherently difficult problem due to the time-sensitive nature of the traffic and its
unpredictable burstiness. In this study, we present a scheme which controls traffic by dynamically allocating/de-allocating resources among competing VCs based
upon their real-time requirements. This scheme incorporates a form of rate-control,
real-time burst-level scheduling and link-link flow control. We show analytically potential performance improvements of our rate-control scheme and present a scheme
for buffer dimensioning. We also present simulation results of our schemes and discuss the tradeoffs inherent in maintaining high network utilization and statistically
guaranteeing many users' Quality of Service.
Keywords: Asynchronous Transfer Mode (ATM), dynamic traffic control, resource man
agement, rate control, Real-time Variable Bit Rate traffic, continuous media traffic.
1 This work is supported in part by U S WEST Communications.

A short version of this paper appears in Supercomputing 1996
The serial algorithms described in this paper are implemented by the
`METIS: Unstructured Graph Partitioning and Sparse Matrix Ordering System'.
METIS is available on WWW at URL:   http://www.cs.umn.edu/karypis/metis/metis.html
Parallel Multilevel k -way Partitioning Scheme for
Irregular Graphs
George Karypis and Vipin Kumar
University of Minnesota, Department of Computer Science
Minneapolis, MN 55455,   Technical Report: 96-036
fkarypis, kumarg@cs.umn.edu
Last updated on October 24, 1996 at 9:02am
Abstract
In this paper we present a parallel formulation of a multilevel k-way graph partitioning algorithm. The multilevel
k-way partitioning algorithm reduces the size of the graph by collapsing vertices and edges (coarsening phase), finds
a k-way partitioning of the smaller graph, and then it constructs a k-way partitioning for the original graph by projecting and refining the partition to successively finer graphs (uncoarsening phase). A key innovative feature of our
parallel formulation is that it utilizes graph coloring to effectively parallelize both the coarsening and the refinement
during the uncoarsening phase. Our algorithm is able to achieve a high degree of concurrency, while maintaining the
high quality partitions produced by the serial algorithm. We test our scheme on a large number of graphs from finite
element methods, and transportation domains. For graphs with a million vertices, our parallel formulation produces
high quality 128-way partitions on 128 processors in a little over two seconds, on Cray T3D. Thus our parallel algorithm makes it feasible to perform frequent dynamic graph partition in adaptive computations without compromising
quality.
Keywords: Parallel Graph Partitioning, Multilevel Partitioning Methods, Spectral Partitioning Methods,
Kernighan-Lin Heuristic, Parallel Sparse Matrix Algorithms.
This work was supported by NSF CCR-9423082 and by Army Research Office contract DA/DAAH04-95-1-0538, and by Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement
number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Access to computing facilities was provided by AHPCRC, Minnesota
Supercomputer Institute, Cray Research Inc, and by the Pittsburgh Supercomputing Center. Related papers are available via WWW at URL:
http://www.cs.umn.edu/karypis
+PAGE+

Optimizing Multi-Join Queries in Parallel Relational Databases
Jaideep Srivastava
Gary Elsesser
Computer Science Department
University of Minnesota
Minneapolis, MN 55455
Abstract
Query optimization for parallel machines needs to
consider machine architecture, processor and memory
resources available, and different types of parallelism,
making the search space much larger than the sequential case. In this paper our aim is to determine a plan
that makes the execution of an individual query very
fast, making minimizing parallel execution time the
right objective. This creates the following circular dependence: a plan tree is needed for effective resource
assignment, which is needed to estimate the parallel
execution time, and this is needed for the cost-based
search for a good plan tree. In this paper we propose
a new search heuristic that breaks the cycle by constructing the plan tree layer by layer in a bottom-up
manner. To select nodes at the next level, the lower
and upper bounds on the execution time for plans consistent with the decisions made so far are estimated
and are used to guide the search. A query plan representation for intra- and inter-operator parallelism,
pipelining, and processor and memory assignment is
proposed. Also proposed is a new approach to estimating the parallel execution time of a plan that considers
sum and max of operators working sequentially and
in parallel, respectively. The results obtained from a
prototype optimizer are presented.
1 Introduction

CS/TR-92-80
The COOL architecture and abstractions for object-oriented
distributed operating systems
Rodger Lea, Christian Jacquemot
approved by:
distribution: COOL
abstract: Building distributed operating systems benefits from the micro-kernel approach by allowing better support for modularization. However, we believe
that we need to take this support a step further. A more modular, or object
oriented approach is needed if we wish to cross the barrier of complexity that
is holding back distributed operating system development. The Chorus Ob
ject Oriented Layer (COOL) is a layer built above the Chorus micro-kernel
designed to extend the micro-kernel abstractions with support for object oriented systems. COOL v2, the second iteration of this layer provides generic
support for clusters of objects, in a distributed virtual memory model. It is
built as a layered system where the lowest layer support only clusters and the
upper layers support objects.
c Chorus systemes, 1992
c Chorus systemes, 1992 September 21, 1992
+PAGE+

Using PVS to Analyze Hierarchical State-Based Requirements for
Completeness and Consistency
Mats P.E. Heimdahl
University of Minnesota, Institute of Technology
Department of Computer Science,   4-192 EE/CS Bldg.
Minneapolis, MN 55455
heimdahl@cs.umn.edu
Barbara J. Czerny
Department of Computer Science,   A-714 Wells Hall
Michigan State University
East Lansing, MI 48824
czerny@cps.msu.edu
1 Introduction

AN ALGORITHM FOR GENERATING EXECUTABLE
ASSERTIONS FOR FAULT TOLERANCE
Martina Schollmeyer, Hanan Lutfiyya and Bruce McMillin
CSC-92-01
March 21, 1992
Department of Computer Science
University of Missouri at Rolla
Rolla, Missouri 65401
This work was supported in part by the National Science Foundation under Grant Numbers MIP-8909749 and CDA-8820714, and in part by the AMOCO Faculty Development
Program.
+PAGE+

To be presented at the 17th IEEE Real-Time Systems Symposium, December 1996.
A Framework for Implementing Objects and Scheduling Tasks in Lock-Free
Real-Time Systems
James H. Anderson and Srikanth Ramamurthy
Department of Computer Science, University of North Carolina at Chapel Hill
Abstract
We present an integrated framework for developing real-time systems in which lock-free algorithms are employed to
implement shared objects. There are two key objectives of
our work. The first is to enable functionality for object sharing in lock-free real-time systems that is comparable to that
in lock-based systems. Our main contribution toward this
objective is an efficient approach for implementing multi-object lock-free operations and transactions. A second key
objective of our work is to improve upon previously proposed
scheduling conditions for tasks that share lock-free objects.
When developing such conditions, the key issue is to bound
the cost of operation interferences. We present a general
approach for doing this, based on linear programming.
1. Introduction

Universal Constructions for Large Objects
James H. Anderson and Mark Moir
Dept. of Computer Science, University of North Carolina at Chapel Hill
Abstract
We present lock-free and wait-free universal constructions for implementing large
shared objects. Most previous universal constructions require processes to copy the
entire object state, which is impractical for large objects. Previous attempts to
address this problem require programmers to explicitly fragment large objects into
smaller, more manageable pieces, paying particular attention to how such pieces are
copied. In contrast, our constructions are designed to largely shield programmers
from this fragmentation. Furthermore, for many objects, our constructions result
in lower copying overhead than previous ones.
Fragmentation is achieved in our constructions through the use of load-linked,
store-conditional, and validate operations on a "large" multi-word shared variable.
Before presenting our constructions, we show that these operations can be efficiently
implemented from similar one-word primitives.
1 Introduction

View-Dependent Simplification Of Arbitrary Polygonal Environments
David Luebke, Carl Erikson
Department of Computer Science
University of North Carolina at Chapel Hill
1. ABSTRACT
Hierarchical dynamic simplification (HDS) is a new approach to
the problem of simplifying arbitrary polygonal environments.
HDS operates dynamically, retessellating the scene continuously
as the users viewing position shifts, and adaptively, processing
the entire database without first decomposing the environment
into individual objects. The resulting system allows real-time
display of very complex polygonal CAD models consisting of
thousands of parts and hundreds of thousands of polygons. HDS
supports various preprocessing algorithms and various runtime
criteria, providing a general framework for dynamic view-dependent simplification.
Briefly, HDS works by clustering vertices together in a
hierarchical fashion. The simplification process continuously
queries this hierarchy to generate a scene containing only those
polygons that are important from the current viewpoint. When
the volume of space associated with a vertex cluster occupies less
than a userspecified amount of the screen, all vertices within
that cluster are collapsed together and degenerate polygons
filtered out. HDS maintains an active list of visible polygons for
rendering. Since frame-to-frame movements typically involve
small changes in viewpoint, and therefore modify the active list
by only a few polygons, the method takes advantage of temporal
coherence for greater speed.
CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and
Object Modeling - surfaces and object representations.
Additional Keywords: polygonal simplification, level of detail, view
dependent rendering.
2. INTRODUCTION

Modeling and Parameter Estimation of the Human Index Finger
Robert N. Rohling and John M. Hollerbach
Biorobotics Laboratory, McGill University
3775 University St., Montreal, Quebec H3A 2B4
Abstract
Precise teleoperation of dextrous robotic hands by
hand masters requires an accurate human hand model.
A kinematic model of a human index finger is developed as an example for human hand modeling. The
parameters of the model are determined by open-loop
kinematic calibration. Singular value decomposition is
used as a tool for analyzing the kinematic model and
the identification process. Accurate and reliable results are obtained only when the numerical condition
is minimized through parameter scaling, model reduction and pose set selection. The identified kinematic
parameters show the kinematic model and calibration
procedure have an accuracy on the order of a few millimeters.
1 Introduction

A Dossier Driven Persistent Objects Facility
Robert Mecklenburg, Charles Clark, Gary Lindstrom and Benny Yih
University of Utah Center for Software Science
Department of Computer Science
Salt Lake City, UT 84112
E-mail: fmecklen,clark,gary,yihg@cs.utah.edu
Abstract
We describe the design and implementation of a persistent object storage facility
based on a dossier driven approach. Objects are characterized by dossiers which describe both their language defined and "extra-linguistic" properties. These dossiers are
generated by a C++ preprocessor in concert with an augmented, but completely C++
compatible, class description language. The design places very few burdens on the application programmer and can be used without altering the data member layout of application objects or inheriting from special classes. The storage format is kept simple to allow
the use of a variety of data storage backends. In addition, these dossiers can be used to
implement (or augment) a run-time typing facility compatible with the proposed ANSI
C++ standard. Finally, by providing a generic object to byte stream conversion the persistent object facility can also be used in conjunction with an interprocess communication
facility to provide object-level communication between processes. 1
1 Motivation

The Next Frontier: Interactive and Closed Loop Performance
Steering
Daniel A. Reed Christopher L. Elford
Tara M. Madhyastha Evgenia Smirni
Stephen E. Lamm
freed,elford,tara,esmirni,slammg@cs.uiuc.edu
Department of Computer Science
University of Illinois
Urbana, Illinois 61801
Abstract
Software for a growing number of problem domains
has complex, time varying behavior and unpredictable
resource demands (e.g., WWW servers and parallel input/output systems). While current performance analysis tools provide insights into application dynamics
and the causes of poor performance, with a posteriori analysis one cannot adapt to temporally varying
application resource demands and system responses.
We believe that the solution to this performance optimization conundrum is integration of dynamic performance instrumentation and on-the-fly performance
data reduction with real-time adaptive control mechanisms that select and configure resource management
algorithms automatically, based on observed application behavior, or interactively, through high-modality
virtual environments. We motivate this belief by first
describing our experiences with performance analysis tools, input/output characterization, and WWW
server analysis, and then sketching the design of interactive and closed loop adaptive control systems.
1 Introduction

Compilation of Constraint Systems to
Procedural Parallel Programs
Ajita John and J. C. Browne
Dept. of Computer Sciences
University of Texas,   Austin, TX 78712
fajohn,browneg@cs.utexas.edu
Abstract. This paper describes the first results from research 1 on the
compilation of constraint systems into task level parallel programs in a
procedural language. This is the only research, of which we are aware,
which attempts to generate efficient parallel programs for numerical
computations from constraint systems. Computations are expressed as
constraint systems. A dependence graph is derived from the constraint
system and a set of input variables. The dependence graph, which exploits the parallelism in the constraints, is mapped to the target language CODE, which represents parallel computation structures as generalized dependence graphs. Finally, parallel C programs are generated.
The granularity of the derived dependence graphs depends upon the
complexity of the operations represented in the type system of the constraint specification language. To extract parallel programs of appropriate granularity, the following features have been included: (i) modularity, (ii) operations over structured types as primitives, (iii) definition
of sequential C functions. A prototype of the compiler has been implemented. The execution environment or software architecture is specified
separately from the constraint system. The domain of matrix computations has been targeted for applications. Some examples have been
programmed. Initial results are very encouraging.
1 Introduction

Optimal Wire-Sizing Formula Under the Elmore Delay Model
Chung-Ping Chen, Yao-Ping Chen, and D. F. Wong
Department of Computer Sciences, University of Texas,   Austin, Texas 78712
Abstract
In this paper, we consider non-uniform wire-sizing. Given a
wire segment of length L, let f(x) be the width of the wire
at position x, 0 x L. We show that the optimal wire-sizing function that minimizes the Elmore delay through the
wire is f(x) = ae bx , where a &gt; 0 and b &gt; 0 are constants
that can be computed in O(1) time. In the case where lower
bound (L &gt; 0) and upper bound (U &gt; 0) on the wire widths
are given, we show that the optimal wire-sizing function f(x)
is a truncated version of ae bx that can also be determined in
O(1) time. Our wire-sizing formula can be iteratively applied
to optimally size the wire segments in a routing tree.
1 Introduction

In Proceedings of the Eighth International Workshop
on Qualitative Physics about Physical Systems (QR-94)
Nara, Japan, 1994.
Model Decomposition and Simulation
Daniel J. Clancy and Benjamin Kuipers
Department of Computer Sciences
University of Texas at Austin
Austin, Texas 78712
clancy@cs.utexas.edu and kuipers@cs.utexas.edu
Abstract
Qualitative reasoning uses incomplete knowledge to compute a description of the possible behaviors for dynamic
systems. For complex systems containing a large number
of variables and constraints, the simulation frequently is
intractable or results in a large, incomprehensible behavioral description. Abstraction and aggregation techniques are required during the simulation to eliminate
irrelevant details and highlight the important characteristics of the behavior. The total temporal ordering of
unrelated events provided by a traditional state-based
qualitative representation is one such irrelevant distinction. Model decomposition and simulation addresses this
problem.
Model decomposition uses a causal analysis of the model
to partition the variables into tightly connected components. The components are simulated separately in
the order dictated by the causal analysis beginning with
causally upstream components. Information from the
simulation of causally upstream components is used to
constrain the behavior of downstream components. If a
feedback loop exists between components or a set of components are acausally related, then a concurrent simulation is performed for these components. A truth maintenance system is used to record and retract assumptions
made during this concurrent simulation.
Model decomposition provides a general architecture
which separates the method of simulation from the model
decomposition algorithm. This architecture can be used
to introduce alternative abstraction techniques to eliminate other irrelevant distinctions.
1 Introduction

Programming the Web:
An Application-oriented Language for Hypermedia Services
David A. Ladd J. Christopher Ramming
ladd@research.att.com jcr@research.att.com
AT&T Bell Laboratories
October 9, 1995
Abstract
MAWL is an application language for programming interactive services in the context of the Worldwide
Web. The language is small, because no construct was introduced without compelling justification; as with
yacc [8], general-purpose computation is done in a host language. MAWL offers conveniences such as control
abstraction, persistent state management, synchronization, and shared memory. In addition, the MAWL
compiler performs static checking designed to prevent common Web programming errors. In this paper we
discuss the design and engineering of MAWL.
We describe the problems MAWL is intended to solve, and then discuss our design choices in the context
of our general language design philosophy, We also include an appendix of commentary on several short
MAWL programs.
1 Introduction

Stability and Chaos in an Inertial Two
Neuron System
Diek W. Wheeler 1 and W. C. Schieve
Ilya Prigogine Center for Studies in Statistical Mechanics and
Complex Systems
and
Physics Department, The University of Texas,
Austin, TX 78712
Abstract.
Inertia is added to a continuous-time, Hopfield [1] effective-neuron system.
We explore the effects on the stability of the fixed points of the system. A two
neuron system with one or two inertial terms added is shown to exhibit chaos.
The chaos is confirmed by Lyapunov exponents, power spectra, and phase space
plots.
INTRODUCTION

ON UNAPPROXIMABLE VERSIONS OF NP-COMPLETE
PROBLEMS
DAVID ZUCKERMAN
Abstract.
We prove that all of Karp's 21 original NP -complete problems have a version that's hard to
approximate. These versions are obtained from the original problems by adding essentially the same,
simple constraint. We further show that these problems are absurdly hard to approximate. In fact, no
polynomial-time algorithm can even approximate log (k) of the magnitude of these problems to within
any constant factor, where log (k) denotes the logarithm iterated k times, unless N P is recognized by
slightly superpolynomial randomized machines. We use the same technique to improve the constant
* such that MAX CLIQUE is hard to approximate to within a factor of n * . Finally, we show that it
is even harder to approximate two counting problems: counting the number of satisfying assignments
to a monotone 2-SAT formula and computing the permanent of -1,0,1 matrices.
Key words. NP-complete, unapproximable, randomized reduction, clique, counting problems,
permanent, 2SAT
AMS subject classifications. 68Q15, 68Q25, 68Q99
1. Introduction.

Recursive Functions of Symbolic
Expressions and Their Computation by
Machine, Part I
John McCarthy,   Massachusetts Institute of Technology,   Cambridge, Mass.
April 1960
1 Introduction

Frangipani: A Scalable Distributed File System
Chandramohan A. Thekkath
Timothy Mann
Edward K. Lee
Systems Research Center
Digital Equipment Corporation
130 Lytton Ave, Palo Alto, CA 94301
Abstract
The ideal distributed file system would provide all its users with coherent, shared access to the same set of files,yet would be arbitrarily
scalable to provide more storage space and higher performance to
a growing user community. It would be highly available in spite of
component failures. It would require minimal human administration, and administration would not become more complex as more
components were added.
Frangipani is a new file system that approximates this ideal, yet
was relatively easy to build because of its two-layer structure. The
lower layer is Petal (described in an earlier paper), a distributed
storage service that provides incrementally scalable, highly available, automatically managed virtual disks. In the upper layer,
multiple machines run the same Frangipani file system code on top
of a shared Petal virtual disk, using a distributed lock service to
ensure coherence.
Frangipani is meant to run in a cluster of machines that are under
a common administration and can communicate securely. Thus the
machines trust one another and the shared virtual disk approach is
practical. Of course, a Frangipani file system can be exported to
untrusted machines using ordinary network file access protocols.
We have implemented Frangipani on a collection of Alphas
running DIGITAL Unix 4.0. Initial measurements indicate that
Frangipani has excellent single-server performance and scales well
as servers are added.
1 Introduction

The Cost of Recovery in Message Logging Protocols
Sriram Rao, Lorenzo Alvisi, and Harrick M. Vin
Department of Computer Sciences
The University of Texas at Austin
Taylor Hall 2.124, Austin, Texas 78712-1188, USA
E-mail: fsriram,lorenzo,ving@cs.utexas.edu,   Telephone: (512) 471-9792, Fax: (512) 471-8885
URL: http://www.cs.utexas.edu/users/fsriram,lorenzo,ving
Abstract
Message logging is a popular technique for building low-overhead protocols that tolerate
process crash failures. Past research in message logging has focused on studying the relative
overhead imposed by pessimistic, optimistic, and causal protocols during failure-free executions. In this paper, we give the first experimental evaluation of the performance of these
protocols during recovery. We discover that, if a single failure is to be tolerated, pessimistic
and causal protocols perform best, because they avoid rollbacks of correct processes. For
multiple failures, however, the dominant factor in determining performance becomes where
the recovery information is logged (i.e. at the sender, at the receiver, or replicated at a
subset of the processes in the system) rather than when this information is logged (i.e. if
logging is synchronous or asynchronous). From our results, we distil a few lessons that can
guide the design of message-logging protocols that combine low-overhead during failure-free
executions with fast recovery.
1 Introduction

Constructing Scripts from Components:
Working Note 6
Peter Clark and Bruce Porter
Dept. CS, UT Austin
fpclark,porterg@cs.utexas.edu
1 Introduction

Design Goals for ACL2
Matt Kaufmann and J Strother Moore
Technical Report 101   August, 1994
Computational Logic, Inc.
1717 West Sixth Street, Suite 290
Austin, Texas 78703-4776
TEL: +1 512 322 9951
EMAIL: kaufmann@cli.com and moore@cli.com
This work was supported in part at Computational Logic, Inc., by the Defense
Advanced Research Projects Agency, ARPA Order 7406. The views and conclusions contained in this document are those of the author(s) and should not
be interpreted as representing the official policies, either expressed or implied,
of Computational Logic, Inc., the Defense Advanced Research Projects Agency
or the U.S. Government.
+PAGE+

Broadcasting on Meshes with Worm-Hole Routing
Michael Barnett
Department of Computer Science
University of Idaho
Moscow, Idaho 83844-1010
mbarnett@cs.uidaho.edu
David G. Payne
Supercomputer Systems Division
Intel Corporation
15201 N.W. Greenbrier Pkwy
Beaverton, Oregon 97006
payne@ssd.intel.com
Robert A. van de Geijn
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712-1188
rvdg@cs.utexas.edu
Jerrell Watts
Scalable Concurrent Programming Laboratory
California Institute of Technology
Pasadena, California 91125
jwatts@scp.caltech.edu
Original Version: November 2, 1993
Revised Version: September 21, 1994
Abstract
We address the problem of broadcasting on mesh architectures with arbitrary (non-power-two) dimensions. It is assumed that such mesh architectures employ cut-through or worm-hole routing. The
primary focus is on avoiding network conflicts in the various proposed algorithms. We give algorithms
for performing a conflict-free minimum-spanning tree broadcast, a pipelined algorithm that is similar to
Ho and Johnsson's EDST algorithm for hypercubes, and a novel scatter-collect approach that is a natural
choice for communication libraries due to its simplicity. Results obtained on the Intel Paragon system
are included.
1 Introduction

Towards Usable and Lean
Parallel Linear Algebra Libraries
Almadena Chtchelkanova Carter Edwards
John Gunnels Greg Morrow
James Overfelt Robert van de Geijn
Department of Computer Sciences
and
Texas Institute for Computational and Applied Mathematics
The University of Texas at Austin
Austin, Texas 78712
May 1, 1996
Abstract
In this paper, we introduce a new parallel library effort, as part of the PLAPACK
project, that attempts to address discrepencies between the needs of applications and
parallel libraries. A number of contributions are made, including a new approach
to matrix distribution, new insights into layering parallel linear algebra libraries, and
the application of "object based" programming techniques which have recently become
popular for (parallel) scientific libraries. We present an overview of a prototype library,
the SL Library, which incorporates these ideas. Preliminary performance data shows
this more application-centric approach to libraries does not necessarily adversely impact
performance, compared to more traditional approaches.
This project is sponsored in part by the Office of Naval Research under Contract N00014-95-1-0401,
the NASA High Performance Computing and Communications Program's Earth and Space Sciences Project
under NRA Grant NAG5-2497, the PRISM project under ARPA grant P-95006.
Corresponding and presenting author.   Dept. of Computer Sciences, The University of Texas at Austin,
Austin, TX 78712,  ( 512) 471-9720 (Office), (512) 471-8885 (Fax),   rvdg@cs.utexas.edu.
+PAGE+

Anatomy of a Parallel Out-of-Core Dense Linear Solver
Kenneth Klimkowski
Texas Institute for Computational
and Applied Mathematics
The University of Texas at Austin
Austin, Texas 78712
ken@ticam.utexas.edu
Robert A. van de Geijn
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712
rvdg@cs.utexas.edu
Abstract In this paper, we describe the design
and implementation of the Platform Independent
Parallel Solver (PIPSolver) package for the out-of-core (OOC) solution of complex dense linear systems. Our approach is unique in that it allows essentially all of RAM to be filled with the current
portion of the matrix (slab) to be updated and factored, thereby greatly improving the computation to
I/O ratio over previous approaches. Experiences
and performance are reported for the Cray T3D
system.
INTRODUCTION

Automated Proof Support for
Reasoning about Distributed Mobile
Programs
A thesis
submitted in partial fulfilment
of the requirements for
the degree
of
Bachelor of Technology
in
Computer Science and Engineering
by
B Karthikeyan
T R Vishwanath
under the guidance of
Dr Sanjiva Prasad
Department of Computer Science & Engineering
Indian Institute of Technology, Delhi
May 1997
+PAGE+

NetSolve: A Network Server
for Solving Computational Science Problems
Henri Casanova
University of Tennessee
UTK, Dept. of Computer Science   104, Ayres Hall. KNOXVILLE, TN 37996-1301.
casanova@cs.utk.edu
http://www.cs.utk.edu/~casanova
Jack Dongarra
University of Tennessee,  Oak Ridge National Laboratory
UTK, Dept. of Computer Science   104, Ayres Hall. KNOXVILLE, TN 37996-1301.
dongarra@cs.utk.edu
http://www.netlib.org/utk/people/JackDongarra.html
November 12, 1996
Abstract
This paper presents a new system, called NetSolve, that allows users to access computational resources, such as hardware and software, distributed across the network. The development of NetSolve
was motivated by the need for an easy-to-use, efficient mechanism for using computational resources remotely. Ease of use is obtained as a result of different interfaces, some of which require no programming
effort from the user. Good performance is ensured by a load-balancing policy that enables NetSolve to
use the computational resources available as efficiently as possible. NetSolve offers the ability to look
for computational resources on a network, choose the best one available, solve a problem (with retry for
fault-tolerance), and return the answer to the user.
Keywords
Networking, Heterogeneity, Load Balancing,
Client-Server, Fault Tolerance, Numerical Computing, Virtual Library.
+PAGE+

What robots can do:
Robot programs and effective achievability
Fangzhen Lin
Department of Computer Science
The Hong Kong University of Science and Technology
Clear Water Bay, Hong Kong
flin@cs.ust.hk
Hector Levesque
Department of Computer Science
University of Toronto
Toronto, Canada M5S 3H5
hector@ai.toronto.edu
May 15, 1997
Abstract
In this paper, we propose a definition of goal achievability: given a basic action theory describing an initial state of the world and some primitive actions available to a
robot, including some actions which return binary sensing information, what goals can
be achieved by the robot? The main technical result of the paper is a proof that a simple
robot programming language is universal, in that any effectively achievable goal can be
achieved by getting the robot to execute one of the robot programs. The significance of
this result is at least two fold. First, it is in many ways similar to the equivalence theorem between Turing machines and recursive functions, but applied to robots whose
actions are specified by an action theory. Secondly, it provides formal justifications for
using the simple robot programming language as a foundation for our work on robotics.
+PAGE+

An Ordering on Subgoals for Planning
Fangzhen Lin
D epartment of Computer Science
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
@cs.ust.hk
Abstract
Subgoal ordering is a type of control information that has received much attention
in AI planning community. In this paper we formulate precisely a subgoal ordering
in the situation calculus. We show how information about this subgoal ordering can
be deduced from the background action theory. We also show for both linear and
nonlinear planners how knowledge about this ordering can be used in a provably
correct way to avoid unnecessary backtracking.
1 Introduction

How to Progress a Database (and Why)
I. Logical Foundations
Fangzhen Lin and Ray Reiter
Department of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
email: @ai.toronto.edu reiter@ai.toronto.edu
Abstract
One way to think about STRIPS is as a mapping from databases to databases, in the following sense: Suppose we want to know what
the world would be like if an action, represented by the STRIPS operator ff, were done
in some world, represented by the STRIPS
database D 0 . To find out, simply perform
the operator ff on D 0 (by applying ff's elementary add and delete revision operators to
D 0 ). We describe this process as progressing
the database D 0 in response to the action ff.
In this paper, we consider the general problem of progressing an initial database in response to a given sequence of actions. We
appeal to the situation calculus and an axiomatization of actions which addresses the
frame problem (Reiter [13], Lin and Reiter
[8]). This setting is considerably more general than STRIPS. Our results concerning
progression are mixed. The (surprising) bad
news is that, in general, to characterize a progressed database we must appeal to second
order logic. The good news is that there are
many useful special cases for which we can
compute the progressed database in first order logic; not only that, we can do so efficiently.
1 INTRODUCTION

Specifying Instructions' Semantics Using CSDL
(Preliminary Report)
Norman Ramsey and Jack W. Davidson
Department of Computer Science
University of Virginia
Charlottesville, VA 22903
June 10, 1998
+PAGE+

Approaching the 5/4-Approximation
for Rectilinear Steiner Trees
Piotr Berman Ulrich Fomeier Marek Karpinski
Michael Kaufmann Alexander Zelikovsky
September 29, 1995
Abstract
The rectilinear Steiner tree problem requires a shortest tree spanning a given vertex subset
in the plane with rectilinear distance. It was proved that the output length of Zelikovsky's [25]
and Berman/Ramaiyer [3] heuristics is at most 1.375 and 97
72 1:347 of the optimal length,
respectively. It was claimed that these bounds are not tight. Here we improve these bounds to
1.3125 and 61
48 1:271, respectively, and give efficient algorithms to find approximations of such
quality. We also prove that for Zelikovsky's heuristic this bound cannot be less than 1.3.
Keywords: Algorithms, approximations, Steiner tree
1 Introduction

A Distributed Protocol for Channel-Based
Communication with Choice
Frederick Knabe
European Computer-Industry Research Centre GmbH
Munich, Germany
knabe@ecrc.de
Abstract
Recent attempts at incorporating concurrency into functional languages have
identified synchronous communication via shared channels as a promising primitive. An additional useful feature found in many proposals is a nondeterministic
choice operator. Similar in nature to the CSP alternative command, this operator
allows different possible actions to be guarded by sends or receives. Choice is
difficult to implement in a distributed environment because it requires offering
many potential communications but closing only one. In this paper we present
the first distributed, deadlock-free algorithm for choice.
Keywords: Distributed protocols, channels, synchronous communication,
choice operator, CSP alternative command.
1. Introduction

Reprinted from 1995 Usenix Technical Conference * January 16-20, 1995 * New Orleans, LA
The New Jersey Machine-Code Toolkit
Norman Ramsey
Bell Communications Research
Mary F. Fernandez
Department of Computer Science, Princeton University
Abstract
The New Jersey Machine-Code Toolkit helps programmers write applications that process machine code.
Applications that use the toolkit are written at an
assembly-language level of abstraction, but they recognize and emit binary. Guided by a short instruction-set specification, the toolkit generates all the bit-manipulating code.
The toolkit's specification language uses four concepts: fields and tokens describe parts of instructions,
patterns describe binary encodings of instructions or
groups of instructions, and constructors map between
the assembly-language and binary levels. These concepts are suitable for describing both CISC and RISC
machines; we have written specifications for the MIPS
R3000, SPARC, and Intel 486 instruction sets.
We have used the toolkit to help write two applications: a debugger and a linker. The toolkit generates
efficient code; for example, the linker emits binary up
to 15% faster than it emits assembly language, making
it 1.7-2 times faster to produce an a.out directly than
by using the assembler.
1 Introduction

Non-Tree Routing
Bernard A. McCoy and Gabriel Robins
Department of Computer Science, University of Virginia,   Charlottesville, VA 22903-2442
Appeared in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,
Vol. 14, No. 6, June 1995, pp. 780-784.
Abstract
An implicit premise of existing routing methods is that the routing topology must correspond to a
tree (i.e., it does not contain cycles). In this paper we investigate the consequences of abandoning this
basic axiom, and instead we allow routing topologies that correspond to arbitrary graphs (i.e., where
cycles are allowed). We show that non-tree routing can significantly improve signal propagation delay,
reduce signal skew, and afford increased reliability with respect to open faults that may be caused by
manufacturing defects and electro-migration. Simulations on uniformly-distributed nets indicate that
depending on net size and technology parameters, our non-tree routing construction reduces maximum
sourse-sink SPICE delay by an average of up to 62%, and reduces signal skew by an average of up to
63%, as compared with Steiner routing. Moreover, up to 77% of the total wirelength in non-trees can
tolerate an open fault without disconnecting the circuit.
1 Introduction

Scheduling and Page Migration for Multiprocessor Compute Servers
Rohit Chandra, Scott Devine, Ben Verghese,
Anoop Gupta, and Mendel Rosenblum
Computer Systems Laboratory
Stanford University,   Stanford CA 94305
Abstract
Several cache-coherent shared-memory multiprocessors have been
developed that are scalable and offer a very tight coupling between
the processing resources. They are therefore quite attractive for
use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management,
however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of
OS scheduling and page migration policies on the performance
of such compute servers. Our experiments are done on the Stan-ford DASH, a distributed-memory cache-coherent multiprocessor.
We show that for our multiprogramming workloads consisting of
sequential jobs, the traditional Unix scheduling policy does very
poorly. In contrast, a policy incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to twofold performance improvement. For our workloads consisting of
multiple parallel applications, we compare space-sharing policies
that divide the processors among the applications to time-slicing
policies such as standard Unix or gang scheduling. We show
that space-sharing policies can achieve better processor utilization
due to the operating point effect, but time-slicing policies benefit
strongly from user-level data distribution. Our initial experience
with automatic page migration suggests that policies based only
on TLB miss information can be quite effective, and useful for
addressing the data distribution problems of space-sharing sched-ulers.
1 Introduction

Using Runtime Measured Workload Characteristics in
Parallel Processor Scheduling
Thu D. Nguyen, Raj Vaswani, and John Zahorjan
Department of Computer Science and Engineering,   Box 352350
University of Washington
Seattle, WA 98195-2350 USA
Technical Report UW-CSE-95-10-01
October 15, 1995
+PAGE+

Semantic Query Optimization in Datalog Programs
(Extended Abstract)
Alon Y. Levy
AT&T Bell Laboratories
levy@research.att.com
Yehoshua Sagiv
Hebrew University, Jerusalem
sagiv@cs.huji.ac.il
Abstract
Semantic query optimization refers to the process of using
integrity constraints (ic's) in order to optimize the evaluation
of queries. The process is well understood in the case
of unions of select-project-join queries (i.e., nonrecursive
datalog). For arbitrary datalog programs, however, the
issue has largely remained an unsolved problem. This
paper studies this problem and shows when semantic query
optimization can be completely done in recursive rules
provided that order constraints and negated EDB subgoals
appear only in the recursive rules, but not in the ic's.
If either order constraints or negated EDB subgoals are
introduced in ic's, then the problem of semantic query
optimization becomes undecidable. Since semantic query
optimization is closely related to the containment problem
of a datalog program in a union of conjunctive queries, our
results also imply new decidability and undecidability results
for that problem when order constraints and negated EDB
subgoals are used.
1 Introduction

PCp
3 : A C Front End for
Preprocessor Analysis and Transformation
Greg J. Badros
gjb@cs.washington.edu
16 October 1997
Abstract
Though the C preprocessor provides necessary language features, it does so in an unstructured way. The lexical nature of cpp creates numerous problems for software engineers
and their tools, all stemming from the chasm between the engineer's view of the source code
and the compiler's view. The simplest way to reduce this problem is to minimize use of the
preprocessor. In light of the data collected in a prior empirical analysis, this paper describes
a tool to aid the software engineer in analyses targeted at replacing preprocessor constructs
with language features. Existing tools for analyzing C source in the context of the preprocessor are unsuitable for such transformations. This work introduces a new approach: tightly
integrating the preprocessor with a C language parser, permitting the code to be analyzed at
both the preprocessor and syntactic levels simultaneously. The front-end framework, called
PCp 3 , combines a preprocessor, a parser, and arbitrary Perl subroutine "hooks" invoked upon
various preprocessor and parser events. PCp 3 's strengths and weaknesses are discussed in the
context of several program understanding and transformation tools, including a conservative
analysis to support replacing cpp's #define directives with C++ language features.
1 Introduction

Automatic Dynamic Compilation Support for
Event Dispatching in Extensible Systems
Craig Chambers, Susan J. Eggers,
Joel Auslander, Matthai Philipose, Markus Mock, Przemyslaw Pardyak
Department of Computer Science and Engineering
University of Washington
Box 352350, Seattle, WA 98195-2350
(206) 685-2094; fax: (206) 543-2969
-chambers, eggers,ausland,matthai,mock,pardy-@cs.washington.edu
Abstract
This paper describes extensions to an automatic dynamic compilation framework to support
optimized event dispatching in the SPIN extensible operating system.
1 Introduction

Effective Cache Prefetching on Bus-Based Multiprocessors
Dean M. Tullsen and Susan J. Eggers
University of Washington
Abstract
Compiler-directed cache prefetching has the potential to hide much of the high memory latency seen by
current and future high-performance processors. However, prefetching is not without costs, particularly on a
multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and
data sharing.
We simulate the effects of a compiler-directed prefetching algorithm, running on a range of bus-based multiprocessors. We show that, despite a high memory latency, this architecture does not necessarily support prefetching
well, in some cases actually causing performance degradations. We pinpoint several problems with prefetching on
a shared memory architecture (additional conflict misses, no reduction in the data sharing traffic and associated
latencies, a multiprocessor's greater sensitivity to memory utilization and the sensitivity of the cache hit rate to
prefetch distance) and measure their effect on performance. We then solve those problems through architectural
techniques and heuristics for prefetching that could be easily incorporated into a compiler: 1) victim caching,
which eliminates most of the cache conflict misses caused by prefetching in a direct-mapped cache, 2) special
prefetch algorithms for shared data, which significantly improve the ability of our basic prefetching algorithm
to prefetch invalidation misses, and 3) compiler-based shared data restructuring, which eliminates many of the
invalidation misses the basic prefetching algorithm doesn't predict. The combined effect of these improvements
is to make prefetching effective over a much wider range of memory architectures.
keywords: cache prefetching, bus-based multiprocessor, cache misses, prefetching strategies, parallel programs, false sharing, memory latency
1 Introduction

Journal of Artificial Intelligence Research 6 (1997) 35-85 Submitted 7/96; published 1/97
SCREEN: Learning a Flat Syntactic and Semantic Spoken
Language Analysis Using Artificial Neural Networks
Stefan Wermter   wermter@informatik.uni-hamburg.de
Volker Weber   weber@informatik.uni-hamburg.de
Department of Computer Science
University of Hamburg
22527 Hamburg, Germany
Abstract
Previous approaches of analyzing spontaneously spoken language often have been based
on encoding syntactic and semantic knowledge manually and symbolically. While there
has been some progress using statistical or connectionist language models, many current
spoken-language systems still use a relatively brittle, hand-coded symbolic grammar or
symbolic semantic component.
In contrast, we describe a so-called screening approach for learning robust processing
of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic,
semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we
use a flat connectionist analysis. This screening approach aims at supporting speech and
language processing by using (1) data-driven learning and (2) robustness of connectionist
networks. In order to test this approach, we have developed the screen system which is
based on this new robust, learned and flat analysis.
In this paper, we focus on a detailed description of screen's architecture, the flat
syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed
evaluation analysis of the robustness under the influence of noisy or incomplete input.
The main result of this paper is that flat representations allow more robust processing of
spontaneous spoken language than deeply structured representations. In particular, we
show how the fault-tolerance and learning capability of connectionist networks can support
a flat analysis for providing more robust spoken-language processing within an overall
hybrid symbolic/connectionist framework.
1. Introduction

Journal of Artificial Intelligence Research 8 (1998) 165-222 Submitted 8/97; published 6/98
Model-Based Diagnosis using Structured System
Descriptions
Adnan Darwiche   darwiche@aub.edu.lb
Department of Mathematics
American University of Beirut
PO Box 11-236
Beirut, Lebanon
Abstract
This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the
system description is augmented with a system structure (a directed graph explicating the
interconnections between system components). Specifically, we first introduce the notion of
a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses,
such as minimal conflicts, correspond to syntactic variations on a consequence. Second,
we propose a new syntactic variation on the consequence known as negation normal form
(NNF) and discuss its merits compared to standard variations. Third, we introduce a basic
algorithm for computing consequences in NNF given a structured system description. We
show that if the system structure does not contain cycles, then there is always a linear-size
consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences
and the topology of the underlying system structure. Finally, we present an algorithm
that enumerates the preferred diagnoses characterized by a consequence. The algorithm is
shown to take linear time in the size of the consequence if the preference criterion satisfies
some general conditions.
1. Introduction

Software Deviation Analysis: A "Safeware" Technique
Jon Damon Reese and Nancy G. Leveson
Dept. of C.S.E. Safeware Engineering Corp.
University of Washington   7200 Lower Ridge, Unit B
Box 352350 Everett, WA 98203, U.S.A.
Seattle, WA 98195, U.S.A.
fjdreese,levesong@cs.washington.edu
fjdreese,levesong@safeware-eng.com
Abstract
Standard safety analysis techniques are often ineffective when computers and digital devices are integrated into plant control. The Safeware methodology and its
set of supporting safety analysis techniques (and prototype tools) includes modeling
and hazard analysis of complex systems where the components may be a mixture of
humans, hardware, and software. This paper describes one of the Safeware hazard
analysis techniques, Software Deviation Analysis, that incorporates the beneficial features of HAZOP (such as guidewords, deviations, exploratory analysis, and a systems
engineering approach) into an automated procedure that is capable of handling the
complexity and logical nature of computer software.
This work was partly funded by NASA/Langley Grant NAG-1-1495, NSF Grant CCR-9396181, and the
California PATH Program of the University of California, in cooperation with the California Department
of Transportation and the U.S. Department of Transportation.
+PAGE+

Partition Based Spatial-Merge Join
Jignesh M. Patel David J. DeWitt
Computer Sciences Department,
University of Wisconsin, Madison
fjignesh, dewittg@cs.wisc.edu
Abstract
This paper describes PBSM (Partition Based Spatial-Merge), a new algorithm for performing spatial join operation. This algorithm is especially effective when neither of the inputs to the join have an index on the joining
attribute. Such a situation could arise if both inputs to the join are intermediate results in a complex query, or in
a parallel environment where the inputs must be dynamically redistributed. The PBSM algorithm partitions the
inputs into manageable chunks, and joins them using a computational geometry based plane-sweeping technique. This paper also presents a performance study comparing the the traditional indexed nested loops join
algorithm, a spatial join algorithm based on joining spatial indices, and the PBSM algorithm. These comparisons
are based on complete implementations of these algorithms in Paradise, a database system for handling GIS applications. Using real data sets, the performance study examines the behavior of these spatial join algorithms in a
variety of situations, including the cases when both, one, or none of the inputs to the join have an suitable index.
The study also examines the effect of clustering the join inputs on the performance of these join algorithms. The
performance comparisons demonstrates the feasibility, and applicability of the PBSM join algorithm.
1 Introduction

Demand Interprocedural Dataflow Analysis
Susan Horwitz, Thomas Reps, and Mooly Sagiv
University of Wisconsin
Abstract
An exhaustive dataflow-analysis algorithm associates with each point in a program a set of dataflow facts
that are guaranteed to hold whenever that point is reached during program execution. By contrast, a
demand dataflow-analysis algorithm determines whether a single given dataflow fact holds at a single given
point.
This paper presents a new demand algorithm for interprocedural dataflow analysis. The algorithm has
four important properties:
g It provides precise (meet-over-all-interprocedurally-valid-paths) solutions to a large class of problems.
g It has a polynomial worst-case cost for both a single demand and a sequence of all possible demands.
g The worst-case total cost of the sequence of all possible demands is no worse than the worst-case cost
of a single run of the current best exhaustive algorithm.
g Experimental results show that in many situations (e.g., when only a small number of demands are
made, or when most demands are answered yes) the demand algorithm is superior to the current best
exhaustive algorithm.
CR Categories and Subject Descriptors: D.2.2 [Software Engineering]: Tools and Techniques; D.3.4
[Programming Languages]: Processors compilers, optimization; E.1 [Data Structures] graphs; F.2.2
[Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems computations on discrete structures; G.2.2 [Discrete Mathematics]: Graph Theory graph algorithms
General Terms: Algorithms, Experimentation, Theory
Additional Key Words and Phrases: demand dataflow analysis, distributive dataflow framework, graph
reachability, interprocedural dataflow analysis, interprocedurally realizable path, interprocedurally valid
path, meet-over-all-valid-paths solution
On leave from IBM Scientific Center, Haifa, Israel.
This work was supported in part by a David and Lucile Packard Fellowship for Science and Engineering, by the National Science
Foundation under grants CCR-8958530 and CCR-9100424, by the Defense Advanced Research Projects Agency under ARPA Order
No. 8856 (monitored by the Office of Naval Research under contract N00014-92-J-1937), by the Air Force Office of Scientific
Research under grant AFOSR-91-0308, and by a grant from Xerox Corporate Research.
Part of this work was done while the authors were visiting the University of Copenhagen.
A preliminary version of this paper appeared in SIGSOFT 95: Proceedings of the Third ACM SIGSOFT Symposium on Foundations
of Software Engineering (Washington DC, October 10-13, 1995) [15]
Authors' address:   Computer Sciences Department; Univ. of Wisconsin;   1210 West Dayton Street; Madison, WI 53706; USA.
Electronic mail: -horwitz, reps, sagiv-@cs.wisc.edu.
+PAGE+

Maximal-Munch Tokenization in Linear Time
THOMAS REPS
University of Wisconsin
The lexical-analysis (or scanning) phase of a compiler attempts to partition the input stream into a sequence of tokens.
The convention in most languages is that the input is scanned left to right, and each token identified is a maximal
munch of the remaining inputthe longest prefix of the remaining input that is a token of the language. Most textbooks on compiling have extensive discussions of lexical analysis in terms of finite-state automata and regular expressions: Token classes are defined by a set of regular expressions R i , 1 i k, and the lexical analyzer is based on some
form of finite-state automaton for recognizing the language L (R 1 + R 2 + . . . + R k ). However, the treatment is unsatisfactory in one respect: The theory of finite-state automata assumes that the end of the input stringi.e., the right-hand-side boundary of the candidate for recognitionis known a priori, whereas a scanner must identify the next token
without knowing a definite bound on the extent of the token.
Although most of the standard compiler textbooks discuss this issue, the solution they sketch out is one thatfor
certain sets of token definitionscan cause the scanner to exhibit quadratic behavior in the worst case. This property is
not only dissatisfying, it blemishes an otherwise elegant treatment of lexical analysis.
In this paper, we rectify this defect: We show that, given a deterministic finite-state automaton that recognizes the
tokens of a language, maximal-munch tokenization can always be performed in time linear in the size of the input.
CR Categories and Subject Descriptors: D.3.1 [Programming Languages]: Formal Definitions and Theory syntax;
D.3.4 [Programming Languages]: Processors compilers; F.1.1 [Computation by Abstract Devices]: Models of
Computation automata; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and
Problems pattern matching; I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search backtracking, dynamic programming; I.5.4 [Pattern Recognition]: Applications text processing
General Terms: Algorithms, Theory
Additional Key Words and Phrases: memoization, tabulation, tokenization
1. INTRODUCTION

DEVise: Integrated Querying and
Visual Exploration of Large Datasets
M. Livny, R. Ramakrishnan, K. Beyer, G. Chen, D. Donjerkovic,
S. Lawande, J. Myllymaki and K. Wenger
Department of Computer Sciences, University of Wisconsin-Madison
fmiron,raghu,beyer,guangshu,donjerko,ssl,jussi,wengerg@cs.wisc.edu
Abstract
DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentations of large
tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework
is being implemented in a tool that has been already successfully applied to a variety of real applications by a number
of user groups.
Our emphasis is on developing an intuitive yet powerful set of querying and visualization primitives that can be
easily combined to develop a rich set of visual presentations
that integrate data from a wide range of application domains. While DEVise is a powerful visualization tool, its
greatest strengths are the ability to interactively explore a
visual presentation of the data at any level of detail (including retrieving individual data records), and the ability to
seamlessly query and combine data from a variety of local
and remote sources. In this paper, we present the DEVise
framework, describe the current tool, and report on our experience in applying it to several real applications.
1 Introduction

Cost-Aware WWW Proxy Caching Algorithms
Pei Cao Sandy Irani
Department of Computer Science, Information and Computer Science Department,
University of Wisconsin-Madison. University of California-Irvine.
cao@cs.wisc.edu irani@ics.uci.edu
Abstract
Web caches can not only reduce network traffic and
downloading latency, but can also affect the distribution of web traffic over the network through cost-aware caching. This paper introduces GreedyDual-Size, which incorporates locality with cost and size
concerns in a simple and non-parameterized fashion
for high performance. Trace-driven simulations show
that with the appropriate cost definition, GreedyDual-Size outperforms existing web cache replacement algorithms in many aspects, including hit ratios, latency reduction and network cost reduction. In addition, GreedyDual-Size can potentially improve the
performance of main-memory caching of Web documents.
1 Introduction

LATENT VARIABLES, MEASUREMENT ERROR AND
METHODS FOR ANALYZING LONGITUDINAL
ORDINAL DATA
Mari Palta, Chin-Yu Lin,   University of Wisconsin-Madison
Mari Palta,   504 N Walnut Street, Madison, WI 53705
Key Words: Latent variable, Longitudinal ordinal data, Structural equation model
Abstract
We examine the effects of latent variables, between
individual variability and measurement error in the
outcome on commonly used methods for the analysis of longitudinal ordinal data, such as marginal
and cluster-specific models. The impact of such variability becomes very clear when viewed in the context of structural equation modeling (e.g. Muthen
1979, 1983, 1984, 1988). The structural equation
formulation also provides insight into the assumptions and differences in interpretation of regression
coefficients of these various methods of analysis. We
explore the potential for using structural equations
to model random effects and to adjust for measurement error, and in the process compare results
from marginal modeling using a SAS GEE routine
(Karim and Zeger, 1988), Qu's GAUSS program
(Qu, 1992) for generalized mixed models using GEE,
the MIXOR package for cluster-specific mixed effects models (Hedeker and Gibbons, 1994), and LIS-COMP (Muthen, 1988) for structural equation models. These approaches are illustrated in a longitudinal data set of sleep disorders.
Introduction

Aart J.C. Bik
High Performance Computing Division
Department of Computer Science
Leiden University
P.O. Box 9512, 2300 RA Leiden
The Netherlands
ajcbik@cs.leidenuniv.nl
Tel. +31 71 5277037
Submission to PLDI'96
+PAGE+

Practical Comparison of call string and functional Approach in
Data Flow Analysis
Martin Alt Florian Martin
Universitat des Saarlandes,   P.O. Box 151150, 66041 Saarbrucken,
faltjfloriang@cs.uni-sb.de
October 20, 1995
Abstract
The techniques which are used to implement interprocedural data flow analyzers can be generally
divided into two parts: the call string and the functional approach [18]. Both differ in their time and space
complexity as well as in the preciseness due to properties of the abstract domains and transfer functions.
We have developed a data flow analyzer generator PAG [2] which is able to produce interprocedural
analyzers for both techniques. We specified two variants of constant propagation working in an ANSI-C
compiler; a copy constant propagation that uses distributive transfer function and can be solved precisely,
even interprocedurally [13], and a full constant propagator which includes an interpreter for expressions
of the language. We present the practical relevant results applying both analyzers to a rather fair set of
real-world programs and compare the space/time consumption of the analyzers versus their preciseness.
1 Introduction

Kerberos: An Authentication Service for Open Network Systems
Jennifer G. Steiner
Project Athena
Massachusetts Institute of Technology
Cambridge, MA 02139
steiner@ATHENA.MIT.EDU
Clifford Neuman
Department of Computer Science,   FR-35
University of Washington
Seattle, WA 98195
bcn@CS.WASHINGTON.EDU
Jeffrey I. Schiller
Project Athena
Massachusetts Institute of Technology
Cambridge, MA 02139
jis@ATHENA.MIT.EDU
ABSTRACT
In an open network computing environment, a workstation cannot be trusted to
identify its users correctly to network services. Kerberos provides an alternative
approach whereby a trusted third-party authentication service is used to verify users'
identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT's Project Athena. It describes the protocols used by clients, servers, and
Kerberos to achieve authentication. It also describes the management and replication of
the database required. The views of Kerberos as seen by the user, programmer, and
administrator are described. Finally, the role of Kerberos in the larger Athena picture is
given, along with a list of applications that presently use Kerberos for user authentication. We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
Introduction

DEVise: Integrated Querying and
Visual Exploration of Large Datasets
M. Livny, R. Ramakrishnan, K. Beyer, G. Chen, D. Donjerkovic,
S. Lawande, J. Myllymaki and K. Wenger
Department of Computer Sciences, University of Wisconsin-Madison
fmiron,raghu,beyer,guangshu,donjerko,ssl,jussi,wengerg@cs.wisc.edu
Abstract
DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentations of large
tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework
is being implemented in a tool that has been already successfully applied to a variety of real applications by a number
of user groups.
Our emphasis is on developing an intuitive yet powerful set of querying and visualization primitives that can be
easily combined to develop a rich set of visual presentations
that integrate data from a wide range of application domains. While DEVise is a powerful visualization tool, its
greatest strengths are the ability to interactively explore a
visual presentation of the data at any level of detail (includ-ing retrieving individual data records), and the ability to
seamlessly query and combine data from a variety of local
and remote sources. In this paper, we present the DEVise
framework, describe the current tool, and report on our experience in applying it to several real applications.
1 Introduction

Approximate Analysis of Parallel Processor Allocation Policies
Rajesh K. Mansharamani and Mary K. Vernon
(mansha@cs.wisc.edu) (vernon@cs.wisc.edu)
Computer Sciences Department
University of Wisconsin
1210 West Dayton Street
Madison, WI 53706.
November 29, 1993
Abstract
The complexity of parallel applications and parallel processor scheduling policies makes both exact analysis and simulation difficult, if not intractable, for large systems. In this paper we propose a new approach
to performance modeling of multiprogrammed processor scheduling policies, that of interpolation approximations. We first define a workload model that contains parameters for the essential properties of parallel
applications with respect to scheduling discipline performance, yet lends itself to mathematical analysis. Key
features of the workload model include general distribution of total job processing time, general distribution
of available job parallelism, and a simple characterization of parallelism overheads. We then show that one
can find specific values of the system parameters for which the parallel system under a given scheduling
policy reduces to a queueing system with a known (closed-form) solution. Finally, interpolation between
the points with known solutions is used to arrive at mean response time estimates that hold over the entire
system parameter space. The interpolation approximations readily yield insight into policy behavior and are
easy to evaluate for systems with hundreds of processors.
We illustrate the approach by developing and validating models of three scheduling policies, under the
assumptions of linear job execution rates and independence between job parallelism and processing time.
We discuss several insights and results obtained from the analysis of the three policies under the assumed
workloads. One result clarifies and generalizes observations in two previous simulation studies of how policy
performance varies with the coefficient of variation in job processing requirement. Another result of the
interpolation models yields new insight into how policy performance varies with job parallelism. We also
comment on the generalizations of these insights for workloads with less restrictive assumptions.
This research was partially supported by the National Science Foundation under grants CCR-9024144 and CDA-9024618.
+PAGE+

High-Order Accurate Schemes for Incompressible Viscous Flow
John C. Strikwerday
Computer Sciences Department
University of Wisconsin-Madison
Abstract
We present new finite difference schemes for the incompressible Navier-Stokes equations. The schemes are based on two spatial differencing methods, one is fourth-order
accurate and the other is sixth-order accurate. The temporal differencing is based on
backward differencing formulas. The schemes use non-staggered grids and satisfy regularity estimates, guaranteeing smoothness of the solutions. The schemes are computationally
efficient. Computational results demonstrating the accuracy are presented.
Keywords: incompressible Navier-Stokes, finite difference schemes, GMRES.
AMS(MOS) classifications: 65M05, 65N05, 76D05
1. Introduction.

Use of Application Characteristics and Limited Preemption for
Run-To-Completion Parallel Processor Scheduling Policies *
Su-Hui Chiang , Rajesh K. Mansharamani , and Mary K. Vernon
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706.
email: -suhui, vernon-@cs.wisc.edu
TRDDC
1 Mangaldas Road
Pune 411 050, India.
email: mansha@research.trddc.ernet.in
Abstract
The performance potential of run-to-completion (RTC) parallel
processor scheduling policies is investigated by examining
whether (1) application execution rate characteristics such as average parallelism (avg) and processor working set (pws) and/or (2)
limited preemption can be used to improve the performance of
these policies. We address the first question by comparing policies
(previous as well as new) that differ only in whether or not they
use execution rate characteristics and by examining a wider range
of the workload parameter space than previous studies. We address
the second question by comparing a simple two-level queueing
policy with RTC scheduling in the second level queue against
RTC policies that don't allow any preemption and against dynamic
equiallocation (EQ).
Using simulation to estimate mean response times we find that for
promising RTC policies such as adaptive static partitioning (ASP)
and shortest demand first (SDF), a maximum allocation constraint
that is for all practical purposes independent of avg and pws provides greater and more consistent improvement in policy performance than using avg or pws. Also, under the assumption that job
demand information is unavailable to the scheduler we show that
the ASP-max policy outperforms all previous high performance
RTC policies for workloads with coefficient of variation in processing requirement greater than one. Furthermore, a two-level
queue that allows at most one preemption per job outperforms
ASP-max but is not competitive with EQ.
1. Introduction

RD-OPT: An Efficient Algorithm For Optimizing DCT
Quantization Tables
Viresh Ratnakar
University of Wisconsin-Madison
Computer Sciences Department
Madison, WI 53706
Phone: (608) 262-6627
Email: ratnakar@cs.wisc.edu
Miron Livny
University of Wisconsin-Madison
Computer Sciences Department
Madison, WI 53706
Phone: (608) 262-0856
Email: miron@cs.wisc.edu
Abstract
The Discrete Cosine Transform (DCT) is widely used in lossy image and video compression schemes
such as JPEG and MPEG. In this paper we describe RD-OPT, an efficient algorithm for constructing
DCT quantization tables with optimal rate-distortion tradeoffs for a given image. The algorithm uses
DCT coefficient distribution statistics in a novel way and uses a dynamic programming strategy to
produce optimal quantization tables over a wide range of rates and distortions. It can be used to
compress images at any desired signal-to-noise ratio or compressed size.
1 Introduction

Program Generalization for Software Reuse:
From C to C++
Michael Siff
siff@cs.wisc.edu
Thomas Reps
reps@cs.wisc.edu
University of Wisconsin-Madison
1210 West Dayton Street
Madison, WI 53706
Abstract
We consider the problem of software generalization: Given a program component C, create
a parameterized program component C 0 such that C 0 is usable in a wider variety of syntactic
contexts than C. Furthermore, C 0 should be a semantically meaningful generalization of C;
namely, there must exist an instantiation of C 0 that is equivalent in functionality to C.
In this paper, we present an algorithm that generalizes C functions via type inference. The
original functions operate on specific data types; the result of generalization is a collection of
C++ function templates that operate on parameterized types. This version of the generalization
problem is useful in the context of converting existing C programs to C++.
1 Introduction

Interconvertibility of Set Constraints and
Context-Free Language Reachability
David Melski Thomas Reps
November 18, 1996
Abstract
We show the interconvertibility of context-free-language reachability problems and a class
of set-constraint problems: given a context-free-language reachability problem, we show how to
construct a set-constraint problem whose answer gives a solution to the reachability problem;
given a set-constraint problem, we show how to construct a context-free-language reachability
problem whose answer gives a solution to the set-constraint problem. The interconvertibility
of these two formalisms offers an conceptual advantage akin to the advantage gained from the
interconvertibility of finite-state automata and regular expressions in language theory, namely,
a problem can be formulated in whichever formalism is most natural. It also offers some insight into the "O(n 3 ) bottleneck" for different types of program-analysis problems, and allows
results previously obtained for context-free-language reachability problems to be applied to set
constraint problems.
David Melski Thomas Reps
Computer Sciences Department Computer Sciences Department
University of Wisconsin University of Wisconsin
1210 W. Dayton St. 1210 W. Dayton St.
Madison, WI 53706 Madison, WI 53706
608/262-0016 608/262-2091
melski@cs.wisc.edu   608/262-9777 (fax)
reps@cs.wisc.edu
+PAGE+

MULTI-COORDINATION METHODS FOR
PARALLEL SOLUTION OF
BLOCK-ANGULAR PROGRAMS
By
Golbon Zakeri
A thesis submitted in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy
(Computer Sciences and Mathematics)
at the
UNIVERSITY OF WISCONSIN - MADISON
1995
+PAGE+

, , 1-19 ()
c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
A Comparison of Large Scale Mixed
Complementarity Problem Solvers *
STEPHEN C. BILLUPS   sbillups@carbon.cudenver.edu
Mathematics Department, University of Colorado,   Denver, Colorado 80217
STEVEN P. DIRKSE   steve@gams.com
GAMS Development Corporation,   Washington, DC 20007
MICHAEL C. FERRIS   ferris@cs.wisc.edu
Computer Sciences Department, University of Wisconsin,   Madison, Wisconsin 53706
Received Oct 1, 1995; Revised March 22, 1996
Editor:
Abstract. This paper provides a means for comparing various computer codes for solving large
scale mixed complementarity problems. We discuss inadequacies in how solvers are currently
compared, and present a testing environment that addresses these inadequacies. This testing
environment consists of a library of test problems, along with GAMS and MATLAB interfaces that
allow these problems to be easily accessed. The environment is intended for use as a tool by other
researchers to better understand both their algorithms and their implementations, and to direct
research toward problem classes that are currently the most challenging. As an initial benchmark,
eight different algorithm implementations for large scale mixed complementarity problems are
briefly described and tested with default parameter settings using the new testing environment.
Keywords: complementarity problems, variational inequalities, computation, algorithms
1. Introduction

SYNCHRONOUS AND ASYNCHRONOUS MULTI-COORDINATION
METHODS FOR THE SOLUTION OF BLOCK-ANGULAR
PROGRAMS
R.R. MEYER AND G. ZAKERI
Abstract. Several types of multi-coordination methods for block-angular programs are considered. We present a computational comparison of synchronous multi-coordination methods. The most
efficient of these approaches is shown to involve an intermediate number of blocks in the coordination
phase. We also develop a new stabilization algorithm and present asynchronous multi-coordination
schemes, which are particularly useful when the number of blocks exceeds the number of available
processors or when the block sizes vary significantly.
1. Introduction. In this paper we present multi-coordinator synchronous and

PRECONDITIONING FOR REGULAR ELLIPTIC SYSTEMS
Hsing-Hsia Chen
Department of Mathematics
Chung-Yuan Christian University
Chung-Li, 320, Taiwan
and
John C. Strikwerday
Department of Computer Sciences
University of Wisconsin - Madison
Madison, WI
Abstract. In this paper we examine preconditioning operators for regular elliptic
systems of partial differential operators. We obtain general conditions under which the
preconditioned systems are bounded. We also provide some useful guidelines for choosing
left and right preconditioning operators for regular elliptic systems. The condition numbers
of the discrete operators arising from these preconditioned operators are shown to be
bounded independent of grid spacing. Several examples of the two-dimensional regular
elliptic systems are discussed, including scalar elliptic operators and the Stokes operator
with several different boundary conditions. Several preconditioners for these regular elliptic
systems are presented and used in numerical experiments illustrating the theoretical results.
Key word. preconditioning, elliptic systems, numerical methods
AMS subject classifications. 65N06, 65N22
1. Introduction.

An Evaluation of Object Management System
Architectures for Software Engineering Applications
Jayavel Shanmugasundaram, Barbara Staudt Lerner, Lori Clarke
Department of Computer Science
University of Massachusetts
Amherst, MA 01003 USA
+1 413 545 3787
fshan, lerner, clarkeg@cs.umass.edu
ABSTRACT
Software engineering applications require sophisticated
object management system support for creating and
manipulating software objects. One of the key issues for
object management systems is distribution. Addressing this issue in the context of software engineering applications is particularly challenging because they have
widely varying object access profiles. Two fundamental
approaches to dealing with distribution are the object
server architecture, where objects are shipped to the
application program, and the operation server architecture, where operation requests are shipped to where the
objects reside. We compare these architectures experimentally to determine the conditions under which each
performs better.
KEYWORDS
Distributed object management, experimental evaluation
1 INTRODUCTION

Moufang Quasigroups
Kenneth Kunen 1
University of Wisconsin,   Madison, WI 53706, U.S.A.
kunen@cs.wisc.edu
September 5, 1995
ABSTRACT
Each of the Moufang identities in a quasigroup implies that the
quasigroup is a loop.
x1. Introduction.

Fast and Accurate Flow-Insensitive Points-To Analysis
Marc Shapiro and Susan Horwitz
Computer Sciences Department, University of Wisconsin-Madison
1210 West Dayton Street, Madison, WI 53706 USA
Electronic mail: fmds, horwitzg@cs.wisc.edu
Abstract
In order to analyze a program that involves pointers, it
is necessary to have (safe) information about what each
pointer points to. There are many different approaches
to computing points-to information. This paper addresses techniques for flow- and context-insensitive in-terprocedural analysis of stack-based storage.
The paper makes two contributions to work in this
area:
* The first contribution is a set of experiments that
explore the trade-offs between techniques previously defined by Lars Andersen and Bjarne Steens-gaard. The former has a cubic worst-case running
time, while the latter is essentially linear. However, the former may be much more precise than
the latter. We have found that in practice, Ander-sen's algorithm is consistently more precise than
Steensgaard's. For small programs, there is very
little difference in the times required by the two
approaches; however, for larger programs, Ander-sen's algorithm can be much slower than Steens
gaard's.
* The second contribution is the definition of two
new algorithms. The first algorithm can be "tuned"
so that its worst-case time and space requirements,
as well as its accuracy range from those of Steens-gaard to those of Andersen. We have experimented
with several versions of this algorithm; one version
provided a significant increase in accuracy over
Steensgaard's algorithm, while keeping the running time within a factor of two.
The second algorithm uses the first as a subroutine. Its worst-case time and space requirements
are a factor of log N (where N is the number of
variables in the program) worse than those of
Steensgaard's algorithm. In practice, it appears to
This work was supported in part by the National Science Foundation under grant CCR-8958530, and by the Defense Advanced Research Projects Agency under ARPA Order No. 8856 (monitored by
the Office of Naval Research under contract N00014-92-J-1937).
run about ten times slower than Steensgaard's algorithm; however it is significantly more accurate
than Steensgaard's algorithm, and significantly
faster than Andersen's algorithm on large
programs.
1 Introduction

C**:
A Large-Grain, Object-Oriented, Data-Parallel
Programming Language
James R. Larus, Brad Richards, and Guhan Viswanathan 1
Computer Sciences Department
University of Wisconsin-Madison
1210 West Dayton Street
Madison, WI 53706 USA
UW Technical Report #1126
November 24, 1992
1 This work was supported by the National Science Foundation under grants CCR-9101035 and CDA
9024618.
+PAGE+

Storage Estimation for Multidimensional Aggregates in
the Presence of Hierarchies
Amit Shukla Prasad M. Deshpande
Jeffrey F. Naughton Karthikeyan Ramasamy
famit,pmd,naughton,karthikg@cs.wisc.edu
Computer Sciences Department
University of Wisconsin - Madison
Abstract
To speed up multidimensional data analysis,
database systems frequently precompute aggregates on some subsets of dimensions and
their corresponding hierarchies. This improves
query response time. However, the decision of
what and how much to precompute is a difficult one. It is further complicated by the fact
that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the
database. Hence, it is interesting and useful
to estimate the storage blowup that will result from a proposed set of precomputations
without actually computing them. We propose
three strategies for this problem: one based on
sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different
data distributions and database schemas. The
algorithm based upon probabilistic counting is
particularly attractive, since it estimates the
storage blowup to within provable error bounds
while performing only a single scan of the data.
Work supported by an IBM CAS Fellowship, NSF grant IRI-9157357, and a grant from IBM under the University Partnership
Program.
Permission to copy without fee all or part of this material is
granted provided that the copies are not made or distributed for
direct commercial advantage, the VLDB copyright notice and
the title of the publication and its date appear, and notice is
given that copying is by permission of the Very Large Data Base
Endowment. To copy otherwise, or to republish, requires a fee
and/or special permission from the Endowment.
Proceedings of the 22nd VLDB Conference
Mumbai (Bombay), India, 1996
1 Introduction

Towards Effective and Efficient Free Space Management
Mark L. McAuliffe
University of Wisconsin|Madison
mcauliff@cs.wisc.edu
Michael J. Carey
IBM Almaden Research Center
carey@almaden.ibm.com
Marvin H. Solomon
University of Wisconsin|Madison
solomon@cs.wisc.edu
Abstract
An important problem faced by many database management
systems is the "online object placement problem"|the
problem of choosing a disk page to hold a newly allocated
object. In the absence of clustering criteria, the goal is
to maximize storage utilization. For main-memory based
systems, simple heuristics exist that provide reasonable
space utilization in the worst case and excellent utilization
in typical cases. However, the storage management problem
for databases includes significant additional challenges, such
as minimizing I/O traffic, coping with crash recovery, and
gracefully integrating space management with locking and
logging.
We survey several object placement algorithms, including
techniques that can be found in commercial and research
database systems. We then present a new object placement
algorithm that we have designed for use in Shore, an
object-oriented database system under development at the
University of Wisconsin|Madison. Finally, we present
results from a series of experiments involving actual Shore
implementations of some of these algorithms. Our results
show that while current object placement algorithms have
serious performance deficiencies, including excessive CPU
or main memory overhead, I/O traffic, or poor disk
utilization, our new algorithm consistently demonstrates
excellent performance in all of these areas.
1 Introduction

Asking Questions to Minimize Errors
Nader H. Bshouty
Department of Computer Science
The University of Calgary
2500 University Drive N.W.
Calgary, Alberta, Canada T2N 1N4
bshouty@cpsc.ucalgary.ca
Sally A. Goldman
Department of Computer Science
Washington University
St. Louis, MO 63130
sg@cs.wustl.edu
Thomas R. Hancock
Siemens Corporate Research, Inc.
755 College Road East
Princeton, NJ 08540
hancock@learning.siemens.com
Sleiman Matar
Department of Computer Science
The University of Calgary
2500 University Drive N.W.
Calgary, Alberta, Canada T2N 1N4
sleiman@cpsc.ucalgary.ca
September 1993
WUCS-93-23
Abstract
A number of efficient learning algorithms achieve exact identification of an unknown function
from some class using membership and equivalence queries. Using a standard transformation
such algorithms can easily be converted to on-line learning algorithms that use membership
queries. Under such a transformation the number of equivalence queries made by the query
algorithm directly corresponds to the number of mistakes made by the on-line algorithm. In
this paper we consider several of the natural classes known to be learnable in this setting, and
investigate the minimum number of equivalence queries with accompanying counterexamples
(or equivalently the minimum number of mistakes in the on-line model) that can be made by a
learning algorithm that makes a polynomial number of membership queries and uses polynomial
computation time. We are able both to reduce the number of equivalence queries used by the
previous algorithms and often to prove matching lower bounds. As an example, consider the
class of DNF formulas over n variables with at most k = O(log n) terms. Previously, the
algorithm of Blum and Rudich [BR92] provided the best known upper bound of 2 O(k) log n for
the minimum number of equivalence queries needed for exact identification. We greatly improve
on this upper bound showing that exactly k counterexamples are needed if the learner knows k a
priori and exactly k +1 counterexamples are needed if the learner does not know k a priori. This
exactly matches known lower bounds [BC92]. For many of our results we obtain a complete
characterization of the tradeoff between the number of membership and equivalence queries
needed for exact identification. The classes we consider here are monotone DNF formulas, Horn
sentences, O(log n)-term DNF formulas, read-k sat-j DNF formulas, read-once formulas over
various bases, and deterministic finite automata.
+PAGE+

Learning One-Dimensional Geometric Patterns
Under One-Sided Random Misclassification
Noise
Paul W. Goldberg
Department 1423
Sandia National Laboratories,   MS 1110
P.O. Box 5800
Albuquerque, NM 87185-1110
pwgoldb@cs.sandia.gov
Sally A. Goldman
Dept. of Computer Science
Washington University
St. Louis, MO 63130
sg@cs.wustl.edu
WUCS-94-01
May 12, 1994
This research was performed while visiting Washington University. Currently supported by the
U.S. Department of Energy under contract DE-AC04-76AL85000.
Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707.
+PAGE+

Optimal Solution of Off-line and On-line Generalized Caching
Saied Hosseini-Khayat and Jerome R. Cox, Jr.
Washington University in St. Louis
Abstract. Network traffic can be reduced significantly if caching is utilized effectively. As an effort
in this direction we study the replacement problem
that arises in caching of multimedia objects. The size
of objects and the cost of cache misses are assumed
non-uniform. The non-uniformity of size is inherent in multimedia objects, and the non-uniformity of
cost is due to the non-uniformity of size and the fact
that the objects are scattered throughout the network.
Although a special case of this problem, i.e. the case
of uniform size and cost, has been extensively studied, the general case needs a great deal of study. We
present a dynamic programming method of optimally
solving the off-line and on-line versions of this problem, and discuss the complexity of this method.
Key words: Generalized caching, network traffic,
network caching, file caching, optimal replacement,
replacement algorithm.
I. Introduction

Advantages of a Leveled Commitment
Contracting Protocol
Tuomas W. Sandholm and Victor R. Lesser
Computer Science Department
University of Massachusetts at Amherst
CMPSCI Technical Report 95-72
September 7, 1995
+PAGE+

Techniques for Developing and Measuring
High-Performance Web Servers over ATM Networks
James C. Hu , Sumedh Mungee, Douglas C. Schmidt
fjxh,sumedh,schmidtg@cs.wustl.edu
TEL: (314) 935-4215 FAX: (314) 935-7302
Campus Box 1045/Bryan 509
Washington University
One Brookings Drive
St. Louis, MO 63130, USA
This paper has been submitted to the INFOCOM '98 conference.
Abstract
High-performance Web servers are essential to meet the growing demands of the Internet and large-scale intranets. Satisfying these demands requires a thorough understanding of key
factors affecting Web server performance. This paper presents
empirical analysis illustrating how dynamic and static adaptivity can enhance Web server performance. Two research
contributions support this conclusion.
First, the paper presents results from a comprehensive empirical study of Web servers (such as Apache, Netscape Enterprise, PHTTPD, Zeus, and JAWS) over high-speed ATM networks. This study illustrates their relative performance and
precisely pinpoints the server design choices that cause performance bottlenecks. We found that once network and disk
I/O overheads are reduced to negligible constant factors, the
main determinants of Web server performance are its protocol processing path and concurrency strategy. Moreover, no
single strategy performs optimally for all load conditions and
traffic types.
Second, we describe the design techniques and optimizations used to develop JAWS, our high-performance, adaptive
Web server. JAWS is an object-oriented Web server that was
explicitly designed to alleviate the performance bottlenecks
we identified in existing Web servers. It consistently outperforms all other Web servers over ATM networks. The performance optimizations used in JAWS include adaptive pre-spawned threading, fixed headers, cached date processing,
and file caching. In addition, JAWS uses a novel software architecture that substantially improves its portability and flex
This work was funded in part by NSF grant NCR-9628218, Object Technologies International, Eastman Kodak, and Siemens MED.
ibility, relative to other Web servers. Our empirical results
illustrate that highly efficient communication software is not
antithetical to highly flexible software.
1 Introduction

Measuring the Performance of Communication
Middleware on High-Speed Networks
Aniruddha Gokhale and Douglas C. Schmidt
gokhale@cs.wustl.edu and schmidt@cs.wustl.edu
Department of Computer Science, Washington University
St. Louis, MO 63130, USA
An earlier version of this paper appeared in the Proceedings
of the SIGCOMM Conference, 1996, Stanford University,
August, 1996.
Abstract
Conventional implementations of communication middle-ware (such as CORBA and traditional RPC toolkits) incur
considerable overhead when used for performance-sensitive
applications over high-speed networks. As gigabit networks
become pervasive, inefficient middleware will force programmers to use lower-level mechanisms to achieve the necessary
transfer rates. This is a serious problem for mission/life-critical applications (such as satellite surveillance and medical imaging).
This paper compares the performance of several widely
used communication middleware mechanisms on a high-speed ATM network. The middleware ranged from lower-level mechanisms (such as socket-based C interfaces and
C++ wrappers for sockets) to higher-level mechanisms (such
as RPC, hand-optimized RPC and two implementations of
CORBA - Orbix 2.0.1 and ORBeline 2.0). These measurements reveal that the lower-level C and C++ implementations outperform the CORBA implementations significantly
(the best CORBA throughput for remote transfer was roughly
75 to 80 percent of the best C/C++ throughput for sending scalar data types and only around 33 percent for sending structs containing binary fields), and the hand-optimized
RPC code performs slightly better than the CORBA implementations. Our goal in precisely pinpointing the sources of
overhead for communication middleware is to develop scalable and flexible CORBA implementations that can deliver
gigabit data rates to applications.
Keywords: Communication middleware, distributed object computing, CORBA, high-speed networks.
1 Introduction and Motivation

MINIMIZING MEMORY CACHE USAGE FOR MULTIGRID
ALGORITHMS IN TWO DIMENSIONS
CRAIG C. DOUGLAS
Abstract. Computers today rely heavily on good utilization of their cache memory subsystems.
Compilers are optimized for business applications, not scientific computing ones, however. Automatic
tiling of basic numerical algorithms is simply not provided by any compiler. Thus, absolutely terrible
cache performance is normal for scientific computing applications.
Multigrid algorithms combine several numerical algorithms into a more complicated algorithm.
In this paper, an algorithm is derived that allows for data to pass through cache exactly once per
multigrid level during a V cycle before the level changes. This is optimal cache usage for large
problems that do not fit entirely in cache.
The new algorithm would appear to be quite complicated to implement, leading to spaghetti
coding. Actually, an efficient implementation of the algorithm requires a rigid, highly structured
coding style. A coding example is given that is suitable for almost all common discretization methods.
Numerical experiments are provided that show that the new algorithm is up to an integer factor
faster than the traditional implementation method for common multigrid parameter choices.
Key words. multigrid, cache, threads, sparse matrix, iterative methods, domain decomposition,
compiler optimization.
AMS subject classifications. 65N15, 65N10
1. Introduction. Multigrid methods are widely known as the fastest methods

THE DOMAIN REDUCTION METHOD: HIGH WAY REDUCTION
IN THREE DIMENSIONS AND CONVERGENCE WITH INEXACT
SOLVERS
CRAIG C. DOUGLAS AND JAN MANDEL
Abstract. We study a method for parallel solution of elliptic partial differential equations which
decomposes the problem into a number of independent subproblems on subspaces of the underlying
solution space. Using symmetries of the domain, we obtain up to 64 such subproblems for a 3
dimensional cube and the method reduces to a direct solver. In the general case, or when the
subproblems are solved only approximately, the method becomes an iterative method or can be used
as a preconditioner. Bounds on the resulting convergence factors and condition numbers are given.
1. Introduction. In this paper, we approximate the solution to the elliptic

GEMMW: A PORTABLE LEVEL 3 BLAS WINOGRAD VARIANT OF
STRASSEN'S MATRIX-MATRIX MULTIPLY ALGORITHM
CRAIG C. DOUGLAS , MICHAEL HEROUX , GORDON SLISHMAN x AND ROGER M.
SMITH -
Abstract. Matrix-matrix multiplication is normally computed using one of the BLAS or a
reinvention of part of the BLAS. Unfortunately, the BLAS were designed with small matrices in
mind. When huge, well conditioned matrices are multiplied together, the BLAS perform like the
blahs, even on vector machines. For matrices where the coefficients are well conditioned, Winograd's
variant of Strassen's algorithm offers some relief, but is rarely available in a quality form on most
computers. We reconsider this method and offer a highly portable solution based on the Level 3
BLAS interface.
Key Words. Level 3 BLAS, matrix multiplication, Winograd's variant of Strassen's algorithm,
multilevel algorithms
AMS(MOS) subject classification. Numerical Analysis: Numerical Linear Algebra
1. Preliminaries. Matrix-matrix multiplication is a very basic computer oper

Yale University
Department of Computer Science
Dynamic Fault Diagnosis
William Hurwood
YALEU/DCS/TR-1056
December 1994
+PAGE+

Constructing Logic Programs with
Higher-Order Predicates 1
Jtrgen Fischer Nilsson
Department of Computer Science
Technical University of Denmark
Andreas Hamfelt
Computing Science Department
Uppsala University
Abstract: This paper proposes a logic programming approach based on the
application of a system of higher-order predicates put at disposal within ordinary logic programming languages such as prolog. These higher-order
predicates parallel the higher-order functionals or combinators which form an
established part of contemporary functional programming methodology.
The suggested toolbox of higher-order predicates for composing logic programs
is derived from one universal higher-order predicate. They take the form of
recursion operators (in particular for expressing recursion along lists) intended
to cover all commonly occurring recursion schemes in logic programming practice. Their theoretical sufficiency is proved and their practical adequacy is
argued through examples.
The recursion operators, denoting higher-order relations rather than functions, are brought about straightforwardly through a well-known metalogic
programming technique, rendering superfluous the need for special higher-order unification mechanisms.
Keywords: Relational higher-order and metalogic programming. Logic program recursion schemes. Declarative logic programming methodology.
Perche nessuna cosa si puo amare ne odiare,
se prima no si a cognitio di quella.
- Leonardo da Vinci, Notebooks
1 Introduction: Background and Ob jectives

Facilitating Worst-Case Execution Times Analysis for Optimized Code
Jakob Engblom 1 Andreas Ermedahl 1 Peter Altenbernd 2
1 fjakob,ebbeg@docs.uu.se,   Department of Computer Systems (DoCS),
Uppsala University,   P.O. Box 325, S-751 05 Uppsala, Sweden,   Fax: +46-(0)18-550225
2 peter@c-lab.de,   C-LAB,   D-33094 Paderborn, Germany
Abstract
In this paper we present co-transformation, a novel approach to the mapping of execution information from the
source code of a program to the object code for the purpose of worst-case execution time (WCET) analysis. Our
approach is designed to handle the problems introduced by
optimizing compilers, i.e. that the structure of the object
code is very different from the structure of the source code.
The co-transformer allows us to keep track of how different compiler transformations, including optimizations, influence the execution time of a program. This allows us to
statically calculate the execution time of a program at the
object code level, using information about the program execution obtained at the source code level.
1. Introduction

Experience with MPI: 'Converting
pvmmake to mpimake under LAM'
and 'MPI and Parallel Genetic
Programming'
Judith Ellen Devaney
NIST
jdevaney@nist.gov
June 1995
Abstract
This looks at the issues which arose in porting the pvmmake utility
from PVM to MPI. Pvmmake is a PVM application which allows
a user to send files, execute commands, and receive results from a
single machine on any machine in the virtual machine. Its actions are
controlled by the contents of a configuration file. Its most common
use is to enable management of the development of a parallel program
in a heterogeneous environment. A utility with the same features,
mpimake, was coded up to run under LAM.
Genetic programming is an algorithm which evolves an algorithm
in the form of a program to solve your input problem. The implementation under MPI requires the transfer of dynamic data structures
such as lists and trees. This paper discusses the match between the
requirements of this algorithm and the datatype feature in MPI. A
new library, MPI DataStruct is being developed which can transfer
dynamic data structures, created with pointers, without intervention
by the user.
+PAGE+

An Object-Oriented Approach to Interoperation of
Heterogeneous Information Sources
Ling Liu 1 and Calton Pu 2
1 University of Alberta,   Edmonton, Alberta T6G 2H1 Canada
email: lingliu@cs.ualberta.ca
2 Oregon Graduate Institute,   Portland, Oregon 97291-1000 USA
email: calton@cse.ogi.edu
Abstract. In the modern Internet environment, various types of
data sources have become accessible, including multimedia data and web
pages. Some of the fundamental assumptions in traditional databases,
such as the existence of a global schema and data consistency maintained
by a Data Base Administrator, are no longer true in many of the new
data sources. We outline the DIOM [LP95b] object-oriented approach
to build interoperable heterogeneous information systems despite the
absence of global schema and the presence of data inconsistency. We describe the metadata catalog management component of DIOM, a key service in the support for interoperation among heterogeneous information
sources. To support a flexible and customizable conection between information consumers and information producers, DIOM metadata catalog
development utilizes the adaptive specification mechanisms, provided in
DIOM interface description language, for explicit description of information consumers' domain query requirements and for object-oriented
abstractions of information producers' information sources.
1 Introduction

Microkernel Operating System Architecture and Mach
David L. Black David B. Golub Daniel P. Julin Richard F. Rashid
Richard P. Draves Randall W. Dean Alessandro Forin Joseph Barrera
Hideyuki Tokuda Gerald Malan David Bohman
DRAFT of   June 16, 1991
Abstract
Modular architectures based on a microkernel are suitable bases for the design and implementation
of operating systems. Prototype systems employing microkernel architectures are achieving the levels of
functionality and performance expected and required of commercial products. Researchers at Carnegie
Mellon University, the Open Software Foundation, and other sites are investigating implementations of a
number of operating systems (e.g., Unix 1 , MS-DOS 2 ) that use the Mach microkernel. This paper describes
the Mach microkernel, its use to support implementations of other operating systems, and the status of these
efforts.
1 Introduction

Defining and Measuring Conflicts
in Optimistic Replication
John Heidemann Ashvin Goel Gerald Popek
University of California, Los Angeles
Technical report UCLA-CSD-950033
Abstract
Optimistic replication is often viewed as essential for
large scale systems and for supporting mobile computing. In optimistic replication, updates can be made concurrently to different file replicas, resulting in multiple
versions of the file. To recover from these conflicting
updates, after-the fact conflict resolution actions are
required to recombine multiple versions into one. This
paper defines these concepts and discusses approaches
to measure them in optimistically replicated systems.
Measurement of the number of conflicting updates
and conflict resolution is important to judge the practicality of optimistic replication. An environment
where conflicting updates are frequent will not be attractive since users cannot assume they have up-to-date
data. Although many conflicts can be automatically
resolved, some conflicts require user intervention; such
conflicts cannot be too common. This paper shows an
approach to measure the number of conflicting updates.
From this measurement we derive the actual amount of
work done by the user or system to resolve conflicts
and the minimum amount of work required to resolve
conflicts.
1 Introduction

Weak and Strong Beta Normalisations
in Typed -Calculi
Hongwei Xi
Department of Mathematical Sciences
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract. We present a technique to study relations between weak and
strong fi-normalisations in various typed -calculi. We first introduce a
translation which translates a -term into a I-term, and show that a
-term is strongly fi-normalisable if and only if its translation is weakly
fi-normalisable. We then prove that the translation preserves typability
of -terms in various typed -calculi. This enables us to establish the
equivalence between weak and strong fi-normalisations in these typed
-calculi. This translation can deal with Curry typing as well as Church
typing, strengthening some recent closely related results. This may bring
some insights into answering whether weak and strong fi-normalisations
in all pure type systems are equivalent.
1 Introduction

This research is partially supported by DARPA grant N00014-94-1-0845, DARPA contract F19628--95-C-0193, NSF grant CCR-9224375, and grants from Hewlett-Packard, Intel and Tektronix.
Predictable File Access Latency for
Multimedia
D. Revel, C. Cowan, D. McNamee, C. Pu, and J. Walpole
Department of Computer Science and Engineering
Oregon Graduate Institute of Science & Technology
20000 N.W. Walker Rd., P.O. Box 91000
Portland, OR 97291-1000
(503) 690-1121
revel,crispin,dylan,calton,walpole-@cse.ogi.edu
Abstract
Multimedia applications are sensitive to I/O latency and jitter when accessing
data in secondary storage. Transparent adaptive prefetching (TAP) uses software
feedback to provide multimedia applications with file system quality of service
(QoS) guarantees. We are investigating how QoS requirements can be communicated and how they can be met by adaptive resource management. A preliminary test of adaptive prefetching is presented.
Keywords
QoS, adaptive, multimedia, prefetching
1 INTRODUCTION

Realistic Parsing:
Practical Solutions of Difficult Problems
SYLVAIN DELISLE a & STAN SZPAKOWICZ b
a Dpartement de mathmatiques et dinformatique
Universit du Qubec Trois-Rivires
Trois-Rivires, Qubec, Canada G9A 5H7
email: Sylvain_Delisle@uqtr.uquebec.ca,   phone +1 819 376 5125; fax +1 819 376 5185
b Department of Computer Science
University of Ottawa
Ottawa, Ontario, Canada K1N 6N5
email: szpak@csi.uottawa.ca,   phone +1 613 562 5800 ext. 6687; fax +1 613 562 5187
Abstract
This paper describes work on the linguistic
analysis of texts within a project devoted to
knowledge acquisition from text. We focus
on syntactic processing and present some
key elements of the projects parser that
allow it to deal successfully with technical
texts. The parser is fully implemented and
tested on a variety of real texts;
improvements and enhancements are in
progress. Because our knowledge acquisition
method assumes no a priori model of the
domain of the source text, the parser relies
as much as possible on lexical and syntactic
clues. That is why it strives for full
syntactic analysis rather than some form of
text skimming. We present a practical
approach to four acknowledged difficult
problems which to date have no generally
acceptable answers: phrase attachment; time
constraints for problematic input (how to
avoid long and unproductive computation);
parsing conjoined structures (how to
preserve broad coverage without losing
control of the parsing process); and the
treatment of fragmentary input or fragments
that are a byproduct of a fallback parsing
strategy. We review recent related work and
conclude by listing several future work
items.
Key Words
Text processing, knowledge acquisition from
text, broad-coverage parsing, parsing conjoined
structures.
1. Introduction

Verifying a Self-Stabilizing Mutual Exclusion Algorithm
To Appear in the Proceedings of PROCOMET'98
Shaz Qadeer
Department of EECS
University of California at Berkeley
Berkeley CA 94720
Phone: (510)642-1490
shaz@eecs.berkeley.edu
Natarajan Shankar
Computer Science Laboratory
SRI International
Menlo Park CA 94025
Phone: (415)859-5272
shankar@csl.sri.com
Abstract
We present a detailed description of a machine-assisted verification of an algorithm
for self-stabilizing mutual exclusion that is due to Dijkstra [Dij74]. This verification was
constructed using PVS. We compare the mechanical verification to the informal proof
sketch on which it is based. This comparison yields several observations regarding the
challenges of formalizing and mechanically verifying distributed algorithms in general.
1 Introduction

Handling Infeasible Specifications of Cryptographic Protocols
Li Gong
ORA Corporation Cornell University
301A Dates Drive    Dept. of Computer Science
Ithaca, NY 14850 Ithaca, NY 14853
Abstract
In the verification of cryptographic protocols along the
approach of the logic for authentication by Burrows,
Abadi, and Needham, it is possible to write a specification which does not faithfully represent the real world
situation. Such a specification, though impossible or
unreasonable to implement, can go undetected and be
verified to be correct. It can also lead to logical statements that do not preserve causality, which in turn can
have undesirable consequences. Such a specification,
called an infeasible specification here, can be subtle and
hard to locate. This note shows how the logic of cryptographic protocols by Gong, Needham, and Yahalom
can be enhanced with a notion of eligibility to preserve
causality of beliefs and detect infeasible specifications.
It is conceivable that this technique can be adopted in
other similar logics.
1 Introduction

Integrating Security in a Group Oriented Distributed System
Michael Reiter Kenneth Birman Li Gong
Dept. of Computer Science Dept. of Computer Science ORA Corporation
Cornell University Cornell University   675 Massachusetts Ave.
Ithaca, NY 14853 Ithaca, NY 14853 Cambridge, MA 02139
reiter@cs.cornell.edu ken@cs.cornell.edu li@cambridge.oracorp.com
Abstract
A distributed security architecture is proposed for
incorporation into group oriented distributed systems,
and in particular, into the Isis distributed programming toolkit. The primary goal of the architecture is
to make common group oriented abstractions robust
in hostile settings, in order to facilitate the construction of high performance distributed applications that
can tolerate both component failures and malicious attacks. These abstractions include process groups and
causal group multicast. Moreover, a delegation and
access control scheme is proposed for use in group
oriented systems. The focus of the paper is the security architecture; particular cryptosystems and key
exchange protocols are not emphasized.
1 Introduction

Abstract Datatypes in PVS
S. Owre N. Shankar
Computer Science Laboratory
SRI International
Menlo Park CA 94025
Phone: (415)859-5272
fowre, shankarg@csl.sri.com
URL: http://www.csl.sri.com/sri-csl-fm.html
1 The development of PVS was funded by internal research funding from SRI International. Support for the preparation of this document came from the National Aeronautics and Space Administration Langley Research Center under Contract NAS1-18969. This report is a revised and updated

Toward a MAC policy framework
Xiaolei Qian Teresa F. Lunt
SRI International ARPA/ITO
333 Ravenswood Avenue 3701 North Fairfax Drive
Menlo Park, CA 94025 Arlington, VA 22203
qian@csl.sri.com tlunt@arpa.mil
Abstract
We propose a formal policy framework of MAC policies in multilevel relational databases.
We identify the important components of such policies and their desirable properties. The
framework provides a basis for systematically specifying such policies and characterizing
their potential mismatches. Based on the framework, we compare and unify the MAC
policies and policy components that are proposed in the literature or imposed in existing
systems. Our framework could be used to capture and resolve MAC policy mismatches in
the trusted interoperation of heterogeneous multilevel relational databases.
Keywords
Heterogeneity, interoperation, mandatory access control, multilevel security, relational
database, security policy
1 INTRODUCTION

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 7(5), OCTOBER 1995 1
Enriching the Expressive Power of Security Labels
Li Gong and Xiaolei Qian
Abstract| Common security models such as Bell-LaPadula
focus on the control of access to sensitive data but leave
some important systems issues unspecified, such as the implementation of read-only objects, garbage collection, and
object upgrade and downgrade paths. Consequently, different implementations of the same security model may have
conflicting operational and security semantics. We propose
the use of more expressive security labels for specifying these
system issues within the security model, so that the semantics of a system design are precisely understood and are
independent of implementation details.
Keywords| Data security, garbage collection, multilevel
security, object label, read-only object.
I. Introduction

Using Linguistic Phenomena to Motivate a Set of Rhetorical
Relations
Alistair Knott
Department of Artificial Intelligence, University of Edinburgh
80 South Bridge, Edinburgh EH1 1HN, Scotland
Email: A.Knott@ed.ac.uk
Robert Dale
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, Scotland
Email: R.Dale@ed.ac.uk
May 5, 1993
Running head: Motivating Rhetorical Relations
+PAGE+

Indexing PROLOG Procedures into DAGs
by Heuristic Classification
Michael Sintek
DFKI
Postfach 2080
67608 Kaiserslautern
Germany
May 5, 1994
Abstract
This paper first gives an overview of standard PROLOG indexing and
then shows, in a step-by-step manner, how it can be improved by slightly
extending the WAM indexing instruction set to allow indexing on multiple
arguments. Heuristics are described that overcome the difficulty of computing the indexing WAM code. In order to become independent from a
concrete WAM instruction set, an abstract graphical representation based
on DAGs (called DAXes) is introduced.
The paper includes a COMMON LISP listing of the main heuristics
implemented; the algorithms were developed for RELFUN, a relational-plus-functional language, but can easily be used in arbitrary PROLOG
implementations.
The ideas described in this paper were first presented at the Workshop
"Sprachen fur KI-Anwendungen, Konzepte - Methoden Implementierun-gen" 1992 in Bad Honnef [SS92]. This paper is part of a collaborative work
together with Werner Stein [Ste92].
+PAGE+

A Relational-Functional Integration
for Declarative Programming
Harold Boley
DFKI
Box 2080, 67608 Kaiserslautern, Germany
boley@informatik.uni-kl.de
Abstract. A relational-functional kernel language is introduced that
integrates essential declarative constructs: logic variables and non-determinism from the relational paradigm with nested and higher-order operations from the functional paradigm. Operator definitions use
`valued clauses', subsuming relational Horn clauses and functional
(conditional or unconditional) directed equations. Their semantics complements the atoms in relational Herbrand models by `molecules', which
pair functions, applied to argument terms, with returned-value terms.
All abstract notions are illustrated by concrete declarative programs.
1 Introduction

Layer Reassignment for Antenna Effect
Minimization in 3-Layer Channel Routing
Zhan Chen and Israel Koren
Department of Electrical and Computer Engineering
University of Massachusetts,   Amherst, MA 01003
Abstract
As semiconductor technology enters the deep submicron era, reliability has
become a major challenge in the design and manufacturing of next generation
VLSI circuits. In this paper we focus on one reliability issue the antenna
effect in the context of 3-layer channel routing. We first present an antenna effect model in 3-layer channel routing and, based on this, an antenna effect cost
function is proposed. A layer reassignment approach is adopted to minimize this
cost function and we show that the layer reassignment problem can be formulated as a network bipartitioning problem. Experimental results show that the
antenna effect can be reduced considerably by applying the proposed technique.
Compared with previous work, one advantage of our approach is that no extra
channel area is required for antenna effect minimization. We show that layer
reassignment technique can be used in yield-related critical area minimization
in 3-layer channel routing as well. The trade-off between these two objectives is
also presented.
1: Introduction

Crosstalk Minimization in Three-Layer
HVH Channel Routing
Zhan Chen and Israel Koren
Department of Electrical and Computer Engineering
University of Massachusetts,   Amherst, MA 01003, USA
Abstract
Crosstalk has become a major issue in VLSI design due to the high frequency, long
interconnecting lines and small spacing between interconnects in today's integrated circuits.
In this paper, we study the problem of crosstalk minimization in 3-layer HVH channel
routing. A heuristic algorithm that combines layer reassignment and track reassignment
is presented. This algorithm can iteratively modify the layout so that the crosstalk in the
channel is minimized. Experimental results show that the proposed approach can reduce the
crosstalk by an average of 16.4% on a set of benchmark examples.
1: Introduction

Optimal Routing Control: Game Theoretic Approach
Richard J. La, and Venkat Anantharam
Department of Electrical Engineering and Computer Science
University of California at Berkeley
hyongla@eecs.berkeley.edu, ananth@eecs.berkeley.edu
Abstract
Communication networks shared by selfish users are
considered and modeled as noncooperative repeated
games. Each user is interested only in optimizing its
own performance by controlling the routing of its load.
We investigate the existence of a NEP that achieves
the system-wide optimal cost. The existence of a NEP
that not only achieves the system-wide optimal cost but
also yields a cost for each user no greater than its stage
game NEP cost is shown for two-node multiple link networks. It is shown that more general networks where
all users have the same source-destination pair have
a NEP that achieves the minimum total system cost
under a mild technical condition. It is shown general
networks with users having multiple source-destination
pairs don't necessarily have such an NEP.
1 Introduction

The Common Randomness Capacity of a
Pair of Independent Discrete Memoryless
Channels
S. Venkatesan V. Anantharam zx
13 August 1995
Abstract
We study the following problem: two agents Alice and Bob are connected to each other by independent discrete memoryless channels. They
wish to generate common randomness by communicating interactively over
the two channels. Assuming that Alice and Bob are allowed access to
independent external random sources at rates (in bits per step of communication) of H A and H B , respectively, we show that they can generate common randomness at a rate of max f min [H A + H(W jQ); I(P ; V )] +
min [H B + H(V jP ); I(Q; W )] g bits per step. Here, V is the channel from
Alice to Bob, and W is the channel from Bob to Alice. The maximum
is over all probability distributions P and Q on the input alphabets of V
and W respectively. We also prove a strong converse which establishes the
above rate as the highest attainable in this situation.
Keywords: Common randomness capacity, generating randomness from
noise, interactive communication.
Research supported by NSF IRI 9005849, IRI 9310670, NCR 9422513, and the AT&T
Foundation.
Cornell University and U.C. Berkeley.
Univ. of California, Berkeley.
x Address all correspondence to the second author:   570 Cory Hall, Dept. of EECS, U.C.
Berkeley, Berkeley, CA 94720.
+PAGE+

TIME-FREQUENCY SIGNAL MODELS FOR MUSIC ANALYSIS,
TRANSFORMATION, AND SYNTHESIS
Michael Goodwin Martin Vetterli
Department of Electrical Engineering and Computer Science & Center for New Music and Audio Technologies
University of California at Berkeley
ABSTRACT
In signal analysis-synthesis, the analysis derives a set of parameters that the synthesis uses to reconstruct the original
signal. In musical applications, this reconstruction should
be perceptually accurate, and the parameterization should
allow for such desirable signal modifications as time-scaling,
pitch-shifting, and cross-synthesis; the analysis parameters
should correspond to a signal model that is flexible enough
to allow these transformations. Sinusoidal modeling meets
this flexibility requirement, but has difficulty representing
some salient features of musical signals such as attack transients and noiselike processes. In this paper, sinusoidal
modeling is reviewed and some variations are proposed to
account for its shortcomings; also, wavelet-based representations of musical signals are considered.
1. SIGNAL MODELING FOR MUSIC

Reactive Modules
Rajeev Alur Thomas A. Henzinger
Abstract. We present a formal model for concurrent systems. The model represents
synchronous and asynchronous components in a uniform framework that supports compositional (assume-guarantee) and hierarchical (stepwise-refinement) design and verification. While synchronous models are based on a notion of atomic computation step,
and asynchronous models remove that notion by introducing stuttering, our model is
based on a flexible notion of what constitutes a computation step: by applying an abstraction operator to a system, arbitrarily many consecutive steps can be collapsed into
a single step. The abstraction operator, which may turn an asynchronous system into a
synchronous one, allows us to describe systems at various levels of temporal detail. For
describing systems at various levels of spatial detail, we use a hiding operator that may
turn a synchronous system into an asynchronous one. We illustrate the model with diverse examples from synchronous circuits, asynchronous shared-memory programs, and
synchronous message-passing protocols.
1 Introduction

Capacity, Mutual Information, and Coding
for Finite-State Markov Channels
Andrea J. Goldsmith, Member, IEEE and Pravin P. Varaiya, Fellow, IEEE
Abstract
The Finite-State Markov Channel (FSMC) is a discrete-time varying channel whose variation is determined by a finite-state Markov process. These channels have memory due to the
Markov channel variation. We obtain the FSMC capacity as a function of the conditional channel
state probability. We also show that for i.i.d. channel inputs, this conditional probability converges
weakly, and the channel's mutual information is then a closed-form continuous function of the input
distribution.
We next consider coding for FSMCs. In general, the complexity of maximum-likelihood
decoding grows exponentially with the channel memory length. Therefore, in practice, interleaving
and memoryless channel codes are used. This technique results in some performance loss relative
to the inherent capacity of channels with memory. We propose a maximum-likelihood decision-feedback decoder with complexity that is independent of the channel memory. We calculate the
capacity and cutoff rate of our technique, and show that it preserves the capacity of certain FSMCs.
We also compare the performance of the decision-feedback decoder with that of interleaving and
memoryless channel coding on a fading channel with 4PSK modulation.
Index Terms: Finite-State Markov Channels, Capacity, Mutual Information, Decision-Feedback
Maximum-Likelihood Decoding.
Work supported in part by an IBM graduate fellowship, and in part by the PATH program, Institute of Transportation Studies, University of California, Berkeley.
A. Goldsmith is with the Department of Electrical Engineering, California Institute of Technology, Pasadena, CA
91125.
P. Varaiya is with the Department of Electrical Engineering and Computer Science, University of California,
Berkeley, CA 94720.
+PAGE+

What's Decidable about Hybrid Automata?
Thomas A. Henzinger 2 Peter W. Kopke 2 Anuj Puri 3 Pravin Varaiya 3
Abstract. Hybrid automata model systems with both
digital and analog components, such as embedded control programs. Many verification tasks for such programs
can be expressed as reachability problems for hybrid automata. By improving on previous decidability and undecidability results, we identify the precise boundary between decidability and undecidability of the reachability
problem for hybrid automata.
On the positive side, we give an (optimal) PSPACE
reachability algorithm for the case of initialized rectangular automata, where all analog variables follow trajectories within piecewise-linear envelopes and are reinitialized
whenever the envelope changes. Our algorithm is based
on a translation of an initialized rectangular automaton
into a timed automaton that defines the same timed language. The translation has practical significance for verification, because it guarantees the termination of symbolic
procedures for the reachability analysis of initialized rectangular automata.
On the negative side, we show that several slight generalizations of initialized rectangular automata lead to an
undecidable reachability problem. In particular, we prove
that the reachability problem is undecidable for timed automata with a single stopwatch.
1 Introduction

An Analysis of Geographical Push-Caching +L +
James Gwertzman,   Microsoft Corporation
Margo Seltzer,   Harvard University
Abstract
Most caching schemes in wide-area, distributed systems are client-initiated. Decisions of when and
where to cache information are made without the benefit of the server's global knowledge of the usage
patterns. In this paper, we present a new caching strategy: geographical push-caching. Using the server's
global knowledge and a derived network topology, we distribute data to cooperating servers. The World
Wide Web is an example of a wide-area system that will benefit from distance-sensitive caching, and we
present an architecture that allows a Web server to autonomously replicate HTML pages. We use a trace-driven simulation to evaluate several competing caching strategies. Our results show that geographical
push-caching reduces bandwidth consumption and sever load by the same amount as web proxy caching,
but with a savings in global cache space of almost two orders of magnitude. More importantly, servers
that wish to reduce Internet bandwidth consumption and their load can do so without waiting for web
proxies to be implemented world-wide. Furthermore, geographical push-caching helps distribute server
load for all web servers, not just the most popular as is the case with proxy caching.
1 Introduction

To appear in the 10th International Parallel Processing Symposium, April 1996. 1
Modeling the Communication Performance of the IBM SP2
Gheith A. Abandah Edward S. Davidson
Advanced Computer Architecture Laboratory, Department of EECS
University of Michigan
1301 Beal Avenue, Ann Arbor, MI 48109-2122
gabandah,davidson@eecs.umich.edu
Abstract
The objective of this paper is to develop models that
characterize the communication performance of a message-passing multicomputer by taking the IBM SP2 as a case
study. The paper evaluates and models the three aspects
of the communication performance: scheduling overhead,
message-passing time, and synchronization overhead. Performance models are developed for the basic communication patterns, enabling the estimation of the communication
times of a message-passing application. Such estimates facilitate activities such as application tuning, selection of the
best available implementation technique, and performance
comparisons among different multicomputers.
1. Introduction

Efficient Formulation for Optimal Modulo Schedulers
Alexandre E. Eichenberger Edward S. Davidson
ECE Department EECS Department
North Carolina State University University of Michigan
Raleigh, NC 27695-7911 Ann Arbor, MI 48109-2122
alexe@eos.ncsu.edu davidson@eecs.umich.edu
Abstract
Modulo scheduling algorithms based on optimal
solvers have been proposed to investigate and tune the
performance of modulo scheduling heuristics. While
recent advances have broadened the scope for which
the optimal approach is applicable, this approach
increasingly suffers from large execution times. In this
paper, we propose a more efficient formulation of the
modulo scheduling space that significantly decreases
the execution time of solvers based on integer linear
programs. For example, the total execution time is
reduced by a factor of 8.6 when 782 loops from the
Perfect Club, SPEC, and Livermore Fortran Kernels
are scheduled for minimum register requirements using
the more efficient formulation instead of the traditional
formulation. Experimental evidence further indicates
that significantly larger loops can be scheduled under
realistic machine constraints.
1 Introduction

A Chromaticity Space for Specularity, Illumination
Color- and Illumination Pose-Invariant 3-D Object
Recognition
Daniel Berwick and Sang Wook Lee
(dberwick@umich.edu and swlee@umich.edu)
Dept. of Electrical Enginnering and Computer Science
University of Michigan
Ann Arbor, MI 48104
Abstract
Most of the recent color recognition/indexing approaches concentrate on establishing invariance to illumination color to improve the utility of color recognition. However, other effects caused by illumination pose and specularity on three-dimensional
object surfaces have not received notable attention. We present a chromaticity recognition method that discounts the effects of illumination pose, illumination color and
specularity. It utilizes a chromaticity space based on log-ratio of sensor responses for
illumination pose and color invariance. A model-based specularity detection/rejection
algorithm can be used to improve the chromaticity recognition and illumination estimation for objects including specular reflections.
+PAGE+

A Scalable Key Distribution Hierarchy
Patrick McDaniel Sugih Jamin
Electrical Engineering and Computer Science Department
University of Michigan
Ann Arbor, MI 48109-2122
fpdmcdan,jaming@eecs.umich.edu
April 13, 1998
Abstract
As the use of the Internet for electronic commerce, audio
and video conferencing, and other applications with sensitive content grows, the need for secure services becomes
critical. Central to the success of these services is the support for secure public key distribution. Although there are
several existing services available for this purpose, they
are not very scalable, either because they depend on a centralized server or rely on ad hoc trust relationships.
In this paper, we present and examine a flexible approach to certificate distribution scalable to arbitrarily
large networks. We propose a two level hierarchy where
certificates can be independently authenticated by one or
more peer authorities, called keyservers. Certificates for
end-user and host entities are managed within local domains, called enterprises. By administering certificates
close to the source, we reduce the load on the key servers
and the effects of network topology changes. We describe
the design of our system and present a preliminary performance analysis based on traces of present-day DNS requests.
1 Introduction

To Appear in AI Magazine,
Summer/Fall 1994.
An Introduction to Least Commitment
Planning
Daniel S. Weld 1
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
weld@cs.washington.edu
Abstract
Recent developments have clarified the process of generating partially ordered, partially specified sequences of actions whose execution
will achive an agent's goal. This paper summarizes a progression of
least commitment planners, starting with one that handles the sim
ple strips representation, and ending with one that manages actions
with disjunctive precondition, conditional effects and universal quantification over dynamic universes. Along the way we explain how
Chapman's formulation of the Modal Truth Criterion is misleading
and why his NP-completeness result for reasoning about plans with
conditional effects does not apply to our planner.
1 I thank Franz Amador, Tony Barrett, Darren Cronquist, Denise Draper, Ernie Davis,

A Novel Framework for Decentralized Supervisory Control with Communication
George Barrett
grbarret@eecs.umich.edu
Stephane Lafortune
stephane@eecs.umich.edu
Department of Electrical Engineering and Computer Science
The University of Michigan
1301 Beal Avenue
Ann Arbor, MI 48109-2122
ABSTRACT
The decentralized control problem that we address in
this paper is that of several communicating supervisory
controllers, each with different information, working in
concert to exactly achieve a given legal sublanguage of
the uncontrolled system's language model. We present
a novel information structure formalism for dealing with
this class of problems. Preliminary results are presented
which elucidate a fundamental concept in decentralized
control problems: the importance of controllers anticipating future possible communications.
1. INTRODUCTION

MDARTS: A Multiprocessor Database Architecture for Real-Time
Systems
Real-Time Computing Laboratory
Computer Science and Engineering Division
Department of Electrical Engineering and Computer Science
The University of Michigan
Ann Arbor, Michigan 48109-2122
(313) 763-0391
ABSTRACT
Some of the advanced real-time systems being proposed, such as the Next Generation Workstation/Machine
Controller (NGC) for automated factories, require a built-in database to support concurrent data access and provide
well-defined interfaces between software modules. However, conventional database systems do not provide the performance levels or response time guarantees needed by real-time applications. To address the need for high-performance
real-time database systems, we propose to design, implement, and evaluate an object-oriented software system called
Multiprocessor Database Architecture for Real-Time Systems (MDARTS). An important feature of MDARTS is that
it supports explicit specification of real-time requirements and semantic constraints at an object-granularity level.
The database examines these specifications at runtime during application initialization and dynamically adjusts
its data management strategy accordingly to provide hard real-time guarantees. For maximum performance on
shared-memory multiprocessors, MDARTS supports concurrent, direct, shared-memory data access. Prior real-time
database systems do not support per-object dynamic configuration during initialization, and they either work only on
uniprocessors or use relatively slow inter-process communication for all transactions. The unique design of MDARTS
will support a transaction execution time two to three orders of magnitude faster than current real-time database
systems for multiprocessors. For data access with less stringent timing constraints, MDARTS also supports remote
transactions across networks and provides interfaces to external database systems. Once we have implemented the
basic MDARTS architecture, we will demonstrate its capabilities by using it to develop distributed, open architecture
controllers for actual manufacturing machine tools.
+PAGE+

Internet Routing Instability
Craig Labovitz, G. Robert Malan, and Farnam Jahanian
University of Michigan
Department of Electrical Engineering and Computer Science
1301 Beal Ave.
Ann Arbor, Michigan 48109-2122
flabovit, rmalan, farnamg@eecs.umich.edu
Abstract
This paper examines the network inter-domain routing information exchanged between backbone service providers at
the major U.S. public Internet exchange points. Internet
routing instability, or the rapid fluctuation of network reach-ability information, is an important problem currently facing the Internet engineering community. High levels of network instability can lead to packet loss, increased network
latency and time to convergence. At the extreme, high levels of routing instability have lead to the loss of internal
connectivity in wide-area, national networks. In this paper,
we describe several unexpected trends in routing instability,
and examine a number of anomalies and pathologies observed in the exchange of inter-domain routing information.
The analysis in this paper is based on data collected from
BGP routing messages generated by border routers at five
of the Internet core's public exchange points during a nine
month period. We show that the volume of these routing updates is several orders of magnitude more than expected and
that the majority of this routing information is redundant,
or pathological. Furthermore, our analysis reveals several
unexpected trends and ill-behaved systematic properties in
Internet routing. We finally posit a number of explanations
for these anomalies and evaluate their potential impact on
the Internet infrastructure.
1 Introduction

Spheres of Control:
An Approach to Advanced Recovery
C. Wallace N. Soparkar
Electrical Engineering & Computer Science
The University of Michigan
Ann Arbor, MI 48109-2122
USA
fwallace,soparkarg@eecs.umich.edu
Abstract
Recovery from failures and erroneous executions is a crucial but complicated issue for concurrently accessed data systems. Increasingly sophisticated techniques are being developed to improve performance
and functionality of recovery protocols. To better understand and analyze recovery schemes, we reexamine the concept of spheres of control [Dav78], using it as a unifying framework for specifying diverse
recovery models simply and precisely. We constrain sphere-of-control formulations appropriately to capture transaction-oriented recovery in both centralized and distributed environments and with different
types of schedules, as well as semantics-based recovery and compensation. In addition, we discuss how
the operational semantics methodology of evolving algebras [Gur95] can model spheres of control formally
and refine them to lower levels of abstraction.
1 Introduction

Distributed Pipeline Scheduling: End-to-End Analysis of
Heterogeneous, Multi-Resource Real-Time Systems
Saurav Chatterjee and Jay Strosnider
Department of Electrical & Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
This research was supported in part by a grant from the Office
of Naval Research, in part by a grant from the Naval Research
and Development Laboratory, and in part by a grant from
Siemens Corporate Research.
In 15th IEEE International Conference on Distributed Computing Systems, May 1995.
Abstract
This paper presents an hierarchical end-to-end
analysis technique that decomposes the very complex
heterogeneous multi-resource scheduling problem into
a set of single resource scheduling problems with well
defined interactions. We define heterogeneity both in
resource types, e.g., CPU, and in scheduling policies,
e.g., rate-monotonic scheduling. This analysis
technique is one phase of our systems integration
framework for designing large-scale, heterogeneous,
distributed real-time systems whose timing properties
can be strictly controlled and analyzed. This approach,
denoted the Distributed Pipelining Framework,
exploits the natural pipelining execution pattern found
in a large number of continuous (periodic) applications
executing over heterogenous resources. A teleconference application is used in this paper to show the
utility of the approach.
1. Introduction

Modelling rational inquiry in non-ideal agents
Antonio Moreno
Escola Tecnica Superior d'Enginyeria
Departament d'Enginyeria Informatica
Universitat Rovira i Virgili (URV)
Carretera de Salou, s/n. 43006-Tarragona
amoreno@etse.urv.es
Abstract
The construction of rational agents is one of the goals that has been pursued in
Artificial Intelligence (AI). In most of the architectures that have been proposed for
this kind of agents, its behaviour is guided by its set of beliefs. In our work, rational
agents are those systems that are permanently engaged in the process of rational
inquiry; thus, their beliefs keep evolving in time, as a consequence of their internal
inference procedures and their interaction with the environment. Both AI researchers
and philosophers are interested in having a formal model of this process, and this is
the main topic in our work.
Beliefs have been formally modelled in the last decades using doxastic logics. The
possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically
omniscient and perfect reasoners. We avoid these problems by replacing possible
worlds by conceivable situations, which are all the situations that the modelled agent
is capable of considering.
In this document we show how this notion of conceivable situations may be used
to model the process of rational inquiry in which a non-ideal rational agent is engaged. We define a wide class of agents, called rational inquirers, which are a general
abstraction of any kind of non-ideal agent. We show how the beliefs of these kind of
agents evolve in time as a consequence of a multi-dimensional belief analysis, and we
use the framework of conceivable situations in order to model this evolution.
Keywords: rational inquiry, doxastic logics, logical omniscience, perfect reasoning, possible
worlds, Kripke semantics, dynamic multi-dimensional belief analysis
+PAGE+

Some Geographical Applications of
Genetic Programming on the Cray T3D
Supercomputer
I. Turton, S. Openshaw and G. Diplock
School of Geography, University of Leeds,   Leeds, UK
email: ian@geog.leeds.ac.uk, stan@geog.leeds.ac.uk, gary@geog.leeds.ac.uk
April 15, 1996
Abstract
The paper describes some geographical applications of a parallel GP code
which is run on a Cray T3D 512 processor supercomputer to create new
types of well performing mathematical models. A series of results are described which allude to the potential power of the method for which there
are many practical applications in spatial data rich environments where
there are no suitable existing models and no soundly based theoretical
framework on which to base them.
1 Introduction

Simulation of Simplicity:
A Technique to Cope with Degenerate Cases
in Geometric Algorithms 1
Herbert Edelsbrunner 2 and Ernst Peter Mucke 2
Abstract
This paper describes a general-purpose programming technique, called the Simulation of
Simplicity, which can be used to cope with degenerate input data for geometric algorithms.
It relieves the programmer from the task to provide a consistent treatment for every single
special case that can occur. The programs that use the technique tend to be considerably
smaller and more robust than those that do not use it. We believe that this technique will
become a standard tool in writing geometric software.
Keywords: Computational geometry, degenerate data, implementation, programming
tool, perturbation, determinants, symbolic computation.
ACM Transactions on Graphics, 9(1):66-104, 1990.
1 Research of both authors was supported by Amoco Foundation Faculty Development Grant CS 1-6-44862. It

, , 1-30 ()
c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
On the Optimality of the Simple Bayesian
Classifier under Zero-One Loss
PEDRO DOMINGOS   pedrod@ics.uci.edu
MICHAEL PAZZANI   pazzani@ics.uci.edu
Department of Information and Computer Science, University of California,   Irvine, CA 92697
Editor: Gregory Provan
Abstract. The simple Bayesian classifier is known to be optimal when attributes are independent
given the class, but the question of whether other sufficient conditions for its optimality exist has
so far not been explored. Empirical results showing that it performs surprisingly well in many
domains containing clear attribute dependences suggest that the answer to this question may be
positive. This article shows that, although the Bayesian classifier's probability estimates are only
optimal under quadratic loss if the independence assumption holds, the classifier itself can be
optimal under zero-one loss (misclassification rate) even when this assumption is violated by a
wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian
classifier has a much greater range of applicability than previously thought. For example, in this
article it is shown to be optimal for learning conjunctions and disjunctions, even though they
violate the independence assumption. Further, studies in artificial domains show that it will often
outperform more powerful classifiers for common training set sizes and numbers of attributes, even
if its bias is a priori much less appropriate to the domain. This article's results also imply that
detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier,
and this is also verified empirically.
Keywords: Simple Bayesian classifier, naive Bayesian classifier, zero-one loss, optimal classification, induction with attribute dependences
1. Introduction

STATISTICAL LANGUAGE MODELING FOR SPEECH DISFLUENCIES
Andreas Stolcke Elizabeth Shriberg
Speech Technology and Research Laboratory
SRI International,   Menlo Park, CA 94025
stolcke@speech.sri.com
ees@speech.sri.com
ABSTRACT
Speech disfluencies (such as filled pauses, repetitions, restarts) are
among the characteristics distinguishing spontaneous speech from
planned or read speech. We introduce a language model that predicts disfluencies probabilistically and uses an edited, fluent context
to predict following words. The model is based on a generalization
of the standard N-gram language model. It uses dynamic programming to compute the probability of a word sequence, taking into
account possible hidden disfluency events. We analyze the model's performance for various disfluency types on the Switchboard
corpus. We find that the model reduces word perplexity in the
neighborhood of disfluency events; however, overall differences
are small and have no significant impact on recognition accuracy.
We also note that for modeling of the most frequent type of dis-fluency, filled pauses, a segmentation of utterances into linguistic
(rather than acoustic) units is required. Our analysis illustrates a
generally useful technique for language model evaluation based on
local perplexity comparisons.
1. MOTIVATION AND OVERVIEW

INTERNATIONAL COMPUTER SCIENCE INSTITUTE
I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198 *   (510) 643-9153 * FAX (510) 643-7684
Some MPEG Decoding Functions
on Spert
An Example for Assembly
Programmers
Arno Formella
TR-94-027
October 1994
Abstract
We describe our method how to implement C-program sequences in torrent (T0)
assembler code while there is no efficient automatic tool. We use re-structuring of
the source code, vectorization, dataflow graphs, a simple scheduling strategy and a
straight forward register allocation algorithm. We define some lower and an upper
bound for the expected run time. For two functions, namely the color transformation
and reverse DCT, we achieve almost 54, respectively 16 times the performance of a
Sparc 2 workstation.
+PAGE+

Getting a Grip: A Computational Model of the
Acquisition of Verb Semantics for Hand Actions
David Bailey
Computer Science Division, Univ. of California at Berkeley
and International Computer Science Institute
1947 Center St. Suite 600, Berkeley CA 94704
dbailey@icsi.berkeley.edu
Abstract
We present a computational model of how verbs might be learned within
the limited domain of hand actions. We hypothesize that such verbs refer to
the activities of underlying motor schemas, and leverage this constraint to
build a system with strong enough biases that it can learn from a reasonably
small number of examples, while still having adequate flexibility to learn the
hand-action verbs of any language. The completed system should demonstrate its knowledge both by labelling its own behavior and by carrying out
verbal commands in a simulated world.
1 Overview

Storage of Two and Three Dimensional Raster Type Data for
Optimized Retrieval of One, Two or Three Dimensional Features
Kjell Bratbergsengen
Department of Computer Systems and Information Science
Norwegian University of Science and Technology,   Trondheim, Norway
email: kjellb@idi.ntnu.no
Abstract
We are analyzing storage structures for two and three dimensional raster type data which are used
for feature retrieval. The features are one, two or three dimensional objects with regular outlines
like a rectangle or a prism. The features could be parts of a map or image, an area of special
interest for searching after oil, a sequence of ultra sound images, and so on. The storage medium
is magnetic disk. The data are stored in chunks or blocks representing a regular part of the source
object. We analyze the shape and the size to minimize the cost of retrieval. The optimization is
based on minimum time to do retrieval. We have five combinations: Lines and areas from areas and
volumes, and volumes from volumes. The optimal block sizes for random retrieval varies, with case,
feature size and disk characteristics. One general observation is that longer disk tracks gives larger
blocks. For line retrieval the optimal block size is only depending on disk track length. For other
cases it is also depending on the feature size. For partly sequential retrieval the block size is not the
actual block size used during retrieval, but the smallest addressing unit, and the optimal addressing
unit could be rather small.
The analysis reveals that using too small blocks could be very costly. The time could easily
double or triple if small blocks are used. In many cases the optimal block size is several tracks.
Keywords: storage and retrieval, matrix, optimization
1 Problem Introduction

Video Server on an ATM Connected Cluster of Workstations
Olav Sandsta, Stein Langrgen, and Roger Midtstraum
Department of Computer and Information Science
Norwegian University of Science and Technology
N-7034 Trondheim, Norway
folavsa, steinl, rogerg@idi.ntnu.no
Abstract
Video servers are important for applications which make
use of digital video. The video servers should provide better
functionality than most of today's video servers offer, - e.g.,
support of flexible and instant user interactions, delivery of
multiple video formats and support of virtual video documents. In this paper we discuss the requirements that video
servers should fulfill and we describe the design and implementation of the Elvira video server. The Elvira video server
is built on a cluster of standard UNIX workstations interconnected by an ATM switch. The capacity of the Elvira server
is evaluated and we show the effects of different strategies
for allocation of video data across nodes and disks.
1. Introduction

Write Optimized Object-Oriented Database Systems
Kjetil Nrvag and Kjell Bratbergsengen
Department of Computer and Information Science
Norwegian University of Science and Technology
7034 Trondheim, Norway
fnoervaag, kjellbg@idi.ntnu.no
Abstract
In a database system, read operations are much more
common than write operations, and consequently, database
systems have been read optimized. As the size of main memory increases, more of the database read requests will be satisfied from the buffer system, and the amount of disk write
operations relative to disk read operations will increase.
This calls for a focus on write optimized database systems.
In this paper, we present solutions to this problem. We describe in detail the data structures and algorithms needed
to realize a write optimized object-oriented database system
in the context of Vagabond, an OODB currently being implemented at our department. In Vagabond, focus has been
to provide support for applications which have earlier used
file systems because of the limited data bandwidth in current
database systems, typical examples are super computing applications and geographical information systems
1. Introduction

On the Computational Requirements of
Virtual Reality Systems 1
Frank Devai
School of Computing & Mathematics, University of Ulster
Abstract
The computational requirements of high-quality, real-time rendering exceeds the limits of
generally available computing power. However illumination effects, except shadows, are
less noticeable on moving pictures. Shadows can be produced with the same techniques
used for visibility computations, therefore the basic requirements of real-time rendering
are transformations, pre-selection of the part of the scene to be displayed and visibility
computations. Transformations scale well, ie, their time requirement grows linearly with
the input size. Pre-selection, if implemented by the traditional way of polygon clipping,
has a growing rate of N log N in the worst case, where N is the total number of edges in
the scene. Visibility computations, exhibiting a quadratic growing rate, are the bottleneck
from a theoretical point of view. Three approaches are discussed to speed up visibility
computations: (i) reducing the expected running time to O(N log N ) (ii) using approximation algorithms with O(N K) worst-case time, where K is the linear resolution of the
image, and (iii) applying parallel techniques leading to logarithmic time in the worst-case.
Though the growing rate of the time requirement of pre-selection is significantly slower
than that of visibility, it is demonstrated that pre-selection has to deal with a significantly
higher amount of data than visibility computations, as the average clipping volume is 1/27
of the volume of the model.
1 Introduction

Signal and Image Processing in Java
Jonathan Campbell and Fionn Murtagh
University of Ulster, Magee College,   Derry, BT48 7JL
email: jg.campbell@ulst.ac.uk.
Revisions available from
http://www.infm.ulst.ac.uk/research/preprints.html
Original paper presented at IMVIP '97
University of Ulster, Magee College, Derry
10-13 September 1997.
9 September 1997
Revised 20 September 1997
Revised 6 November 1997
Abstract
We describe the implementation of a multi-purpose data analysis laboratory, DataLab-J, in
the programming language Java. We briefly trace the stages of the evolution of DataLab from a
FORTRAN-IV system in 1973 to the current Java development. Description of this evolution allows
us to discuss some key design and functionality decisions and issues that arose throughout the years;
many of these issues remain topical, so, in addition to an evaluation of Java, we identify and discuss
what are for us the major issues in the design of such software. Moreover, we address questions
raised by the need to convert legacy systems, e.g. those programmed in C and various versions of
FORTRAN. The experience of redesign and implementation in Java is described, together with a
brief evaluation of the suitability of Java for 'number-crunching'. Overall conclusions are drawn,
regarding design of such software, lessons learned, traps to avoid, and on Java itself.
1 Introduction

A Rule-Based CQL for 2-Dimensional Tables
Mohand-Sad Hacid, Patrick Marcel, and Christophe Rigotti
Laboratoire d'Ingenierie des Systemes d'Information
INSA Lyon,   B^atiment 501, F-69621 Villeurbanne Cedex
Tel : 72 43 85 88 Fax: 72 43 87 13
fmohand,patrick,crigg@lisi.insa-lyon.fr
Abstract. We describe the core of a rule-based CQL, devoted to the
manipulation of 2-dimensional tabular databases. The rules provide a
simple and declarative way to restructure and query tables, and the constraints allow to define cell contents by formulas over concrete domains.
We define a model-theoretic semantics and develop an equivalent fixpoint
theory that leads to a naive evaluation procedure.
1 Introduction

SEE ME, HEAR ME: INTEGRATING AUTOMATIC SPEECH RECOGNITION AND
LIP-READING
Paul Duchnowski 1 Uwe Meier 1 Alex Waibel 1;2
1 University of Karlsruhe,   Karlsruhe, Germany   2 Carnegie Mellon University,   Pittsburgh PA, USA
ABSTRACT
We present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. A Multi-State Time Delay
Neural Network performs the recognition of spelled letter
sequences taking advantage of lip images from a standard
camera. The problems addressed include efficient but effective representation of the visual information and optimum
manner of combining the two modalities when rendering a
decision. We show results for several alternatives to direct
gray level image as the visual evidence. These are: Principal
Components, Linear Discriminants, and DFT coefficients.
Dimensionality of the input is decreased by a factor of 12
while maintaining recognition rates. Combination of the
visual and acoustic information is performed at three different levels of abstraction. Results suggest that integration
of higher order input features works best. On a continuous
spelling task, visual-alone recognition of 45-55%, when combined with acoustic data, lowers audio-alone error rates by
30-40%.
1. INTRODUCTION

A Stochastic Case Frame Approach for Natural Language Understanding
Wolfgang Minker, Samir Bennacef, Jean-Luc Gauvain
Spoken Language Processing Group
LIMSI-CNRS
91403 Orsay cedex, FRANCE
email: fminker,bennacef,gauvaing@limsi.fr
ABSTRACT
A stochastically based approach for the semantic analysis component of a natural spoken language system for the ATIS task has been
developed. The semantic analyzer of the spoken language system
already in use at LIMSI makes use of a rule-based case grammar. In
this work, the system of rules for the semantic analysis is replaced
with a relatively simple, first order Hidden Markov Model. The
performance of the two approaches can be compared because they
use identical semantic representations despite their rather different
methods for meaning extraction. We use an evaluation methodology
that assesses performance at different semantic levels, including the
database response comparison used in the ARPA ATIS paradigm.
1. INTRODUCTION

Tracking Drifting Concepts
By Minimizing Disagreements
David P. Helmbold and Philip M. Long
CIS Board
UC Santa Cruz
Santa Cruz, CA 95064
March 24, 1994
Abstract
In this paper we consider the problem of tracking a subset of a domain (called the target) which
changes gradually over time. A single (unknown) probability distribution over the domain is used
to generate random examples for the learning algorithm and measure the speed at which the target
changes.
Clearly, the more rapidly the target moves, the harder it is for the algorithm to maintain a good
approximation of the target. Therefore we evaluate algorithms based on how much movement of
the target can be tolerated between examples while predicting with accuracy *. Furthermore, the
complexity of the class H of possible targets, as measured by d, its VC-dimension, also effects the
difficulty of tracking the target concept.
We show that if the problem of minimizing the number of disagreements with a sample from
among concepts in a class H can be approximated to within a factor k, then there is a simple tracking
algorithm for H which can achieve a probability * of making a mistake if the target movement rate
is at most a constant times * 2 =(k(d + k) ln 1
* ), where d is the Vapnik-Chervonenkis dimension of
H. Also, we show that if H is properly PAC-learnable, then there is an efficient (randomized)
algorithm that with high probability approximately minimizes disagreements to within a factor of
7d + 1, yielding an efficient tracking algorithm for H which tolerates drift rates up to a constant
times * 2 =(d 2 ln 1
In addition, we prove complementary results for the classes of halfspaces and axis-aligned hy
perrectangles showing that the maximum rate of drift that any algorithm (even with unlimited
computational power) can tolerate is a constant times * 2 =d.
1 Introduction

A Monotonic Measure for Optimal
Feature Selection
Huan Liu 1 and Hiroshi Motoda 2 and Manoranjan Dash 3
1 Dept of Info Sys & Comp Sci, National University of Singapore,   Singapore 119260.
2 Division of Intelligent Sys Sci, Osaka University,   Ibaraki, Osaka 567, Japan.
3 BioInformatics Centre, National University of Singapore,   Singapore 119074.
fliuh, manoranjg@iscs.nus.edu.sg motoda@sanken.osaka-u.ac.jp
Abstract. Feature selection is a problem of choosing a subset of relevant
features. In general, only exhaustive search can bring about the optimal
subset. With a monotonic measure, exhaustive search can be avoided
without sacrificing optimality. Unfortunately, most error- or distance-based measures are not monotonic. A new measure is employed in this
work that is monotonic and fast to compute. The search for relevant
features according to this measure is guaranteed to be complete but not
exhaustive. Experiments are conducted for verification.
1 Introduction

Understanding Neural Networks via Rule Extraction
Rudy Setiono and Huan Liu
Department of Information Systems and Computer Science
National University of Singapore
Kent Ridge, Singapore 0511
frudys,liuhg@iscs.nus.sg
Abstract
Although backpropagation neural networks
generally predict better than decision trees do
for pattern classification problems, they are often regarded as black boxes, i.e., their predictions are not as interpretable as those of decision trees. This paper argues that this is because there has been no proper technique that
enables us to do so. With an algorithm that
can extract rules 1 , by drawing parallels with
those of decision trees, we show that the predictions of a network can be explained via rules extracted from it, thereby, the network can be understood. Experiments demonstrate that rules
extracted from neural networks are comparable with those of decision trees in terms of predictive accuracy, number of rules and average
number of conditions for a rule; they preserve
high predictive accuracy of original networks.
1 Introduction

To appear in Expert Systems with Applications: An International Journal, Vol.10(1996)
Efficient Rule Induction from Noise Data
Huan Liu
Department of Information Systems and Computer Science
National University of Singapore
Kent Ridge, Singapore 0511
liuh@iscs.nus.sg
Tel: (+65)-772-6563; Fax: (+65) 779-4580
Acknowledgments
Many thanks to Rudy Setiono and Tiow Seng Tan for providing valuable com
ments and help.
+PAGE+

To appear in the journal Presence
06/02/97 4:57 PM 1
Integrating Pedagogical Agents into Virtual Environments
W. Lewis Johnson
Jeff Rickel
Randy Stiles
Allen Munro 1
Abstract
In order for a virtual environment to be effective as a training tool, it is not enough to
concentrate on the fidelity of the renderings and the accuracy of the simulated behaviors.
The environment should help trainees develop an understanding of the task being trained,
and should provide guidance and assistance as needed. This paper describes a system for
developing virtual environments in which pedagogical capabilities are incorporated into
autonomous agents that interact with trainees. These pedagogical agents can monitor
trainees progress and provide guidance and assistance. The agents interact with
simulations of objects in the environment, and with trainees. The paper describes the
architectural features of the environment and of the agents that permit the agents to meet
instructional objectives within the virtual environment. It also discusses how agent-based
instruction is combined with other methods of delivering instruction.
1. Introduction

Generating Word Lattices from Abstract Meaning Representation
Irene Langkilde and Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
ilangkil@isi.edu and knight@isi.edu
Abstract
Large-scale generation of natural language requires an
abstract meaning representation and a mechanism for
integrating immense amounts of lexical, morphological, grammatical, and conceptual knowledge. The
availability of corpus-based statistical knowledge motivates the invention of a new style of generation in
which word lattices compactly encode many possible
sentence renderings and a statistical extractor chooses
the best ones. The focus of generation thus shifts
to how word lattices can be generated from abstract
meaning representation. This paper presents a flexible
meaning representation scheme and generation mechanism. It includes an efficient generation algorithm and
grammar formalism that maps from a meaning representation to a lattice. This mapping is flexible enough
to allow meaning representation along a continuum of
semantic depth.
Introduction

A Multicast Congestion Control Mechanism Using Representatives
Dante DeLucia Katia Obraczka
Hughes Research Laboratories USC Information Sciences Institute
3011 Malibu Canyon Road 4676 Admiralty Way Suite 1001
Malibu CA 90265 Marina Del Rey, CA 90292
email: dante@usc.edu email: katia@isi.edu
Abstract
In this paper, we propose a congestion control
mechanism for reliable multicast applications that
uses a small set of group members, or representatives,
to provide timely and accurate feedback on behalf of
congested subtrees of a multicast distribution tree.
Our algorithm does not need to compute round-trip
time (RTT) from all receivers to the source, nor does
it require knowledge of group membership or network
topology. Through simulations, we evaluate our algorithm with and without TCP cross traffic. This initial evaluation study shows that our algorithm takes
advantage of network bandwidth when available, yet
does not starve competing flows.
1 Introduction

A Tool for Massively Replicating Internet Archives:
Design, Implementation, and Experience
Katia Obraczka
University of Southern California
Information Science Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
katia@isi.edu
Peter Danzig, Dante DeLucia, Erh-Yuan Tsai
University of Southern California
Computer Science Department
Los Angeles, CA 90089-0781
fdanzig, dante, erhyuantg@usc.edu
Abstract
This paper reports the design, implementation, and performance of a scalable and efficient tool to replicate Internet information services. Our tool targets replication
degrees of tens of thousands of weakly-consistent replicas scattered throughout the Internet's thousands of autonomously administered domains. The main goal of our
replication tool is to make existing replication algorithms
scale in today's exponentially-growing, autonomously-managed internetworks.
1. Introduction

Information Gathering Plans With Sensing Actions
Naveen Ashish and Craig A. Knoblock Alon Levy
Information Sciences Institute and AT&T Bell Laboratories
Department of Computer Science AI Principles Research Dept.
University of Southern California   600 Mountain Ave., Room 2A-440
4676 Admiralty Way Murray Hill, NJ 07974
Marina del Rey, CA 90292   levy@research.att.com
fashish,knoblockg@isi.edu
Abstract
Information gathering agents can automate the task of retrieving and integrating data
from a large number of diverse information sources. The key issue in their performance is
efficient query planning that minimizes the number of information sources used to answer
a query. Previous work on query planning has considered generating information gathering
plans solely based on compile-time analysis of the query and the models of the information
sources. We argue that at compile-time it may not be possible to generate an efficient
plan for retrieving the requested information because of the large number of possibly
relevant sources. We describe an approach that naturally extends query planning to use
run-time information to optimize queries that involve many sources. First, we describe an
algorithm for generating a discrimination matrix, which is a data structure that identifies
the information that can be sensed at run-time to optimize a query plan. Next, we describe
how the discrimination matrix is used to decide which of the possible run-time sensing
actions to perform. Finally, we demonstrate that this approach yields significant savings
(over 90% for some queries) in a real-world task.
The first and second authors is supported in part by Rome Laboratory of the Air Force Systems Command
and the Advanced Research Projects Agency under contract no. F30602-94-C-0210, and in part by the National
Science Foundation under grant number IRI-9313993. The views and conclusions contained in this paper are the
author's and should not be interpreted as representing the official opinion or policy of ARPA, RL, NSF, AT&T
Labs, or any person or agency connected with them.
+PAGE+

Rule Induction for Semantic Query Optimization
Chun-Nan Hsu and Craig A. Knoblock
Information Sciences Institute and Department of Computer Science
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
fchunnan,knoblockg@isi.edu
Abstract
Semantic query optimization can dramatically speed up database query answering by
knowledge intensive reformulation. But the
problem of how to learn required semantic
rules has not previously been solved. This
paper describes an approach using an inductive learning algorithm to solve the problem. In our approach, learning is triggered
by user queries and then the system induces semantic rules from the information in
databases. The inductive learning algorithm
used in this approach can select an appropriate set of relevant attributes from a potentially huge number of attributes in real-world
databases. Experimental results demonstrate
that this approach can learn sufficient background knowledge to reformulate queries and
provide a 57 percent average performance improvement.
1 INTRODUCTION

A Synergy of Agent Components:
Social Comparison for Failure Detection
Gal A. Kaminka Milind Tambe
Information Sciences Institute and Computer Science Department
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
galk, tambe-@isi.edu
1 Overview

Learning Finite Automata Using Local Distinguishing Experiments +L +
Wei-Min Shen
Microelectronics and Computer Technology Corporation
3500 West Balcones Center Drive
Austin, TX 78759, U.S.A.
Abstract
One of the open problems listed in [ Rivest and
Schapire, 1989 ] is whether and how that the
copies of L in their algorithm can be combined into one for better performance. This
paper describes an algorithm called D that
does that combination. The idea is to represent
the states of the learned model using observable
symbols as well as hidden symbols that are constructed during learning. These hidden symbols are created to reflect the distinct behaviors
of the model states. The distinct behaviors are
represented as local distinguishing experiments
(LDEs) (not to be confused with global distinguishing sequences), and these LDEs are created when the learner's prediction mismatches
the actual observation from the unknown machine. To synchronize the model with the environment, these LDEs can also be concatenated to form a homing sequence. It can
be shown that D can learn, with probability
1 ~, a model that is an *-approximation of
the unknown machine, in a number of actions
polynomial in the size of the environment and
1 Introduction

Adaptive Agent Tracking in Real-world
Multi-Agent Domains: A Preliminary Report
Milind Tambe, Lewis Johnson and Wei-Min Shen
Information Sciences Institute and Computer Science Department
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
ftambe,johnson,sheng@isi.edu
October 30, 1996
Abstract
Intelligent interaction in multi-agent domains frequently requires an agent to track other
agents' mental states: their current goals, beliefs, and intentions. Accuracy in this agent
tracking task is critically dependent on the accuracy of the tracker's (tracking agent's) model
of the trackee (tracked agent). Unfortunately, in real-world situations, model imperfections
arise due to the tracker's resource and information constraints, as well as due to trackees'
dynamic behavior modification. While such model imperfections are unavoidable, a tracker
must nonetheless attempt to be adaptive in its agent tracking. This article identifies key issues
in adaptive agent tracking and presents an approach called DEFT. At its core, DEFT is based
on discrimination-based learning. The main idea is to identify the deficiency of a model based
on tracking failures, and revise the model by using features that are critical in discriminating
successful and failed tracking episodes. Because in real-world situations the set of candidate
discriminating features is very large, DEFT relies on knowledge-based focusing to limit the
discrimination to those features that it determines were relevant in successful tracking episodes
with an autonomous explanation capability as a major source of this knowledge. This article
reports on experiments with an implementation of key aspects of DEFT in a complex synthetic
air-to-air combat domain.
+PAGE+

To appear in the Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98)
Learning to Predict User Operations for Adaptive Scheduling
Melinda T. Gervasio and Wayne Iba and Pat Langley
Institute for the Study of Learning and Expertise
2164 Staunton Court, Palo Alto, California 94306
fgervasio,iba,langleyg@isle.org
Abstract
Mixed-initiative systems present the challenge of finding an effective level of interaction between humans
and computers. Machine learning presents a promising approach to this problem in the form of systems
that automatically adapt their behavior to accommodate different users. In this paper, we present an empirical study of learning user models in an adaptive
assistant for crisis scheduling. We describe the problem domain and the scheduling assistant, then present
an initial formulation of the adaptive assistant's learning task and the results of a baseline study. After this,
we report the results of three subsequent experiments
that investigate the effects of problem reformulation
and representation augmentation. The results suggest
that problem reformulation leads to significantly better accuracy without sacrificing the usefulness of the
learned behavior. The studies also raise several interesting issues in adaptive assistance for scheduling.
Introduction

To appear in Proceedings of the Second International Conference on AI Planning Systems (1994). Chicago: AAAI Press.
Reactive and Automatic Behavior in Plan Execution
Pat Langley Wayne Iba Jeff Shrager
Robotics Laboratory Recom Technologies Palo Alto Research Center
Computer Science Dept.   Mail Stop 269-2   Xerox Corporation
Stanford University NASA Ames Research Center   3333 Coyote Hill Road
Stanford, CA 94305 Moffett Field, CA 94035 Palo Alto, CA 94304
langley@cs.stanford.edu iba@wind.arc.nasa.gov shrager@xerox.com
Abstract
Much of the work on execution assumes that the agent
constantly senses the environment, which lets it respond
immediately to errors or unexpected events. In this paper, we argue that this purely reactive strategy is only
optimal if sensing is inexpensive, and we formulate a simple model of execution that incorporates the cost of sensing. We present an average-case analysis of this model,
which shows that in domains with high sensing cost or
low probability of error, a more `automatic' strategy -
one with long intervals between sensing can lead to
less expensive execution. The analysis also shows that
the distance to the goal has no effect on the optimal sensing interval. These results run counter to the prevailing
wisdom in the planning community, but they promise a
more balanced approach to the interleaving of execution
and sensing.
Reactive and Automatic Execution

Where Do SE-trees Perform? (Part I)
Ron Rymon
Intelligent Systems Program
901 Cathedral of Learning
University of Pittsburgh
Pittsburgh, PA 15260
E-mail: Rymon@ISP.Pitt.edu
March 5, 1995
Abstract
As a classifier, a Set Enumeration (SE) tree can be viewed as a generalization of
decision trees. We empirically characterize domains in which SE-trees are particularly
advantageous relative to decision trees. Specifically, we show that:
1. SE-trees excel in domains in which relatively few examples are available; and

Advanced Transaction Processing in Multilevel Secure File
Stores
Elisa Bertino Sushil Jajodia Luigi Mancini Indrajit Ray x
Abstract
The concurrency control requirements for transaction processing in a multilevel secure file system are different from those in conventional transaction processing systems.
In particular, there is the need to coordinate transactions at different security levels
avoiding both potential timing covert channels and the starvation of transactions at
higher security levels. Suppose a transaction at a lower security level attempts to write
a data item that is being read by a transaction at a higher security level. On the one
hand, a timing covert channel arises if the transaction at the lower security level is either
delayed or aborted by the scheduler. On the other hand, the transaction at the high
security level may be subjected to an indefinite delay if it is forced to abort repeatedly.
This paper extends the classical two-phase locking mechanism to multilevel secure
file systems. The scheme presented here prevents potential timing covert channels and
avoids the abort of higher level transactions nonetheless guaranteeing serializability.
The programmer is provided with a powerful set of linguistic constructs that supports
exception handling, partial rollback and forward recovery. The proper use of these
constructs can prevent the indefinite delay in completion of a higher level transaction,
and allows the programmer to trade off starvation with transaction isolation.
Index Terms|Data management system, File system management, Transaction processing, Concurrency control, Two-phase locking, Exception handling, Security kernel,
Mandatory access control, Covert channels.
1 Introduction

Image Indexing and Retrieval Based on
Human Perceptual Color Clustering
Yihong Gong, Guido Proietti , and Christos Faloutsos
Robotics Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
fygong, proietti, christosg@cs.cmu.edu
Abstract
We propose a new image retrieval method based
on human perceptual clustering of color images. This
color clustering produces for each image a small set of
representative colors which captures the color properties of the image, and a small set of sizable contiguous
regions which captures the spatial/geometrical properties of the image. The proposed method outperforms
the traditional histogram and its improved methods not
only with its richer image retrieval capabilities which
cover a wider spectrum of user requirements, but also
with its powerful indexing scheme which is essential to
cater for large scale image databases.
1 Introduction

Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, 1995.
Using Introspective Reasoning to Refine Indexing
Susan Fox and David B. Leake
Computer Science Department
Lindley Hall 215
Indiana University
Bloomington, IN 47405 USA
E-mail: fsfox,leakeg@cs.indiana.edu
Abstract
Introspective reasoning about a system's own
reasoning processes can form the basis for
learning to refine those reasoning processes.
The ROBBIE 1 system uses introspective reasoning to monitor the retrieval process of a
case-based planner to detect retrieval of inappropriate cases. When retrieval problems
are detected, the source of the problems is explained and the explanations are used to determine new indices to use during future case
retrieval. The goal of ROBBIE's learning is to
increase its ability to focus retrieval on relevant
cases, with the aim of simultaneously decreasing the number of candidates to consider and
increasing the likelihood that the system will be
able to successfully adapt the retrieved cases to
fit the current situation. We evaluate the benefits of the approach in light of empirical results
examining the effects of index learning in the
ROBBIE system.
1 Introduction

ACCESSIBILITY UNDER SAMPLING
* Eduardo D. Sontag ** Hector J. Sussmann
Department of Mathematics
Rutgers University
New Brunswick, NJ 08903
ABSTRACT
This note addresses the following problem: Find conditions under which a continuous-time (nonlinear)
system gives rise, under constant rate sampling, to a discrete-time system which satisfies the accessibility
property.
* Research supported in part by US Air Force Grant AFOSR 80-0196
** Research supported in part under an NSF Grant
+PAGE+

NEURAL NETWORKS FOR CONTROL
Eduardo D. Sontag
Department of Mathematics, Rutgers University
New Brunswick, NJ 08903, USA
Abstract
This paper starts by placing neural net techniques in a general nonlinear
control framework. After that, several basic theoretical results on networks
are surveyed.
1 Introduction

Global Stabilization of Linear Discrete-Time Systems
with Bounded Feedback
Yudi Yang
IBM,   MD340
1311 Mamaroneck Ave
White Plains, NY 10605
yudiy@vnet.ibm.com
Eduardo D. Sontag
Department of Mathematics
Rutgers University
New Brunswick, NJ 08903
sontag@hilbert.rutgers.edu
Hector J. Sussmann
Department of Mathematics
Rutgers University
New Brunswick, NJ 08903
sussmann@hilbert.rutgers.edu
Abstract
This paper deals with the problem of global stabilization of linear discrete time systems by means of
bounded feedback laws. The main result proved is an analog of one proved for the continuous time case
by the authors, and shows that such stabilization is possible if and only if the system is stabilizable
with arbitrary controls and the transition matrix has spectral radius less or equal to one. The proof
provides in principle an algorithm for the construction of such feedback laws, which can be implemented
either as cascades or as parallel connections ("single hidden layer neural networks") of simple saturation
functions.
1 Introduction

A Construction of a Cipher
From a Single Pseudorandom Permutation
Shimon Even 1 and Yishay Mansour 2
Abstract
We suggest a scheme for a block cipher which uses only one randomly chosen permutation, F . The key, consisting of two blocks, K 1
and K 2 is used in the following way: The message block is XORed
with K 1 before applying F , and the outcome is XORed with K 2 , to
produce the cryptogram block. We show that the resulting cipher
is secure (when the permutation is random or pseudorandom). This
removes the need to store, or generate a multitude of permutations.
1 Comp. Sci. Dept., Technion, Israel Institute of Technology,   Haifa, Israel 32000.

ON COMPUTABLE BELIEFS OF RATIONAL MACHINES
Nimrod Megiddo
Abstract. Traditional decision theory has assumed that agents have complete, consistent and readily available beliefs and preferences. Obviously, even if
an expert system has complete and consistent beliefs, it cannot have them readily
available. Moreover, some beliefs about beliefs are not even approximately computable. It is shown that if all players have complete and consistent beliefs, they
can compute approximate beliefs about beliefs of any order by considering events
arbitrarily close in some well-defined sense to the ones in question.
1. Introduction

Constructing Small Sample Spaces Satisfying Given
Constraints
Daphne Koller
e-mail: daphne@cs.stanford.edu
Nimrod Megiddo
e-mail: megiddo@almaden.ibm.com
Abstract
Abstract. The subject of this paper is finding small sample spaces for joint distributions of
n discrete random variables. Such distributions
are often only required to obey a certain limited set of constraints of the form P r(E) = .
We show that the problem of deciding whether
there exists any distribution satisfying a given
set of constraints is NP-hard. However, if the
constraints are consistent, then there exists a distribution satisfying them which is supported by
a "small" sample space (one whose cardinality is
equal to the number of constraints). For the important case of independence constraints, where
the constraints have a certain form and are consistent with a joint distribution of n independent
random variables, a small sample space can be
constructed in polynomial time. This last result
is also useful for de-randomizing algorithms. We
demonstrate this technique by an application to
the problem of finding large independent sets in
sparse hypergraphs.
Department of Computer Science, Stanford University,   Stanford, CA 94305;   and IBM Almaden Research
Center,   650 Harry Road, San Jose, CA 95120.
IBM Almaden Research Center,   650 Harry Road, San
Jose, CA 95120;   and School of Mathematical Sciences, Tel
Aviv University,   Tel Aviv, Israel.
Research supported in part by ONR Contract N00014-91-C-0026 and by the Air Force Office of Scientific
Research (AFSC), under Contract F49620-91-C-0080.
(Paste copyright notice here.)
1. Introduction

Published in SPIE Proceeding #1667 Practical Holography VI (SPIE, Bellingham, WA, February 1992) paper #04 (in press).
Optimization of Hologram Computation for Real-Time Display
Mark Lucente
MIT Media Laboratory
Spatial Imaging Group
20 Ames St.
Cambridge, MA 02139
USA
ABSTRACT
Several methods of increasing the speed and simplicity of the computation of off-axis transmission holograms are
presented, with applications to the real-time display of holographic images. A bipolar intensity approach enables a
linear summation of interference fringes, a factor of two speed increase, and the elimination of image noise caused by
object self-interference. An order of magnitude speed increase is obtained through the use of precomputed look-up
tables containing a large array of elemental interference patterns corresponding to point source contributions from
each of the possible locations in image space. Results achieved using a data-parallel supercomputer to compute
horizontal-parallax-only holographic patterns containing 6 megasamples indicate that an image comprised of 10,000
points with arbitrary brightness (grayscale) can be computed in under one second.
INTRODUCTION

Authoring and Transcription Tools
for Speech-Based Hypermedia Systems
Barry Arons
MIT Media Laboratory
20 Ames Street, E15-353
Cambridge MA, 02139
Phone: +1 617-253-2245
E-mail: barons@media-lab.mit.edu
Abstract
Authoring is usually one of the most difficult parts in the design and implementation of hypertext and
hypermedia systems. This problem is exacerbated if the data to be presented by the system is speech,
rather than text or graphics, because of the slow and serial nature of speech. This paper provides an
overview of speech-only hypermedia, discusses the difficulties associated with authoring databases for
such a system, and explores a variety of techniques to assist in the authoring process.
Speech-Only Hypermedia

The Evolution of Memory and Mental Models
Using Genetic Programming
Scott Brave
Computer Science Dept.
Stanford University
Stanford, California 94305
brave@cs.stanford.edu
ABSTRACT
This paper applies genetic programming
to the evolution of intelligent agents that
gradually build internal representations of
their surroundings for later use in
planning. The method used allows for the
creation of dynamically determined
representations that are not pre-designed
by the human creator of the system. In an
illustrative path-planning problem, evolved
programs learn a model of their world and
use this internal representation to plan
their successive actions. The results show
that the proposed method is successful in
evolving programs that solve the planning
problem and is thus a worthy basis for
further investigation.
1. Introduction

DESIGNING AN ECOLOGY OF DISTRIBUTED AGENTS
by
Nelson Minar
B.A. Mathematics (1994)
Reed College
&lt;nelson@media.mit.edu&gt;
http://www.media.mit.edu/nelson/
Submitted to the Program in Media Arts and Sciences, School of Architecture and
Planning, in partial fulfillment of the requirements for the degree of Master of Science in
Media Arts and Sciences at the Massachusetts Institute of Technology
September 1998
c fl1998 Massachusetts Institute of Technology. All rights reserved.
Author
Nelson Minar
Department of Media Arts and Sciences
August 7, 1998
Certified by
Pattie Maes
Associate Professor of Media Arts and Sciences
MIT Media Lab
Accepted by
Stephen A. Benton
Professor of Media Arts and Sciences
Chair, Departmental Committee on Graduate Students
Program in Media Arts and Sciences
+PAGE+

State Reconstruction for Determining Predictability
in Driven Nonlinear Acoustical Systems
Diploma Thesis
MEDIA LABORATORY
Massachusetts Institute of Technology
Professor Neil Gershenfeld
Institut fur Elektrische Nachrichtentechnik
Rheinisch-Westphalische Technische Hochschule Aachen
Univ. Professor Dr.-Ing. H.D. Luke
by
Bernd Schoner
May 1996
+PAGE+

Stability of the replica symmetric solution for the information
conveyed by a neural network
Simon Schultzy and Alessandro Trevesz
Department of Experimental Psychology,   South Parks Rd.,   University of Oxford,   Oxford OX1
3UD, U.K.
Programme in Neuroscience, International School for Advanced Studies,   via Beirut 2-4, 34013
Trieste, Italy
(November 7, 1997)
Abstract
The information that a pattern of firing in the output layer of a feedforward
network of threshold-linear neurons conveys about the network's inputs is
considered. A replica-symmetric solution is found to be stable for all but
small amounts of noise. The region of instability depends on the contribution
of the threshold and the sparseness: for distributed pattern distributions,
the unstable region extends to higher noise variances than for very sparse
distributions, for which it is almost nonexistant.
84.35.+i,89.70.+c,87.10.+e
Typeset using REVT E X
+PAGE+

An Optimal Weighting Criterion of Case Indexing for Both Numeric
and Symbolic Attributes
Takao Mohri and Hidehiko Tanaka
Information Engineering Course, Faculty of Engineering
The University of Tokyo
7-3-1 Hongo Bunkyo-ku, Tokyo 113, Japan
fmohri,tanakag@MTL.T.u-tokyo.ac.jp
Abstract
Indexing of cases is an important topic for Memory-Based Reasoning(MBR). One key problem is how to
assign weights to attributes of cases. Although several
weighting methods have been proposed, some methods cannot handle numeric attributes directly, so it
is necessary to discretize numeric values by classification. Furthermore, existing methods have no theoretical background, so little can be said about optimality.
We propose a new weighting method based on a statistical technique called Quantification Method II. It can
handle both numeric and symbolic attributes in the
same framework. Generated attribute weights are optimal in the sense that they maximize the ratio of variance between classes to variance of all cases. Experiments on several benchmark tests show that in many
cases, our method obtains higher accuracies than some
other weighting methods. The results also indicate
that it can distinguish relevant attributes from irrelevant ones, and can tolerate noisy data.
Introduction

A Comparative Study of Reliable Error Estimators
for Pruning Regression Trees
Lus Torgo
LIACC/FEP University of Porto
R. Campo Alegre, 823, 2 - 4150 PORTO - PORTUGAL
Phone : (+351) 2 607 8830 Fax : (+351) 2 600 3654
email : ltorgo@ncc.up.pt   WWW : http://www.ncc.up.pt/~ltorgo
Abstract. This paper presents a comparative study of several methods for estimating
the true error of treestructured regression models. We evaluate these methods in the
context of regression tree pruning. Pruning is considered a key issue for obtaining
reliable treestructured models in a real world scenario. The major step of a pruning
process consists of obtaining accurate estimates of the error of alternative tree
models. We evaluate experimentally four methods for obtaining these estimates in
twelve domains. The goal of this evaluation was to characterise the performance of
the methods in the task of selecting the best possible tree among the set of trees
considered during pruning. The results of the comparison show that certain
estimators lead to poor decisions in some domains. The Cross Validation variant that
we have proposed achieved the best results on the setups we have considered.
Keywords : Machine Learning, Regression Trees, Pruning methods.
1 Introduction

Rule Combination
in
Inductive Learning
Luis Torgo
LIACC
R.Campo Alegre, 823 - 2.
4100 PORTO
PORTUGAL
Telf. : (+351) 2 600 16 72 - Ext. 115
Fax : (+351) 2 600 3654
email : ltorgo@ciup1.ncc.up.pt
Abstract. This paper describes the work on methods for combining rules
obtained by machine learning systems. Three methods for obtaining the
classification of examples with those rules are compared. The advantages and
disadvantages of each method are discussed and the results obtained on three
real world domains are commented. The methods compared are: selection of
the best rule; PROSPECTOR-like probabilistic approximation for rule
combination; and MYCIN-like approximation. Results show significant
differences between methods indicating that the problemsolving strategy is
important for accuracy of learning systems.
1 Introduction

Rule Revision with Recurrent Neural Networks
Christian W. Omlin a;b and C.L. Giles a;c
a NEC Research Institute,   4 Independence Way, Princeton, New Jersey
b Computer Science Department, Rensselaer Polytechnic Institute,   Troy, New York
c Institute for Advanced Computer Studies, University of Maryland,   College Park, Maryland
Abstract
Recurrent neural networks readily process, recognize and generate temporal sequences. By encoding
grammatical strings as temporal sequences, recurrent neural networks can be trained to behave like deterministic sequential finite-state automata. Algorithms have been developed for extracting grammatical
rules from trained networks. Using a simple method for inserting prior knowledge (or rules) into recurrent
neural networks, we show that recurrent neural networks are able to perform rule revision. Rule revision
is performed by comparing the inserted rules with the rules in the finite-state automata extracted from
trained networks. The results from training a recurrent neural network to recognize a known non-trivial,
randomly generated regular grammar show that not only do the networks preserve correct rules but that
they are able to correct through training inserted rules which were initially incorrect. (By incorrect, we
mean that the rules were not the ones in the randomly generated grammar.)
Index Terms: Deterministic Finite-State Automata, Genuine and Incorrect Rules, Knowledge Insertion
and Extraction, Recurrent Neural Networks, Regular Languages, Rule Revision.
Published in IEEE Trans. on Knowledge and Data Engineering, vol. 8, no. 1, p. 183, 1996. Copyright IEEE.
+PAGE+

A Delay Damage Model Selection Algorithm for NARX
Neural Networks
Tsungnan Lin 1;2y ,C. Lee Giles 1;3 , Bill G. Horne 1 , S.Y. Kung 2
1 NEC Research Institute,   4 Independence Way, Princeton, NJ 08540
2 Department of Electrical Engineering, Princeton University,   Princeton, NJ 08540
3 UMIACS, University of Maryland,   College Park, MD 20742
Abstract
Recurrent neural networks have become popular models for system identification and time
series prediction. NARX (Nonlinear AutoRegressive models with eXogenous inputs) neural
network models are a popular subclass of recurrent networks and have been used in many
applications. Though embedded memory can be found in all recurrent network models, it is
particularly prominent in NARX models.
We show that using intelligent memory order selection through pruning and good initial
heuristics significantly improves the generalization and predictive performance of these nonlinear
systems on problems as diverse as grammatical inference and time series prediction.
Keywords: Recurrent neural networks, tapped-delay lines, long-term dependencies,
time series, automata, memory, temporal sequences, gradient descent training, latching,
NARX networks, auto-regressive, pruning, embedding theory.
Published in IEEE Transactions on Signal Processing, "Special Issue on Neural Networks," vol. 45, no. 11, p.
2719-2730, 1997. Copyright IEEE.
Current address:   Epson Palo Alto Laboratory,    3145 Porter Drive, Suite 104, Palo Alto, CA 94304
+PAGE+

What Size Neural Network Gives Optimal Generalization?
Convergence Properties of Backpropagation
Steve Lawrence 1;2 , C. Lee Giles 1 , Ah Chung Tsoi 2
flawrence,actg@elec.uq.edu.au, giles@research.nj.nec.com
1 NEC Research Institute,   4 Independence Way, Princeton, NJ 08540
2 Department of Electrical and Computer Engineering
University of Queensland,   St. Lucia 4072, Australia
Technical Report
UMIACS-TR-96-22 and CS-TR-3617
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742
June 1996   (Revised August 1996)
Abstract
One of the most important aspects of any machine learning paradigm is how it scales according
to problem size and complexity. Using a task with known optimal training error, and a pre-specified
maximum number of training updates, we investigate the convergence of the backpropagation algorithm
with respect to a) the complexity of the required function approximation, b) the size of the network in
relation to the size required for an optimal solution, and c) the degree of noise in the training data. In
general, for a) the solution found is worse when the function to be approximated is more complex, for
b) oversized networks can result in lower training and generalization error in certain cases, and for c)
the use of committee or ensemble techniques can be more beneficial as the level of noise in the training
data is increased. For the experiments we performed, we do not obtain the optimal solution in any case.
We further support the observation that larger networks can produce better training and generalization
error using a face recognition example where a network with many more parameters than training points
generalizes better than smaller networks.
Keywords: Local Minima, Generalization, Committees, Ensembles, Convergence, Backpropagation, Smoothness,
Network Size, Problem Complexity, Function Approximation, Curse of Dimensionality.
http://www.neci.nj.nec.com/homepages/lawrence
Also with the   Institute for Advanced Computer Studies, University of Maryland,   College Park, MD 20742.
http://www.neci.nj.nec.com/homepages/giles.html
+PAGE+

Appears in the Seventh International World Wide Web Conference, Brisbane, Australia, Elsevier Science, pp.
95-105, 1998.
Inquirus, the NECI meta search engine
Steve Lawrence and C. Lee Giles
NEC Research Institute,
4 Independence Way, Princeton, NJ 08540, U.S.A.
lawrence@research.nj.nec.com and giles@research.nj.nec.com
Abstract
World Wide Web (WWW) search engines (e.g. AltaVista, Infoseek, HotBot, etc.) have a number of
deficiencies including: periods of downtime, low coverage of the WWW, inconsistent and inefficient user
interfaces, out of date databases, poor relevancy ranking and precision, and difficulties with spamming
techniques. Meta search engines have been introduced which address some of these and other difficulties in
searching the WWW. However, current meta search engines retain some of these difficulties and may also
introduce their own problems (e.g. reduced relevance because one or more of the search engines returns results
with poor relevance). We present Inquirus, the NECI meta search engine, which addresses many of the
deficiencies in current techniques. Rather than working with the list of documents and summaries returned by
search engines, as current meta search engines typically do, the Inquirus meta search engine works by
downloading and analyzing the individual documents. The Inquirus meta search engine makes improvements
over existing search engines in a number of areas, e.g.: more useful document summaries incorporating query
term context, identification of both pages which no longer exist and pages which no longer contain the query
terms, advanced detection of duplicate pages, improved document ranking using proximity information,
dramatically improved precision for certain queries by using specific expressive forms, and quick jump links
and highlighting when viewing the full documents.
Keywords
Information retrieval; Search engine; Meta search; Context-based search
1. Introduction

On the uniqueness of the convolution theorem for
the fourier transform
Harold S. Stone
Lance R. Williams
NEC Research Institute
4 Independence Way
Princeton, NJ 08540
Revision 1,   13 February 1995
Abstract
This paper shows that members of the fourier transform family are the only linear
transforms that have a convolution theorem, that is, that can replace O(N 2 ) operations
of a convolution in a time domain by O(N) operations in a transform domain. Generally,
there is an additional cost to compute the transform itself. Our observation is motivated by
recent activity in wavelet and subband decompositions and related spectral analyses, which
are attractive alternatives for signal compression applications. A natural question when
using such techniques is to determine if convolutions of N -point signals can be calculated
with fewer operations in a compressed transform domain than in an uncompressed time
domain. The answer is negative for a broad set of assumptions. This paper indicates what
assumptions must be relaxed in seeking a linear transform that has a convolution theorem
comparable to the convolution theorem for fourier transforms.
1 Introduction

Chapter 1
Scheduling To Minimize Average Completion Time:
Off-line and On-line Algorithms
Leslie A. Hall David B. Shmoys Joel Wein
Abstract
Time-indexed linear programming formulations have recently received a great deal of attention for their practical
effectiveness in solving a number of single-machine scheduling problems. We show that these formulations are also an
important tool in the design of approximation algorithms
with good worst-case performance guarantees. We give simple new rounding techniques to convert an optimal fractional
solution into a feasible schedule for which we can prove a
constant-factor performance guarantee, thereby giving the
first theoretical evidence of the strength of these relaxations.
Specifically, we consider the problem of minimizing the
total weighted job completion time on a single machine
subject to precedence constraints, and give a polynomial-time (4 + *)-approximation algorithm, for any * &gt; 0;
the best previously known guarantee for this problem was
superlogarithmic. With somewhat larger constants, we also
show how to extend this result to the case with release date
constraints, and still more generally, to the case with m
identical parallel machines. We give two other techniques for
problems in which there are release dates, but no precedence
constraints: the first is based on other new LP rounding
algorithms, whereas the second is a general framework for
designing on-line algorithms to minimize the total weighted
completion time.
1 Introduction

The Anisotropy in the Cosmic Microwave Background
At Degree Angular Scales.
C. B. Netterfield, N. Jarosik, L. Page, D. Wilkinson, & E. Wollack 1
Princeton University, Department of Physics,   Jadwin Hall, P.O. Box 708, Princeton, NJ
08544
Received ; accepted
Submitted Ap. J. Letters
1 NRAO,   2015 Ivy Rd., Charlottesville, VA, 22903
+PAGE+

BUTP-98/
Relation between the simple, guessed,
and the complicated, derived,
super-Hamiltonians for shell dynamics
P. Hajcek
Institute for Theoretical Physics
University of Bern
Sidlerstrasse 5, CH-3012 Bern, Switzerland
March 1998
Abstract
The Hamiltonian dynamics of spherically symmetric massive thin shells
in the general relativity is considered. Two different constraint dynamical
systems representing this dynamics have been described recently; the relation
of these two systems is investigated. The symmetry groups of both systems
are found. The systems are reduced to the presymplectic manifolds 1 and 2 ,
lest non-physical aspects like gauge fixings or embeddings in extended phase
spaces hinder the argument. The following facts are shown. 1 is three- and 2
is five-dimensional; the description of the shell dynamics by 1 is incomplete
so that some measurable properties of the shell cannot be predicted. 1 is
locally equivalent to a subsystem of 2 and the corresponding local morphisms
are not unique, due to the large symmetry group of 2 . The local equivalence
explains why the same radial equation results from both systems; what is,
however, the physical importance of just local, but not global equivalence of
constraint dynamical systems remains unclear.
+PAGE+

Laser Remote Sensing Techniques for Vertical Profiling of Cloud and
Aerosol Extinction and Back-scatter in the Lower Atmosphere.
A Brief Review
$flfl Papayannis*, E. Fokitis
National Technical University of Athens, Physics Department
Zografou Campus, 15780 Zografou, GREECE
Email: apdlidar@central.ntua.gr
Abstract
In this brief contribution we present the three principal laser remote
sensing ( lidar) techniques developed to retrieve the vertical profiling of
clouds and of the suspended aerosols (extinction and backscatter) in the
lower atmosphere, namely in the 0-7 km altitude region. The three lidar
techniques include the elastic ( Klett inversion, Doppler broadening) and
the nonelastic backscattering techniques ( Raman scattering). We report
on the potential of these techniques, as well as on the typical accuracies
of these techniques in the retrieval of the cloud and aerosol extinction
and backscatter vertical profiles in the troposphere (0-7 km ASL).
flfl Introduction

Designing Distributed Applications
with Mobile Code Paradigms
Antonio Carzaniga
Politecnico di Milano
Piazza Leonardo da Vinci, 32
20133 Milano, Italy
+39-2-2399-3638
carzaniga@elet.polimi.it
Gian Pietro Picco
Politecnico di Torino
Corso Duca degli Abruzzi, 24
10129 Torino, Italy
+39-11-564-7008
picco@athena.polito.it
Giovanni Vigna
Politecnico di Milano
Piazza Leonardo da Vinci, 32
20133 Milano, Italy
+39-2-2399-3666
vigna@elet.polimi.it
ABSTRACT
Large scale distributed systems are becoming of
paramount importance, due to the evolution of technology and to the interest of market. Their development,
however, is not yet supported by a sound technological and methodological background, as the results developed for small size distributed systems often do not
scale up. Recently, mobile code languages (MCLs) have
been proposed as a technological answer to the problem.
In this work, we abstract away from the details of these
languages by deriving design paradigms exploiting code
mobility that are independent of any particular technology. We present such design paradigms, together
with a discussion of their features, their application domain, and some hints about the selection of the correct
paradigm for a given distributed application.
Keywords
Mobile code, design paradigms, distributed applications.
INTRODUCTION

Proceedings of the 19th Annual Conference of the Cognitive
Science Society, Mahwah, NJ:Erlbaum p. 253-258 (1997).
253
The Dynamics of Prefrontal Cortico-Thalamo-Basal Ganglionic Loops and
Short-Term Memory Interference Phenomena
Jack Gelfand 1 , Vijay Gullapalli 1 , Marcia Johnson 1 , Carol Raye 1 and
Jeffrey Henderson 2
Department of Psychology 1 and Department of Computer Science 2
Princeton University
Princeton, NJ 08544
jjg@princeton.edu
Abstract
We present computer simulations of a model of the brain
mechanisms operating in short-term memory tasks that are
consistent with the anatomy and physiology of prefrontal
cortex and associated subcortical structures. These
simulations include dynamical processes in thalamo-cortical loops which are used to generate short-term
persistent responses in prefrontal cortex. We discuss this
model in terms of the representation of input stimuli in
cortical association areas and prefrontal short-term
memory areas. We report on interference phenomena that
result from the interaction of these dynamical processes
and lateral projections within cortical columns. These
interference phenomena can be used to elucidate the
representational organization of short-term memory.
Introduction

Uniform Actions in Asynchronous Distributed
Systems
Dalia Malki Ken Birman Aleta Ricciardi Andre Schiper x
TR 94-1447
Department of Computer Science, Cornell University
Abstract
We develop necessary conditions for the development of asynchronous distributed
software that will perform uniform actions (events that if performed by any process,
must be performed at all processes). The paper focuses on dynamic uniformity, which
differs from the classical problems in that processes continually leave and join the
ongoing computation. Here, we first treat a static version of the problem (lacking joins),
and then extend the results so obtained to also include joins. Our results demonstrate
that in contrast to Consensus, which cannot be solved in asynchronous systems with
even a single faulty process, dynamic uniformity can be solved using a failure detection
mechanism that makes bounded numbers of mistakes. Because dynamic uniformity
arises in systems that maintain safety within a "primary partition" of a network, our
paper provides a rigorous characterization of the framework upon which several existing
distributed programming environments are based.
1 Introduction

On the Boosting Ability of Top-Down
Decision Tree Learning Algorithms
Michael Kearns
AT&T Research
Yishay Mansour
Tel-Aviv University
May 1996
Abstract
We analyze the performance of top-down algorithms for decision tree learning, such as those employed
by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms
are boosting algorithms. By this we mean that if the functions that label the internal nodes of the
decision tree can weakly approximate the unknown target function, then the top-down algorithms we
study will amplify this weak advantage to build a tree achieving any desired level of accuracy. The bounds
we obtain for this amplification show an interesting dependence on the splitting criterion used by the
top-down algorithm. More precisely, if the functions used to label the internal nodes have error 1=2
as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees
of size (1=*) O(1= 2 * 2 ) and (1=*) O(log(1=*)= 2 ) (respectively) suffice to drive the error below *. Thus (for
example), a small constant advantage over random guessing is amplified to any larger constant advantage
with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger
bound of (1=*) O(1= 2 ) (which is polynomial in 1=*) is obtained, which is provably optimal for decision
tree algorithms. The differing bounds have a natural explanation in terms of concavity properties of the
splitting criterion.
The primary contribution of this work is in proving that some popular and empirically successful
heuristics that are based on first principles meet the criteria of an independently motivated theoretical
model.
A preliminary version of this paper appears in Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of
Computing, pages 459-468, ACM Press, 1996. Authors' addresses:   M. Kearns,   AT&T Research,   600 Mountain Avenue, Room
2A-423, Murray Hill, New Jersey 07974;   electronic mail mkearns@research.att.com.   Y. Mansour,   Department of Computer
Science, Tel Aviv University,   Tel Aviv, Israel;   electronic mail mansour@math.tau.ac.il.   Y. Mansour was supported in part by
the Israel Science Foundation, administered by the Israel Academy of Science and Humanities, and by a grant of the Israeli
Ministry of Science and Technology.
+PAGE+

Update rules for parameter estimation in Bayesian networks
Eric Bauer
Stanford University
ebauer@cs.stanford.edu
Daphne Koller
Stanford University
koller@cs.stanford.edu
Yoram Singer
AT&T Labs
singer@research.att.com
Abstract
This paper re-examines the problem of parameter estimation in Bayesian networks with missing values and
hidden variables from the perspective of recent work in
on-line learning [12]. We provide a unified framework
for parameter estimation that encompasses both on-line
learning, where the model is continuously adapted to new
data cases as they arrive, and the more traditional batch
learning, where a pre-accumulated set of samples is used
in a one-time model selection process. In the batch case,
our framework encompasses both the gradient projection
algorithm [2, 3] and the EM algorithm [14] for Bayesian
networks. The framework also leads to new on-line and
batch parameter update schemes, including a parameterized version of EM. We provide both empirical and theoretical results indicating that parameterized EM allows
faster convergence to the maximum likelihood parame
ters than does standard EM.
1 Introduction

A Systematic Approach to
Host Interface Design for High-Speed Networks
Peter Steenkiste
School of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, Pennsylvania 15213-3891
Abstract
In recent years, networks with media rates of 100 Mbit/second or more have become widely available (FDDI, ATM,
HIPPI, ..). However, many computer systems cannot make use of the available bandwidth because of the high
overhead associated with network communication. In this paper we review the operations involved in communication over high-speed networks, and we describe optimizations of the network interface that improve network
throughput. We also discuss how the payoff of the optimizations is influenced by features of the host software and
architecture. This paper is based on our experience with the interfaces for the Nectar and Gigabit Nectar networks.
Keywords: network interfaces, high-speed networks, buffer management, memory hierarchy
This research was sponsored by the Defense Advanced Research Projects Agency (DOD) under contract number MDA972-90-C-0035, in part by the National Science Foundation and the Defense Advanced
Research Projects Agency under Cooperative Agreement NCR-8919038 with the Corporation for National Research Initiatives.
+PAGE+

Automatically Selecting and Using Primary Effects
in Planning: Theory and Experiments
Eugene Fink
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
eugene@cs.cmu.edu
http://www.cs.cmu.edu/~eugene
Qiang Yang
School of Computing Science
Simon Fraser University
Burnaby, BC V5A1S6, Canada
qyang@cs.sfu.ca
http://fas.sfu.ca/cs/people/Faculty/Yang
Abstract
The use of primary effects of operators is an effective approach to improving the
efficiency of planning. The characterization of "good" primary effects, however, has
remained at an informal level and there have been no algorithms for selecting primary
effects of operators.
We formalize the use of primary effects in planning and present a criterion for
selecting useful primary effects, which guarantees efficiency and completeness. We
analyze the efficiency of planning with primary effects and the quality of the resulting
plans.
We then describe a learning algorithm that automatically selects primary effects
and demonstrate, both analytically and empirically, that the use of this algorithm
significantly reduces planning time and does not compromise completeness.
Eugene Fink is supported by Wright Laboratory, Aeronautical Systems Center, Air Force Materiel
Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. Qiang Yang is supported by Natural Sciences and Engineering Research Council of Canada (NSERC)
under grant number OGP0184883.
+PAGE+

DYNAMIC COUPLING OF
UNDERACTUATED MANIPULATORS
Marcel Bergerman Christopher Lee Yangsheng Xu
The Robotics Institute
Carnegie Mellon University
Pittsburgh PA 15213
-mbergerm|chrislee|xu+-@cs.cmu.edu
Proceedings of the 4th IEEE Conference on Control Applications, Albany, USA, Sep. 1995, pp. 500-505.
Abstract
In recent years, researchers have been dedicated to the
study of underactuated manipulators which have more
joints than control actuators. In previous works,
assumptions were made as to the existence of enough
dynamic coupling between the active and the passive
joints of the manipulator for it to be possible to control
the position of the passive joints via the dynamic
coupling. In this work, the authors aim to develop an
index to measure the dynamic coupling, so as to address
when control of the underactuated system is possible,
and how the motion and robot configuration can be
designed. We discuss extensively the nature of the
dynamic coupling and of the proposed coupling index,
and their applications in the analysis and design of
underactuated systems, and in control and planning of
robot motion configuration.
1 Introduction

Unification and Polymorphism in Region Inference
Mads Tofte,   Department of Computer Science, University of Copenhagen
Lars Birkedal,   School of Computer Science, Carnegie Mellon University
Dedicated to Robin Milner on the occasion of his 60th birthday.
Abstract
Region Inference is a technique for inferring lifetimes of values in strict, higher-order programming languages such as Standard ML. The purpose of this paper is to show how ideas
from Milner's polymorphic type discipline can serve as a basis for region inference, even in the
presence of a limited form of polymorphic recursion.
1 Introduction

A Linear Spine Calculus
Iliano Cervesato and Frank Pfenning 1
April 10, 1997
CMU-CS-97-125
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Abstract
We present the spine calculus S !ffi&&gt; as an efficient representation for the linear -calculus !ffi&&gt;
which includes intuitionistic functions (!), linear functions (ffi), additive pairing (&), and additive unit
(&gt;). S !ffi&&gt; enhances the representation of Church's simply typed -calculus as abstract Bohm trees
by enforcing extensionality and by incorporating linear constructs. This approach permits procedures
such as unification to retain the efficient head access that characterizes first-order term languages without
the overhead of performing -conversions at run time. Potential applications lie in proof search, logic
programming, and logical frameworks based on linear type theories. We define the spine calculus, give
translations of !ffi&&gt; into S !ffi&&gt; and vice-versa, prove their soundness and completeness with respect
to typing and reductions, and show that the spine calculus is strongly normalizing and admits unique
canonical forms.
1 The authors can be reached at iliano@cs.cmu.edu and fp@cs.cmu.edu.

Sensor-based Registration and Stacking
of Electronic Substrate Layers
Andrew E. Brennemann, 1 Robert Hammer, 2
William V. Jecusco II, 3
and Ralph L. Hollis 4
IBM Research Division
Thomas J. Watson Research Center
Yorktown Heights, New York, USA
Abstract
Substrates for most of today's electronic products contain many wiring layers
which are individually fabricated, mechanically registered with one another,
and laminated together. Alignment tolerances of 0.05 mm to 0.1 mm are
sufficient to register the vertical connection pads or vias on each layer. More
aggressive designs of the future will, however, require manufacturing accuracies of at least an order of magnitude better to accommodate much finer
wire widths and pin spacings. Conventional equipment relying on mechanical
"pin-in-slot" methods will likely be inadequate, and a new approach will be
needed.
We describe here a sensor-based approach for registration and stacking
of electronic substrate sublaminates that replaces pin-in-slot methods, yet
does not require accurate automation equipment. A pilot work cell for this
approach is presented, which has an IBM 7576 coarse-positioning robot, a
specially-developed fine-positioning robot, optical sensors, and several routine
low accuracy fixtures. A novel robot bracing method was used to minimize
environmental vibration during sublaminate stacking.
Pairs of test sublaminates, each containing an identical pattern of 100
m holes, were aligned, stacked and bonded. The accuracy of registration
1 Retired,   4 Morningside Court, Ossining, NY, 10562.

Tactile Gestures for Human/Robot Interaction
Richard M. Voyles, Jr. Pradeep K. Khosla
Robotics Ph.D. Program
Dept. of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
7
Abstract
Gesture-Based Programming is a new paradigm to ease
the burden of programming robots. By tapping in to the
users wealth of experience with contact transitions,
compliance, uncertainty and operations sequencing, we
hope to provide a more intuitive programming environment
for complex, real-world tasks based on the expressiveness
of non-verbal communication. A requirement for this to be
accomplished is the ability to interpret gestures to infer the
intentions behind them. As a first step toward this goal, this
paper presents an application of distributed perception for
inferring a users intentions by observing tactile gestures.
These gestures consist of sparse, inexact, physical
nudges applied to the robots end effector for the
purpose of modifying its trajectory in free space. A set of
independent agents - each with its own local, fuzzified,
heuristic model of a particular trajectory parameter -
observes data from a wrist force/torque sensor to evaluate
the gestures. The agents then independently determine the
confidence of their respective findings and distributed
arbitration resolves the interpretation through voting.
1 Gesture-based programming

Fundamentals of Texture Mapping and Image Warping
Master's Thesis
under the direction of Carlo Sequin
Paul S. Heckbert
Dept. of Electrical Engineering and Computer Science
University of California,   Berkeley, CA 94720
c fl1989 Paul S. Heckbert
June 17, 1989
This Postscript version is missing about 40 paste-up figures. To get a complete version,
order report no. UCB/CSD 89/516 from the Computer Science Division at the address
above.
+PAGE+

Using a DEM to Determine Geospatial Object Trajectories
Robert T. Collins, Yanghai Tsin, J. Ryan Miller and Alan J. Lipton
The Robotics Institute, Carnegie Mellon University,   Pittsburgh, PA. 15213
Email: frcollins,ytsin,jmce,ajlgcs.cmu.edu
Abstract
This paper addresses the estimation of moving object trajectories within a geospatial coordinate system,
using a network of video sensors. A high-resolution
(0.5m grid spacing) digital elevation map (DEM) has
been constructed using a helicopter-based laser range-finder. Object locations are estimated by intersecting viewing rays from a calibrated sensor platform
with the DEM. Continuous object trajectories can then
be assembled from sequences of single-frame location
estimates using spatio-temporal filtering and domain
knowledge.
1 Introduction

Optimizing ML with Run-Time Code Generation
Mark Leone Peter Lee
December 1995
CMU-CS-95-205
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Abstract
We describe the design and implementation of a compiler that automatically translates ordinary
programs written in a subset of ML into code that generates native code at run time. Run-time
code generation can make use of values and invariants that cannot be exploited at compile time,
yielding code that is superior to statically optimal code. But the cost of optimizing and generating
code at run time can be prohibitive. We demonstrate how compile-time specialization can reduce
the cost of run-time code generation by an order of magnitude without greatly affecting code
quality. Several benchmark programs are examined, which exhibit an average cost of six cycles per
instruction generated at run time.
The authors' electronic mail addresses are   Mark.Leone@cs.cmu.edu and Peter.Lee@cs.cmu.edu.
This research was sponsored in part by the Advanced Research Projects Agency CSTO under the title "The Fox
Project: Advanced Langauges for Systems Software," ARPA Order No. C533, issued by ESC/ENS under Contract
No. F19628-95-C-0050. The views and conclusions contained in this document are those of the authors and should
not be interpreted as representing the official policies, either expressed or implied, of the Advanced Research Projects
Agency or the U.S. Government.
+PAGE+

Formal Aspects of Computing (1998) 3: 1-000
c 1998 BCS
Protective Interface Specifications
Gary T. Leavens 2 and Jeannette M. Wing 3
1 Department of Computer Science, Iowa State University,   Ames, IA 50011 USA
2 Computer Science Department, Carnegie Mellon University,   Pittsburgh, PA 15213 USA
Abstract. The interface specification of a procedure describes the procedure's
behavior using pre- and postconditions. These pre- and postconditions are written using various functions. If some of these functions are partial, or underspec-ified, then the procedure specification may not be well-defined.
We show how to write pre- and postcondition specifications that avoid such
problems, by having the precondition "protect" the postcondition from the effects
of partiality and underspecification. We formalize the notion of protection from
partiality in the context of specification languages like VDM-SL and COLD-K.
We also formalize the notion of protection from underspecification for the Larch
family of specification languages, and for Larch show how one can prove that a
procedure specification is protected from the effects of underspecification.
1. The Problem

A Field Guide to Boxology:
Preliminary Classification of Architectural Styles for
Software Systems
Mary Shaw and Paul Clements
Computer Science Department and Software Engineering Institute
Carnegie Mellon University
Pittsburgh, PA 15213
April 1996
Abstract:
Software architects use a number of commonly-recognized styles
to guide their design of system structures. Each of these is appropriate for
some classes of problems, but none is suitable for all problems. How, then,
does a software designer choose an architecture suitable for the problem at
hand? Two kinds of information are required: (1)
careful discrimination
among
the candidate architectures and (2)
design guidance
on how to make
appropriate choices. Here we support
careful discrimination
with a
preliminary classification of styles. We use a two-dimensional classification
strategy with control and data issues as the dominant organizing axes. We
position the major styles within this space and use finer-grained
discriminations to elaborate variations on the styles. This provides a
framework for organizing
design guidance
, which we partially flesh out with
rules of thumb.
Keywords:
software architecture, architectural styles, style classification/taxonomy
This document was created with FrameMaker 4.0.4
+PAGE+

Geometric Sensing of Known Planar Shapes
Yan-Bin Jia Michael Erdmann
The Robotics Institute and School of Computer Science
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213-3891
March 12, 1995
International Journal of Robotics Research, 15(4):365-392, 1996.
+PAGE+

Exploiting Redundancy to Reduce Impact Force
Jin-Oh Kim 2 , Matthew Wayne Gertz 3 , and Pradeep K. Khosla 4
Advanced Manipulators Laboratory
The Robotics Institute
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213
Abstract
This paper presents two strategies for reducing the impact force resulting from the collision of a
kinematically redundant manipulator with its environment, where it is assumed that the impact
event has some finite duration. The first, an impact control strategy, involves adding torques to the
joints of the redundant manipulator to impede motion into the environment with which it is colliding. The second, an impact planning strategy, involves choosing the configuration best suited for
minimizing the impact force from an impact event, the approximate location of which is known
ahead of time. Simulated results from both strategies are presented and discussed, and it is shown
that both are successful in minimizing the impact force resulting from planned and unplanned collisions.
1. This research was funded in part by NASA (grant number NAG-1-1075), the Dept. of Elec. and Comp. Engineering,

Remote Access to Interactive Media
Roger B. Dannenberg
Carnegie Mellon University, School of Computer Science
Pittsburgh, PA 15213 USA
Email: dannenberg@cs.cmu.edu
ABSTRACT
Digital interactive media augments interactive computing with video, audio, computer graphics and text,
allowing multimedia presentations to be individually and dynamically tailored to the user. Multimedia, and
particularly continuous media pose interesting problems for system designers, including those of latency
and synchronization. These problems are especially evident when multimedia data is remote and must be
accessed via networks. Latency and synchronization issues are discussed, and an integrated system,
Tactus, is described. Tactus facilitates the implementation of interactive multimedia computer programs by
managing latency and synchronization in the framework of an object-oriented graphical user interface
toolkit.
1. Introduction

Improving Programming-by-Demonstration With Better Semantic Expression
Thesis Proposal
Richard McDaniel
November 14, 1995
Abstract
The domain of applications that can be created with programming-by-demonstration
(PBD) can be extended by improving the developers ability to communicate with the system. The
techniques provided in this thesis will allow nonprogrammers to create a new variety of complete,
interactive applications including many board games and educational software using PBD.
A PBD software tool uses inferencing to induce programs by watching the developer demonstrate examples that show how the application should behave. Current systems reduce their scope
or resort to having the developer program because they do not provide sufficient ways to express
behaviors and the factors that affect them. Therefore, the goal of this thesis is to develop understandable forms of annotated expression and manipulation that help a system infer a broader range
of behavior. To test these ideas, this proposal introduces a new system called Gamut that will
present the techniques in a unified software tool.
The first technique replaces the macro recorder method for demonstrating behavior used
in other PBD systems with a technique called nudges. The developer demonstrates by correcting
the system at important points during program execution and also using two nudge commands to
communicate important situations. First, the Do Something! nudge causes the system to reconsider
past learned behavior and try to generalize its knowledge to fit the current situation. Using the
Stop That! nudge will point out improper behavior and generate negative examples.
Second, Gamut will use a new deck-of-playing-cards metaphor to express concepts such
as randomness, sequencing, and data storage. By constructing an appropriate deck, shufing, sorting, and playing cards at key moments, developers can incorporate many effects not available without programming in other systems.
Third, Gamut will improve communication about behaviors by making them more manipulable than in previous systems. Behaviors will be represented as small icons near the objects they
affect. Using the familiar cut, copy, and paste commands, the developer can transfer behavior
between objects. Determining how to make a behavior operate in the new context will be inferred
automatically. An objects state from the recent past will be represented as temporal ghosts in
which objects become dimmed, translucent images. Many sorts of behavior refer to prior states
such as a previous position or an old property value. The ghost objects will allow the developer to
make explicit connections.
Finally, to reduce the number of options the system must explore, the developer will be
able to give hints by highlighting important objects and properties. A new inferencing algorithm
will be created that will take advantage of the hints.
By combining these techniques, Gamut will provide a rich medium for expressing developer intentions, fostering greater communication between the PBD system and the developer and
enabling the developer to create highly interactive software with minimal programming expertise.
+PAGE+

A Whole Sentence
Maximum Entropy Language Model
R. Rosenfeld
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Abstract We introduce a new kind of language model, which models whole sentences or utterances directly using the Maximum Entropy
paradigm. The new model is conceptually simpler, and more naturally
suited to modeling whole-sentence phenomena, than the conditional ME
models proposed to date. By avoiding the chain rule, the model treats
each sentence or utterance as a "bag of features", where features are
arbitrary computable properties of the sentence. The model is unnor-malizable, but this does not interfere with training (done via sampling)
or with use. Using the model is computationally straightforward. The
main computational cost of training the model is in generating sample
sentences from a Gibbs distribution. Interestingly, this cost has different dependencies, and is potentially lower, than in the comparable
conditional ME model.
1 Motivation

TOLERATING LATENCY THROUGH
SOFTWARE-CONTROLLED DATA PREFETCHING
a dissertation
submitted to the   department of electrical engineering +L +
and the committee on graduate studies
of   stanford university
in partial fulfillment of the requirements
for the degree of
doctor of philosophy
By
Todd C. Mowry
March 1994
+PAGE+

Tolerating Latency Through Software-Controlled Prefetching
in Shared-Memory Multiprocessors
Todd Mowry and Anoop Gupta
Computer Systems Laboratory
Stanford University, CA 94305
To appear in the Journal of Parallel and Distributed Computing, June 1991.
Abstract
The large latency of memory accesses is a major obstacle in obtaining high processor utilization in large
scale shared-memory multiprocessors. Although the provision of coherent caches in many recent machines
has alleviated the problem somewhat, cache misses still occur frequently enough that they significantly lower
performance. In this paper we evaluate the effectiveness of non-binding software-controlled prefetching, as
proposed in the Stanford DASH Multiprocessor, to address this problem. The prefetches are non-binding in
the sense that the prefetched data is brought to a cache close to the processor, but is still available to the cache
coherence protocol to keep it consistent. Prefetching is software-controlled since the program must explicitly
issue prefetch instructions.
The paper presents results from detailed simulation studies done in the context of the Stanford DASH
multiprocessor. Our results show that for applications with regular data access patterns|we evaluate a particle-based simulator used in aeronautics and an LU-decomposition application|prefetching can be very effective.
It was easy to augment the applications to do prefetching and it increased their performance by 100-150% when
we prefetched directly into the processor's cache. However, for applications with complex data usage patterns,
prefetching was less successful. After much effort, the performance of a distributed-time logic simulation
application that made extensive use of pointers and linked lists could be increased only by 30%. The paper
also evaluates the effects of various hardware optimizations such as separate prefetch issue buffers, prefetching
with exclusive ownership, lockup-free caches, and weaker memory consistency models on the performance of
prefetching.
1 Introduction

Predicting Data Cache Misses in Non-Numeric Applications
Through Correlation Profiling
Todd C. Mowry Chi-Keung Luk
Department of Computer Science Department of Computer Science
Carnegie Mellon University University of Toronto
Pittsburgh, PA 15213 Toronto, Canada M5S 3G4
tcm@cs.cmu.edu luk@eecg.toronto.edu
Abstract
To maximize the benefit and minimize the overhead of software-based latency tolerance techniques,
we would like to apply them precisely to the set of
dynamic references that suffer cache misses. Unfortunately, the information provided by the state-of-the-art cache miss profiling technique (summary profiling)
is inadequate for references with intermediate miss
ratios|it results in either failing to hide latency, or
else inserting unnecessary overhead. To overcome this
problem, we propose and evaluate a new technique|
correlation profiling|which improves predictability by
correlating the caching behavior with the associated dynamic context. Our experimental results demonstrate
that roughly half of the 22 non-numeric applications
we study can potentially enjoy significant reductions
in memory stall time by exploiting at least one of the
three forms of correlation profiling we consider.
1 Introduction

Learning Maps for Indoor Mobile Robot Navigation
Sebastian Thrun
Computer Science Department and Robotics Institute
Carnegie Mellon University, Pittsburgh
Accepted for Publication in Artificial Intelligence
Abstract
Autonomous robots must be able to learn and maintain models of their environments.
Research on mobile robot navigation has produced two major paradigms for mapping indoor
environments: grid-based and topological. While grid-based methods produce accurate
metric maps, their complexity often prohibits efficient planning and problem solving in
large-scale indoor environments. Topological maps, on the other hand, can be used much
more efficiently, yet accurate and consistent topological maps are often difficult to learn
and maintain in large-scale environments, particularly if momentary sensor data is highly
ambiguous. This paper describes an approach that integrates both paradigms: grid-based
and topological. Grid-based maps are learned using artificial neural networks and naive
Bayesian integration. Topological maps are generated on top of the grid-based maps, by
partitioning the latter into coherent regions. By combining both paradigms, the approach
presented here gains advantages from both worlds: accuracy/consistency and efficiency.
The paper gives results for autonomous exploration, mapping and operation of a mobile
robot in populated multi-room environments.
? This research was sponsored in part by the National Science Foundation under award IRI-9313367, and by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel
Command, USAF, and the Darpa Advanced Research Projects Agency (DARPA) under
grant number F33615-93-1-1330. We also acknowledge financial support by Daimler Benz
Corp.
Preprint submitted to Elsevier Science 15 September 1997
+PAGE+

WWW Electronic Commerce and Java Trojan Horses
J. D. Tygar Alma Whitten
tygar@cs.cmu.edu alma@cs.cmu.edu
Carnegie Mellon University
Pittsburgh, PA 15213
Abstract
World Wide Web electronic commerce applications
often require consumers to enter private information (such as credit card numbers) into forms in the
browser window. If third parties can insert trojan
horse applications onto a consumer's machine, they
can monitor keyboard strokes and steal private information.
This paper outlines a simple way to accomplish
this using Java or similar remote execution facilities.
We implemented a simple version of this attack. We
give a general method, window personalization, that
can thwart or prevent this attack.
1 Introduction

Bayesian Analysis of Variance Component Models via Rejection
Sampling
Russell D. Wolfinger
SAS Institute Inc.,   SAS Campus Drive,
Cary, NC 27513, U.S.A.
and Robert E. Kass
Department of Statistics, Carnegie Mellon University
Pittsburgh, PA 15213, U.S.A.
January, 1996
Abstract
We consider the usual Normal linear mixed model for "components of variance" from a Bayesian
viewpoint. Instead of using Gibbs sampling or other Markov Chain schemes that rely on full
conditional distributions, we propose and investigate a method for simulating from posterior distributions based on rejection sampling. The method applies with arbitrary prior distributions but
we also employ as a default reference prior a version of Jeffreys's prior based on the integrated
("restricted") likelihood. We demonstrate the ease of application and flexibility of this approach
in several familiar settings, even in the presence of unbalanced data. A program implementing the
algorithm discussed here will be available in the SAS MIXED procedure.
Some key words: Jeffreys's prior, Mixed model, Posterior simulation, Reference prior, REML.
+PAGE+

Backfitting in Smoothing Spline ANOVA
By Zhen Luo 1
Pennsylvania State University
Abstract
A scheme to compute smoothing spline ANOVA estimates for large data sets with a (near)
tensor-product structure is proposed. Such data sets are common in spatial-temporal analysis and
image analysis. This scheme combines backfitting algorithm with iterative imputation algorithm in
order to save both computational space and time. The convergence of this algorithm and various
ways to further speed it up, such as collapsing component functions and successive over-relaxation,
are discussed. Issues related to its application in spatial-temporal analysis are discussed too. An
application to a global analysis of historical surface temperature data is described.
1 Introduction

Working Paper IS-98-01 (Information Systems)
Leonard N. Stern School of Business, New York University.
In: Proceedings of the IEEE/IAFE/INFORMS Conference on Computational Intelligence for Financial Engineering
(CIFEr'98, New York, March 1998)
http://www.stern.nyu.edu/~aweigend/Research/Papers/TradeStyles
Uncovering Hidden Structure in Bond Futures Trading
Fei CHEN , Stephen FIGLEWSKI ,
Jeffrey HEISLER zz , Andreas S. WEIGEND
Abstract. This study uncovers trading styles in the transaction records of US Treasury bond futures.
It uses transaction-by-transaction data from the Commodity Futures Trading Commissions' (CFTC)
Computerized Trade Reconstruction (CTR) records. The data set consists of 30 million transaction|
the complete US T-bond futures market for 3 years. Each transaction record consists of time (by the
minute), price, volume, buy/sell, and an identifier of the specific account.
We use statistical clustering techniques to group together trades that are similar. Two sets of
assumptions have to be made: (1) What is a trade? We define a trade to begin when an account opens
a position, and to end when its position size returns to zero. We describe each trade by several trade-specific variables (e.g., length of trade, maximum position size, opening move, long or short) and several
exogenous, market-specific variables (e.g., price, volatility, trading volume). (2) What process generated
the data? We assume a mixture of Gaussians. An observed trade is interpreted as a noisy realization of
one of the mixture components. This paper assumes identity covariance matrices. Furthermore, each
trade is fully assigned to a single cluster. We compare this approach to diagonal and to full covariance
structure with probabilistic assignments.
Trade profit was held back in the clustering process. It turns out that the clusters differ significantly in their profit and risk characteristics. Using conditional distributions, we summarize features
of profitable trading styles and contrast them with losing strategies. We find that profitable styles tend
to hold trades longer, trade at higher volatility, and trade earlier in the contracts. We also show how
some clusters uncover "technical" traders. Using the information about the individual accounts, the
assignments of accounts to clusters are described by entropy, and the transitions of a given account
through clusters is modeled by a first order Markov model.
1 Motivation and Overview

Problem Solving for Redesign
Anita Pos 1 and Hans Akkermans 1 and Remco Straatman 2
1 University of Twente (UT)
Department of Computer Science
P.O. Box 217
NL-7500 AE Enschede
The Netherlands
E-mail: fpos,akkermang@cs.utwente.nl
2 University of Amsterdam (UvA)
Department of Social Science Informatics (SWI)
Roetersstraat 15
1081 WB Amsterdam
The Netherlands
E-mail: remco@swi.psy.uva.nl
Abstract. A knowledge-level analysis of complex tasks like diagnosis and design can give us a better understanding of these tasks in terms of the goals they
aim to achieve and the different ways to achieve these goals. In this paper we
present a knowledge-level analysis of redesign. Redesign is viewed as a family of
methods based on some common principles, and a number of dimensions along
which redesign problem solving methods can vary are distinguished. By examining the problem-solving behavior of a number of existing redesign systems and approaches, we came up with a collection of problem-solving methods for redesign
and developed a task-method structure for redesign.
In constructing a system for redesign a large number of knowledge-related choices
and decisions are made. In order to describe all relevant choices in redesign problem solving, we have to extend the current notion of possible relations between
tasks and methods in a PSM architecture. The realization of a task by a problem-solving method, and the decomposition of a problem-solving method into subtasks are the most common relations in a PSM architecture. However, we suggest
to extend these relations with the notions of task refinement and method refinement. These notions represent intermediate decisions in a task-method structure,
in which the competence of a task or method is refined without immediately paying attention to its operationalization in terms of subtasks. Explicit representation
of this kind of intermediate decisions helps to make and represent decisions in a
more piecemeal fashion.
1 Introduction

An Extensible Protocol Architecture for
Application-Specific Networking
Marc E. Fiuczynski
Brian N. Bershad
fmef,bershadg@cs.washington.edu
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
Abstract
Plexus is a networking architecture that allows applications to achieve high performance with customized
protocols. Application-specific protocols are written in
a typesafe language and installed dynamically into the
operating system kernel. Because these protocols execute within the kernel, they can access the network
interface and other operating system services with low
overhead. Protocols implemented with Plexus outperform equivalent protocols implemented on conventional monolithic systems. Plexus runs in the context
of the SPIN extensible operating system.
1 Introduction

-1
The OPENET Architecture
Israel Cidon
Tony Hsiao
Asad Khamisy
Abhay Parekh
Raphael Rom
Moshe Sidi
SMLI TR-95-37   December 1995
Abstract:
ATM networks will soon be moving from the experimental stage of test-beds to a commercial state
where production networks are deployed and operated. The progress of ATM networks appears to be
at risk due to the lack of a universal, open, and efficient ATM network control platform. The emerging
Private Network to Network Interface (PNNI) standard introduces a control platform that can be used
as an internetwork and possibly as an intra-network solution. However, the current PNNI still falls short
in providing an acceptable universal solution, due to lack of performance optimizations for intra-network operation, limited functionality, and the lack of open interfaces for future functional extensions
and services.
OPENET is a common portable, open, and high-performance network control platform based on performance and functional enhancements to the PNNI standard. It is vendor-independent, scalable (in
terms of network size and volume of calls), high-performance (in terms of call processing latency and
throughput), and extensible (in terms of integrating customer-specific and value-added services).
OPENET is designed as an extension to current PNNI so it can serve as a next generation PNNI. It is
compatible with PNNI in the internetworking environment allowing large networks to be partitioned
according to natural topological or organizational boundaries rather than the artificial use of internet-work interfaces at vendor boundaries.
This report describes the OPENET architecture. The major novelties of the OPENET architecture compared to the current PNNI are: the use of native ATM switching for the dissemination of utilization
updates; lightweight call setup; take down and modification signaling; a new signaling paradigm that
better supports fast reservation and multicast services; and a rich signaling infrastructure that enables
the development of augmented services (such as mobility, directory, etc.), leveraging the existing functions of the network control platform.
email address:
raphael.rom@eng.sun.com
M/S 29-01
2550 Garcia Avenue
Mountain View, CA 94043
+PAGE+

Two Computer Systems Paradoxes: Serialize-to-Parallelize,
and Queuing Concurrent-Writes
Rimon Orni and Uzi Vishkin
September 17, 1995
Abstract
We present and examine the following Serialize-to-Parallelize Paradox: suppose a
programmer has a parallel algorithm in mind; the programmer must serialize the algorithm, and is actually trained to suppress its parallelism, while writing code; later,
however, compilation and runtime techniques are used to reverse the results of this serialization effort and extract as much parallelism as possible. This work actually provides
examples where parallel or parallel-style code enables extracting more parallelism than
standard serial code.
The "arbitrary concurrent-write" convention is useful in parallel algorithms and programs and appears to be not too difficult to implement in hardware for serial machines.
Still, typically concurrent-writes to the same memory location in a program are implemented by queuing the write operations, thus requiring time linear in the number of
writes. We call this the Queuing Concurrent-Writes Paradox.
Assuming that providing useful, easy-to-program programming paradigms to improve the overall effectiveness of computer systems is of interest, this work is a modest
example for applying such software-driven considerations to computer architecture issues. This work may be the first to relate parallel algorithms and parallel programming
with the technology of instruction level parallelism.
1 Introduction

Mobile Robot Localization using Landmarks
Margrit Betke
Massachusetts Institute of Technology
Laboratory for Computer Science
Cambridge, MA 02139
Leonid Gurvits
NEC Research Institute
4 Independence Way
Princeton, NJ 08540
April 27, 1995
Abstract
We describe an efficient method for localizing a mobile robot in an environment with landmarks. We assume that the robot can identify these landmarks
and measure their bearings relative to each other. Given such noisy input, the
algorithm estimates the robot's position and orientation with respect to the
map of the environment. The algorithm makes efficient use of our representation of the landmarks by complex numbers. The algorithm runs in time linear
in the number of landmarks. We present results of simulations and propose
how to use our method for robot navigation.
Keywords: Robotics, mobile robot localization, landmark navigation, map
algorithms, triangulation.
Part of this research was done while the author was visiting Siemens Corporate Research. The
author is also supported by NSF grant ASC-9217041. Author's net address:   margrit@lcs.mit.edu
Author's net address:   gurvits@research.nj.nec.com
+PAGE+

Learning and Vision Algorithms for
Robot Navigation
by
Margrit Betke
S.M., Massachusetts Institute of Technology (1992)
Submitted to the   Department of Electrical Engineering and Computer
Science   in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Electrical Engineering and Computer Science
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
June 1995
c Massachusetts Institute of Technology 1995. All rights reserved.
Author : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
Department of Electrical Engineering and Computer Science
May 18, 1995
Certified by : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
Ronald L. Rivest
Professor
Thesis Supervisor
Accepted by : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
F. R. Morgenthaler
Chairman, Department Committee on Graduate Students
+PAGE+

CAR-TR-858
N00014-95-1-0521
June 1997
Information-Conserving Object Recognition
Margrit Betke and Nicholas C. Makris :
Computer Vision Laboratory
Center for Automation Research
University of Maryland
College Park, MD 20742-3275
Abstract
The problem of recognizing objects imaged in complex real-world scenes is examined from
a parametric perspective using the theory of statistical estimation. A scalar measure of an
object's complexity, which is invariant under affine transformation and changes in image noise
level, is extracted from the object's Fisher information. The volume of Fisher information is
shown to provide an overall statistical measure of the object's recognizability in a particular
image, while the complexity provides an intrinsically physical measure that characterizes the
object in any image. An information-conserving method is then developed for recognizing
an object imaged in a complex scene. Here the term "information-conserving" means that
the method uses all the measured data pertinent to the object's recognizability, attains the
theoretical lower bound on estimation error for any unbiased estimate of the parameter vector
describing the object, and therefore is statistically optimal. This method is then successfully
applied to finding objects imaged in thousands of complex real-world scenes.
The support of the Office of Naval Research under Contract N00014-95-1-0521 is gratefully acknowledged. Author's new address (starting September 1997):   Department of Computer Science, Boston College,
Fulton Hall, Chestnut Hill, MA 02167.   Email: betke@cs.bc.edu
N. C. Makris was with the Naval Research Laboratory, Washington, DC 20375. His new address is   Department of Ocean Engineering, Massachusetts Institute of Technology,   Cambridge, MA 02138.
+PAGE+

Augmenting Collective Adaptation
with Simple Process Agents
Thomas Haynes
Department of Mathematical & Computer Sciences
600 South College Ave.
The University of Tulsa
Tulsa, OK 74104-3189
e-mail: haynes@euler.mcs.utulsa.edu
Abstract
We have integrated the distributed search of genetic
programming based systems with collective memory
to form a collective adaptation search method. Such a
system significantly improves search as problem complexity is increased. However, there is still considerable scope for improvement. In collective adaptation, search agents gather knowledge of their environment and deposit it in a central information repository. Process agents are then able to manipulate that
focused knowledge, exploiting the exploration of the
search agents. We examine the utility of increasing
the capabilities of the centralized process agents.
Introduction

Parallel Algorithms
Guy E. Blelloch and Bruce M. Maggs
School of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
guyb@cs.cmu.edu, bmm@cs.cmu.edu
Introduction

Trainable Cataloging for Digital Image Libraries with Applications
to Volcano Detection
M.C. Burl yz , U.M. Fayyad , P. Perona , P. Smyth
California Institute of Technology Jet Propulsion Laboratory
MS 116-81 | Pasadena, CA 91125 MS 525-3660 | Pasadena, CA 91109
fburl,peronag@systems.caltech.edu ffayyad,pjsg@aig.jpl.nasa.gov
Computation and Neural Systems Technical Report
CNS-TR-96-01 |   October 2, 1996
Abstract
Users of digital image libraries are often not interested in image data per se but in derived
products such as catalogs of objects of interest. Converting an image database into a usable
catalog is typically carried out manually at present. For many larger image databases the
purely manual approach is completely impractical. In this paper we describe the development
of a trainable cataloging system: the user indicates the location of the objects of interest for
a number of training images and the system learns to detect and catalog these objects in the
rest of the database. In particular we describe the application of this system to the cataloging
of small volcanoes in radar images of Venus. The volcano problem is of interest because of the
scale (30,000 images, order of 1 million detectable volcanoes), technical difficulty (the variability
of the volcanoes in appearance) and the scientific importance of the problem. The problem of
uncertain or subjective ground truth is of fundamental importance in cataloging problems of this
nature and is discussed in some detail. Experimental results are presented which quantify and
compare the detection performance of the system relative to human detection performance. The
paper concludes by discussing the limitations of the proposed system and the lessons learned of
general relevance to the development of digital image libraries.
Keywords: digital image libraries, pattern recognition, science data analysis, volcanoes, Venus,
SAR, detection, classification, learning, remote sensing
+PAGE+

Perfect Simulation of some Point Processes
for the Impatient User
Elke Thonnes
Department of Statistics, University of Warwick
February 9, 1998
Abstract
Recently Propp and Wilson [14] have proposed an algorithm, called
Coupling from the Past (CFTP), which allows not only an approximate but perfect (i.e. exact) simulation of the stationary distribution
of certain finite state space Markov chains. Perfect Sampling using
CFTP has been successfully extended to the context of point processes, amongst other authors, by Haggstrom et al. [5]. In [5] Gibbs
sampling is applied to a bivariate point process, the penetrable spheres
mixture model [19]. However, in general the running time of CFTP
in terms of number of transitions is not independent of the state sampled. Thus an impatient user who aborts long runs may introduce a
subtle bias, the user impatience bias. Fill [3] introduced an exact sampling algorithm for finite state space Markov chains which, in contrast
to CFTP, is unbiased for user impatience. Fill's algorithm is a form
of rejection sampling and similar to CFTP requires sufficient mono-tonicity properties of the transition kernel used. We show how Fill's
version of rejection sampling can be extended to an infinite state space
context to produce an exact sample of the penetrable spheres mixture
process and related models. Following [5] we use Gibbs sampling and
make use of the partial order of the mixture model state space. Thus
Research supported by EPSRC earmarked studentship and University of Warwick
graduate award. Postal address:   Dept. of Statistics, University of Warwick,   Coventry,
CV4 7AL, UK
+PAGE+

127
Progress for Local Variables in UNITY
Rob Udink and Ted Herman and Joost Kok
Department of Computer Science, Utrecht University,
P.O. Box 80089, 3508 TB Utrecht, The Netherlands
e-mail: frob,ted,joostg@cs.ruu.nl
A new notion of refinement for UNITY programs with local variables is defined. This
notion is compositional in the following sense: programs can be refined in arbitrary contexts such that all unless and leadsto properties (i.e. temporal properties for both safety
and progress) of the composition are preserved. The refinement notion is based on preservation of a new kind of UNITY-like property that takes into account the locality of
variables. We do a small case study about registers.
Keyword Codes: F.3.1
Keywords: Specifying and Verifying and Reasoning about Programs
1. INTRODUCTION

On the Relation Between Unity Properties
and Sequences of States
R.T. Udink
Utrecht University, Department of Computer Science,
P.O. Box 80.089, 3508 TB Utrecht, the Netherlands
J.N. Kok
Utrecht University, Department of Computer Science,
P.O. Box 80.089, 3508 TB Utrecht, the Netherlands
ABSTRACT Stepwise refinement of programs has proven to be a suitable
method for developing parallel and distributed programs. We examine and compare a
number of different notions of program refinement for Unity. Two of these notions are
based on execution sequences. Refinement corresponds to the reduction of the set of execution sequences, i.e. reducing the amount of nondeterminism. The other refinement
notions are based on Unity properties as introduced by Chandy and Misra. The Unity approach is to refine specifications. Although it has proven a suitable formalism for deriving
algorithms, it seems less suitable for handling implementation details. Following Sanders
and Singh, we formalize program refinement in the Unity framework as the preservation
of Unity properties. We show that Unity properties are not powerful enough to characterize execution sequences. As a consequence, the notion of property-preserving refinement
differs from the notion of reducing the set of execution sequences.
Keywords Semantic models, Unity, program refinement.
CONTENTS
0 Introduction

Adaptive Information Filtering using Evolutionary Computation
D.R. Tauritz & J.N. Kok & I.G. Sprinkhuizen-Kuyper
Department of Computer Science, Leiden University
P.O. Box 9512, 2300 RA Leiden, The Netherlands
1 Introduction

SuperWeb: Towards a Global Web-Based Parallel Computing Infrastructure
Albert D. Alexandrov, Maximilian Ibel, Klaus E. Schauser, and Chris J. Scheiman
Department of Computer Science
University of California, Santa Barbara
Santa Barbara, CA 93106
fberto,ibel,schauser,chrissg@cs.ucsb.edu
Abstract
The Internet, best known by most users as the WorldWide-Web, continues to expand at an amazing pace. We
propose a new infrastructure to harness the combined resources, such as CPU cycles or disk storage, and make them
available to everyone interested. This infrastructure has the
potential for solving parallel supercomputing applications
involving thousands of cooperating components. Our approach is based on recent advances in Internet connectivity
and the implementation of safe distributed computing embodied in languages such as Java.
We developed a prototype of a global computing infrastructure, called SuperWeb, that consists of hosts, brokers
and clients. Hosts register a fraction of their computing resources (CPU time, memory, bandwidth, disk space) with
resource brokers. Client computations are then mapped by
the broker onto the registered resources. We examine an economic model for trading computing resources, and discuss
several technical challenges associated with such a global
computing environment.
Keywords: Global computing, Internet, Java, WorldWide-Web, massively parallel computing, secure computing,
microeconomic model.
1 Introduction
