
Protocols for Collecting Responses
in Multi-hop Radio Networks
Chungki Lee James E. Burns
Mostafa H. Ammar
GIT-CC-92/28
June 1992
Abstract
The problem of collecting responses in multi-hop radio networks is considered. A given node, called the source, is to collect a specified number of
responses from nodes in a radio network. The problem arises in several
applications of distributed systems. A deterministic and a randomized protocol for the problem are presented. The two protocols are analyzed and
their performance is compared. Conclusions are drawn about the suitability
of our protocols in various network environments.
College of Computing
Georgia Institute of Technology
Atlanta, Georgia 30332-0280
+PAGE+

GIT-CC-92/60
A Model-Based Approach to
Analogical Reasoning and Learning in Design
Sambasiva R. Bhatta
bhatta@cc.gatech.edu
A THESIS PROPOSAL
Presented to
The Academic Faculty
In Partial Fulfillment
of the Requirements for the Degree
Doctor of Philosophy
in Information and Computer Science
The Committee:
Dr. Ashok Goel (Advisor)
Dr. Richard Catrambone (Psy)
Dr. T. Govindaraj (ISyE)
Dr. Janet Kolodner
Dr. Ashwin Ram
Georgia Institute of Technology
November 1992
This work has been supported by research grants from the Office of Naval Research (contract
N00014-92-J-1234) and NSF, a CER grant from NSF (grant CCR-86-19886), and equipment
donated by IBM, NCR, and Symbolics.
+PAGE+

Fast Barrier Synchronization in Wormhole k-ary n-cube Networks
with Multidestination Worms
Dhabaleswar K. Panda
Dept. of Computer and Information Science
The Ohio State University,   Columbus, OH 43210-1277
Tel: (614)-292-5199, Fax: (614)-292-2911
E-mail: panda@cis.ohio-state.edu
Abstract
Proc. of the Int'l Symposium on High Performance
Computer Architecture (HPCA '95), pp. 200-209.
This paper presents a new approach to implement fast barrier synchronization in wormhole k-ary
n-cubes. The novelty lies in using multidestination
messages instead of the traditional single destination
messages. Two different multidestination worm types,
gather and broadcasting, are introduced to implement
the report and wake-up phases of barrier synchronization, respectively. Algorithms for complete and arbitrary set barrier synchronization are presented using
these new worms. It is shown that complete barrier
synchronization in a k-ary n-cube system with e-cube
routing can be implemented with 2n communication
start-ups as compared to 2n log 2 k start-ups needed
with unicast-based message passing. For arbitrary set
barrier, an interesting trend is observed where the synchronization cost keeps on reducing beyond a certain
number of participating nodes.
1 Introduction

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. XX, NO. Y, MONTH 1999 1
A Trip-based Multicasting Model in
Wormhole-routed Networks with Virtual Channels
Yu-Chee Tseng, Dhabaleswar K. Panda, Member, IEEE, and Ten-Hwang Lai, Member, IEEE
Abstract| This paper focuses on efficient multicasting in
wormhole-routed networks. A trip-based model is proposed
to support adaptive, distributed, and deadlock-free multiple
multicast on any network with arbitrary topology using at
most two virtual channels per physical channel. This model
significantly generalizes the path-based model proposed earlier [21], [22], which works only for Hamiltonian networks
and can not be applicable to networks with arbitrary topology resulted due to system faults. Fundamentals of the trip-based model, including the necessary and sufficient condition to be deadlock-free, and the use of appropriate number
of virtual channels to avoid deadlock are investigated. The
potential of this model is illustrated by applying it to hyper-cubes with faulty nodes. Simulation results indicate that the
proposed model can implement multiple multicast on faulty
hypercubes with negligible performance degradation.
Keywords| Routing algorithm, Interprocessor communication, Multicast, Virtual channel, Wormhole-routing,
Path-based routing, Collective communication, and Fault
tolerance.
I. Introduction

Clustering and Intra-Processor Scheduling for
Explicitly-Parallel Programs on Distributed-Memory Systems
Vibha A. Dixit-Radiya and Dhabaleswar K. Panda
OSU-CISRC-3/93-TR11
Updated on February 7, 1994
A short version of this report will appear in
International Parallel Processing Symposium, 1994.
+PAGE+

Submitted to SIGGRAPH95.
Space Deformation using Ray Deectors
Yair Kurzion and Roni Yagel
Department of Computer and Information Science
The Ohio State University
Abstract
In this paper we introduce a new approach to the deformation of surface and raster
models in two and three dimensions. Rather then deforming the objects in the
model, we deform the rays used to render the scene. The mechanism to specify the
deformation, which we call a deector, is a vector of gravity positioned in space.
This gravity vector bends any ray that travels through its field of gravity. Images
generated by these curved rays give the impression of a deformed space. Unlike
previous methods that deform all the objects in the scene, our approach deforms
only those parts of the model that contribute to the final image. In addition, using
deectors, our approach can deform any object type that can be rendered by a ray
casting algorithm, providing a unified solution to space deformation.
1. Introduction

An Asymptotically Optimal Minimum
Degree Ordering of Regular Grids
B. Kumar, P. Sadayappan, C.-H. Huang
Department of Computer and Information Science
The Ohio State University,
Columbus, OH 43210
Abstract
It has previously been shown that there exists a minimum degree ordering for regular grids that is considerably worse than nested dissection
in terms of fill-in and operations for factorization [1]. This paper proves
the existence of a minimum degree ordering for regular grids that has the
same optimal asymptotic order complexity for fill-in and operation count
as nested dissection. The analysis is verified by showing exact match between analytical prediction and experimental measurement. The analysis
motivates a peripheral preordering strategy for use with the popular multiple minimum degree (MMD) algorithm, and is shown to consistently
reduce fill-in and operation count for regular grids.
Keywords: Sparse Matrices, Finite Element Grids, Minimum Degree
Ordering, Computational Complexity.
AMS(MOS) subject classifications: 65F05, 65F50, 68R10
1 Introduction

Efficient Rasterization of Implicit Functions
Torsten Mller and Roni Yagel
Department of Computer and Information Science
The Ohio State University
Columbus, Ohio
-moeller, yagel-@cis.ohio-state.edu
Abstract
Implicit curves are widely used in computer graphics because of their powerful features for modeling and their ability for general function description. The most popular rasterization techniques for implicit curves are space subdivision and curve
tracking. In this paper we are introducing an efficient curve tracking algorithm that
is also more robust then existing methods. We employ the Predictor-Corrector
Method on the implicit function to get a very accurate curve approximation in a
short time. Speedup is achieved by adapting the step size to the curvature. In addition, we provide mechanisms to detect and properly handle bifurcation points,
where the curve intersects itself. Finally, the algorithm allows the user to trade-off
accuracy for speed and vice a versa. We conclude by providing examples that dem-onstrate the capabilities of our algorithm.
1. Introduction

Multimedia Database Systems: Challenges and
Opportunities
Yelena Yesha
Dept. of Computer Science
University of Maryland
5401 Wilkens Avenue
Baltimore, MD
Mukesh Singhal
Department of Computer
and Information Science
The Ohio State University
2015 Neil Avenue
Columbus, OH 43210
Abstract
Since multimedia systems typically require storage, retrieval, and manipulation
of huge amount of information, database systems play key role in the design of high-performance multimedia systems. This article examines the issues in the design of
multimedia database systems and explores the current state of the art.
Key Words: Multimedia systems, multimedia databases, information systems.
+PAGE+

Alleviating Consumption Channel Bottleneck
in Wormhole-Routed k-ary n-cube Systems 1
Debashis Basak and Dhabaleswar K. Panda
Dept. of Computer and Information Science
The Ohio State University
Columbus, OH 43210-1277
Tel: (614)-292-5199, Fax: (614)-292-2911
Email: fbasak,pandag@cis.ohio-state.edu
Abstract
This paper identifies performance degradation in wormhole routed k-ary n-cube networks due
to limited number of router-to-processor consumption channels at each node. Many recent research
in wormhole routing have advocated the advantages of adaptive routing and virtual channel flow
control schemes to deliver better network performance. This paper indicates that the advantages
associated with these schemes can not be realized with limited consumption capacity. To alleviate
such performance bottleneck, a new network interface design using multiple consumption channels
is proposed. To match virtual multiplexing on network channels, we also propose each consumption
channel to support multiple virtual consumption channels. The impact of message arrival rate at
a node on the required number of consumption channel is studied analytically. It is shown that
wormhole networks with higher routing adaptivity, dimensionality, degree of hot-spot traffic, and
number of virtual lanes have to take advantage of multiple consumption channels to deliver better
performance. The interplay between system topology, routing algorithm, number of virtual lanes,
messaging overheads, and communication traffic is studied through simulation to derive the effective
number of consumption channels required in a system. Using the on-going technological trend, it is
shown that wormhole-routed systems can use up to 2-4 consumption channels per node to deliver
better system performance.
Keywords: Parallel computer architecture, wormhole routing, k-ary n-cube, consumption Channel, virtual Channel, deterministic routing, adaptive routing, hot-spot traffic, performance evaluation, and interprocessor communication.
1 This research is supported in part by NSF Grant MIP-9309627, Faculty Early Career Development Award MIP-9502294, and an Ohio State University Presidential Fellowship. A preliminary version of this paper[4] has been

Classes as Assertions
Neelam Soundarajan
Computer and Information Science
The Ohio State University
Columbus, OH 43210
e-mail: neelam@cis.ohio-state.edu
Abstract: How do we formally specify the relation between a base class and a
derived class? This question has two parts, a syntactic one, and a semantic one.
The syntactic part is of course the easier of the two and the answer to that part is
the standard contra/co- variance requirement on the arguments and result of any
base class method redefined in the derived class. Our concern in the current paper
is with the semantic part of the question, i.e., how do we specify the behavioral
relation between the base class and the derived class? We show that the standard
answer -which is the semantic counterpart of contra/co-variance- is too rigid, and
does not allow some natural and common forms of inheritance. We then propose a
more flexible way to specify the relation, and show how different types of behavioral relations between base classes and derived classes may be specified using our
notation.
+PAGE+

A Certificate Path Generation Algorithm for
Authenticated Signaling in ATM Networks
Jun Xu Mukesh Singhal
Department of Computer and Information Science
The Ohio State University
Columbus, OH 43210
fjun,singhalg@cis.ohio-state.edu
Abstract
ATM Forum specifies public key cryptography to be the default ATM authentication mechanism and directory services like X.509 to be the infrastructure for public
key distribution and certification. Authenticated signaling, widely acknowledged
as a necessary security feature of ATM network, requires the signaling message to
be authenticated with a digital signature signed by the private key of the calling
party. To verify the digital signature, the called party needs to obtain the public
key of the calling party and a proof of the calling party's ownership to that public
key. In X.509, the standard form of such a proof is a chain of public key certificates, called the certificate path between two parties. Certificate exchange protocol
(CEP), proposed by ATM Forum, requires that another bi-directional connection
be established between two parties to exchange public keys and certificate paths
before an authenticated connection can be set up, which is not an ideal approach.
We propose an algorithm which is embedded into ATM routing protocols to generate a certificate path inside a signaling message on-the-fly as the signaling message
travels through the ATM network. In this approach, all that a calling party needs
to know for authentication purpose is its own public key certificate and the ATM
network builds the rest of the certificate path for it. Related issues like distribution
of public key certificates and optimization of CA hierarchy are also addressed in
this paper.
Keywords: ATM, P-NNI, authenticated signaling, certificate path.
This work was partially supported by NSA Grant MDA904-96-1-0111.
+PAGE+

A Neural Network Pole Balancer
that Learns and Operates on a Real Robot in Real Time
Dean Hougen John Fischer Deva Johnam
hougen@cs.umn.edu jfischer@cs.umn.edu johnam@cs.umn.edu
Artificial Intelligence, Robotics, and Vision Laboratory
Department of Computer and Information Sciences
University of Minnesota
4-192 Electrical Engineering and Computer Science Building
200 Union St. SE, Minneapolis, MN 55455
Abstract
A neural network approach to the classic
inverted pendulum task is presented. This task is the
task of keeping a rigid pole, hinged to a cart and free
to fall in a plane, in a roughly vertical orientation by
moving the cart horizontally in the plane while keeping the cart within some maximum distance of its
starting position. This task constitutes a difficult control problem if the parameters of the cart-pole system
are not known precisely or are variable. It also forms
the basis of an even more complex control-learning
problem if the controller must learn the proper actions
for successfully balancing the pole given only the current state of the system and a failure signal when the
pole angle from the vertical becomes too great or the
cart exceeds one of the boundaries placed on its position.
The approach presented is demonstrated to
be effective for the real-time control of a small, self-contained mini-robot, specially outfitted for the task.
Origins and details of the learning scheme, specifics
of the mini-robot hardware, and results of actual
learning trials are presented.
1 Introduction

A Neural Network for Attentional Spotlight
Wee Kheng Leow and Risto Miikkulainen
Technical Report AI91-165
Department of Computer Sciences,
University of Texas at Austin,   Austin, Texas 78712
leow@cs.utexas.edu, risto@cs.utexas.edu
Abstract
According to space-based theory, visual attention is limited to a local region in space
called the attentional field. Visual information within the attentional field is enhanced
for further processing while information outside is suppressed. There is evidence that
enhancement and suppression are achieved with dynamic weighting of network activity.
This paper discusses a neural network that generates the appropriate weights, called the
attentional spotlight, given the size and the position of the intended attentional field.
The network has three layers. A shunting feedback network serves as the output layer
and performs a critical task which cannot be accomplished by feedforward networks.
1 Introduction

THE USE OF PARTIAL QUANTITATIVE
INFORMATION WITH QUALITATIVE
REASONING
APPROVED BY
SUPERVISORY COMMITTEE:
+PAGE+

Integrating Topological and Metric Maps for Mobile Robot Navigation:
A Statistical Approach
Sebastian Thrun 1 , Steffen Gutmann 2 , Dieter Fox 3 , Wolfram Burgard 3 , and Benjamin J. Kuipers 4
1  Computer Science Department  2 Institut fur Informatik 3 Institut fur Informatik III 4 Computer Science Department
Carnegie Mellon University Universitat Freiburg University of Bonn University of Texas at Austin
Pittsburgh, PA 15213 D-79110 Freiburg, Germany D-53117 Bonn, Germany Austin, TX 78712
submitted to AAAI-98
Abstract
The problem of concurrent mapping and localization has received considerable attention in the mobile robotics community. With few exceptions, existing approaches can largely be
grouped into two distinct paradigms: topological and metric.
This paper proposes a method that integrates both paradigms.
It poses the mapping problem as a statistical maximum likelihood problem, and devises an efficient algorithm for search
in likelihood space. Based on that, it presents an novel mapping algorithm that integrates two phases: a topological and a
metric mapping phase. The topological mapping phase solves a
global position alignment problem between potentially indistinguishable, significant places. The subsequent metric mapping
phase produces a fine-grained metric map of the environment
in floating-point resolution. Experimental results in cyclic environments of sizes up to 80 by 25 meters demonstrate the
appropriateness of this approach.
Introduction

Scheduling Issues in the Co-Synthesis of
Reactive Real-Time Systems 1
Pai Chou, Elizabeth Walkup, Gaetano Borriello 2
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
Technical Report 94-09-04
April 20, 1994
1 An edited version of ths report appears in IEEE Micro, August 1994.
2 This work was supported by an NSF Graduate Fellowship (Walkup), a PYI Award (MIP-8858782),
and by the ARPA/CSTO Microsystems Program under an ONR monitored contract (N00014-91-J-4041). The authors' email addresses are fchou,walkup,gaetanog@cs.washington.edu.
+PAGE+

Massively Parallel Computation for Three-Dimensional
Monte Carlo Semiconductor Device Simulation
Henry Sheng, Roberto Guerrieri
and Alberto Sangiovanni-Vincentelli
Department of Electrical Engineering and Computer Sciences
University of California,   Berkeley, CA 94720, U.S.A.
Dipartimento di Elettronica e Informatica
Universita di Bologna, Italy
Abstract
This work presents a study of the applicability of a massively parallel computing paradigm
to Monte Carlo techniques for device simulation. A unique mapping of Monte Carlo to SIMD
fine-grained parallelism has been developed, decoupling the problem into separate computational domains. For MOSFET simulation, this novel mapping allows estimated speeds of
over 200,000 scatterings processed per second on a 65,536 processor Connection Machine,
nearly a factor of six over the fastest known to date.
1 Introduction

Extraction of Keyphrases from Text:
Evaluation of Four Algorithms
P. Turney
October 23, 1997
National Research
Council Canada
Institute for
Information Technology
Conseil national
de recherches Canada
Institut de technologie
de linformation
ERB-1051
Copyright 1997 by
National Research Council of Canada
Permission is granted to quote short excerpts and to reproduce figures and tables from this report,
provided that the source of such material is fully acknowledged.
+PAGE+

On Hamiltonian Triangulations in Simple Polygons
Giri NARASIMHAN 1
e-mail: giri@next1.msci.memphis.edu
Dept. of Mathematical Sciences
The University of Memphis
Memphis TN 38152
Abstract
A simple polygon P is said to have a Hamilitonian Triangulation if it has a triangulation
whose dual graph contains a Hamiltonian path. Such triangulations are useful in fast
rendering engines in Computer Graphics. Arkin et al. [AHMS] observed that a polygon
has a Hamiltonian triangulation if and only if it is Discretely Straight Walkable, a concept
that is a discrete version of Straight Walkability concept as introduced by Icking and Klein
[IK]. Using this characterization, Arkin et al. also showed an algorithm to recognize such
polygons in time that is linear in the size of the visibility polygon of the given polygon P .
The size of the visbility polygon of P could be quadratic in the size of P and hence their
algorithm could be very inefficient even for nearly convex polygons.
We give a new characterization of polygons with Hamiltonian triangulations. We use
this characterization to present the following algorithms:
* An O(n log n)-time algorithm to recognize polygons with a Hamiltonian triangulation.
* An O(n log n)-time algorithm to construct such a triangulation.
* Given vertices p and q on a simple polygon, an O(n)-time algorithm to determine
whether the polygon is discretely straight walkable with respect to the two vertices.
* An O(n log n)-time algorithm to list out all pairs of points on a simple polygon with
respect to which the polygon is discretely straight walkable.
References
[IK] C. Icking and R. Klein, "The two guards problem," Proc. 7th Annual ACM Symp. on
Computational Geometry, 1991, pp. 166-175.
[AHMS] E.M. Arkin, M. Held, J.S.B. Mitchell, S.S. Skienna, "Hamiltonian Triangulations
for Fast Rendering," Proc. of the 2nd ESA, 1994.
1 Supported in part by NSF Grant CCR-940-9752

The SAMOS Active DBMS Prototype
Stella Gatziu, Andreas Geppert, Klaus R. Dittrich
Institut fur Informatik,   Universitat uZrich 1
Technical Report 94.16
Abstract
We describe SAMOS, an active object-oriented database management system prototype. SAMOS offers a powerful rule definition language, including a small yet powerful set of event definition facilities. It is able to detect primitive and composite events
automatically and efficiently. Upon event detection, SAMOS executes rules attached
to the occurred events.
1 Introduction

On the Robustness of the Damped V -cycle
of the Wavelet Frequency Decomposition
Multigrid Method
Andreas Rieder 1fl
Xiaodong Zhou 1
December 15, 1993
Abstract
The V -cycle of the wavelet variation of the "Frequency decomposition multigrid method" of Hackbusch [Numer. Math., 56, pp.
229-245, 1989] is considered.
It is shown that its convergence speed is not affected by the presence of anisotropy provided that the corresponding coarse grid correction is damped sufficiently strong. Our analysis is based on properties
of wavelet packets which are supplied and proved.
Numerical approximations to the speed of convergence illustrate
the theoretical results.
Key words: wavelets, wavelet packets, robust multilevel methods, V -cycle
Subject classification: AMS(MOS) 65F10, 65N30
1 supported partially by AFOSR under grant number 90-0334 which was funded by
DARPA
partially supported by a Feodor Lynen-Fellowship of the Alexander von Humboldt
Foundation
+PAGE+

Adaptive Wave Propagation Modeling
Raymond O. Wells, Jr.
Department of Mathematics,   Rice University
Houston, TX 77251-1892
ABSTRACT
This paper discusses several current attempts to use acoustic and electromagnetic wave propagation for modeling
physical phenomena and the role that wavelet analysis is playing in these efforts. The first problem involves recent
application of wavelets to computational fluid dynamics. The second problem involves geophysical modeling of the
ocean floor, using acoustic waves, and wavelets have recently been shown to play an important role here already.
The third problem involves modeling of SAR radar images in the context of automatic target recognition efforts.
The fourth problem is global illumination in computer graphics, i.e., simulation of reflected and absorbed light
for everyday environments. The role of wavelets is more embryonic in these latter two areas, but there are some
common principles in all of these modeling efforts, and the methodology of wavelets seems well suited to certain
aspects of these problems.
1 INTRODUCTION AND OVERVIEW

Neuronal Goals: Efficient Coding and Coincidence Detection
Nathan Intrator
School of Mathematical Sciences
Tel Aviv University
nin@cns.brown.edu
Abstract| Barlow's seminal work on minimal entropy codes and unsupervised learning is
reiterated. In particular, the need to transmit the probability of events is put in a practical
neuronal framework for detecting suspicious events. A variant of the BCM learning rule [15]
is presented together with some mathematical results suggesting optimal minimal entropy
coding.
Key words: Sparse coding, Non-Gaussian distributions, BCM Theory, Minimal Entropy
1 Introduction

Computer Vulnerability Analysis
Thesis Proposal
Ivan Krsul
The COAST Laboratory
Department of Computer Sciences
Purdue University
West Lafayette, IN 47907-1398
krsul@cs.purdue.edu
Technical Report CSD-TR-97-026
April 15, 1997
Abstract
Computer security professionals and researchers do not have a history of sharing and analyzing computer
vulnerability information. Scientists and engineers from older or more established fields have long understood
that publicizing, analyzing, and learning from other people's mistakes is essential to the stepwise refinement
of complex systems. Computer scientists, however, have not followed suit. Programmers reinvent classical
programming mistakes, contributing to the reappearance of known vulnerabilities.
In the recent past, computer systems have come to be a part of critical systems that have a direct effect
on the safety and well-being of human beings and hence we must have lower tolerance for software failures.
In the dissertation I will attempt to show that computer vulnerability information presents important
regularities and these can be detected, and possibly visualized, providing important insight about the reason
of their prevalence and existence. The information derived from these observations could be used to improve on
all phases of the development of software systems, as could be in the design, development, debugging, testing
and maintenance of complex computer systems that must implement a set of policies defined by security
analysis.
A significant portion of the work that must be performed will concentrate on the development of classifications and taxonomies that will permit the visualization and analysis of computer vulnerability information.
I hope that these classifications and taxonomies applied to a collection of vulnerabilities will provide a set
of features whose analysis will show that there are clear statistical clusterings and patterns caused because
developers and programmers are not learning from each others mistakes. This analysis may be performed by
applying statistical analysis and knowledge discovery tools.
1 Introduction

Evolving Visually Guided Robots
D. Cliff, P. Husbands, I. Harvey
CSRP 220,   July 1992
Cognitive Science Research Paper
Serial No. CSRP 220
The University of Sussex
School of Cognitive and Computing Sciences
Falmer
Brighton BN1 9QH
England, U.K.
A version of this paper appears in:
Proceedings of SAB92,
the Second International Conference on Simulation of Adaptive Behaviour
J.-A. Meyer, H. Roitblat, and S. Wilson, editors,
MIT Press Bradford Books, Cambridge, MA, 1993.
+PAGE+

Pseudo-Network Drivers and Virtual Networks
S.M. Bellovin*
smb@ulysses.att.com
AT&T Bell Laboratories
Murray Hill, New Jersey 07974
ABSTRACT
Many operating systems have long had pseudo-teletypes, inter-process
communication channels that provide terminal semantics on one end,
and a smart server program on the other. We describe an analogous
concept, pseudo-network drivers. One end of the driver appears to be
a real network device, with the appropriate interface and semantics;
data written to it goes to a program, however, rather than to a physical
medium. Using this and some auxiliary mechanisms, we present a
variety of applications, including system test, network monitoring,
dial-up TCP/IP, and ways to both improve and subvert network
security. Most notably, we show how pseudo-network devices can be
used to create virtual networks and to provide encrypted
communications capability. We describe two implementations, one
using a conventional driver for socket-based systems, and one using
stream pipes for System V.
1. INTRODUCTION

Copyright 1993 ACM 0-8186-4340-4/93/0011 Permission to copy for non-commercial use granted by the Association for Computing Machinery.
MPI: A Message Passing Interface
The MPI Forum
This paper presents an overview of mpi, a proposed
standard message passing interface for MIMD distributed memory concurrent computers. The design
of mpi has been a collective effort involving researchers
in the United States and Europe from many organizations and institutions. mpi includes point-to-point
and collective communication routines, as well as support for process groups, communication contexts, and
application topologies. While making use of new ideas
where appropriate, the mpi standard is based largely
on current practice.
1 Introduction

Type-driven Defunctionalization
Jeffrey M. Bell & Fran~coise Bellegarde & James Hook
Pacific Software Research Center
Oregon Graduate Institute of Science & Technology
PO Box 91000
Portland, Oregon 97291-1000
USA
fbell,bellegar,hookg@cse.ogi.edu
Abstract
In 1972, Reynolds outlined a general method for eliminating functional arguments known as defunctionalization. The
idea underlying defunctionalization is encoding functional
values as first-order data, and then to realized the applications of the encoded function via an apply function. Although this process is simple enough, problems arise when
defunctionalization is used in a polymorphic language. In
such a language, a functional argument of a higher-order
function can take different type instances in different applications. As a consequence, its associated apply function can
be untypable in the soucre language. In the paper we present
a defunctionalization transformation which preserves typa-bility. Moreover, the transformation imposes no restriction
on functional arguments of recursive functions, and it handles functions as results as well as functions encapsulated in
constructors or tuples. The key to this success is the use
of type information in the defunctionalization transformation. Run-time characteristics are preserved by defunction-alization; hence, there is no performance improvement coming from the transformation itself. However closures need
not be implemented to compile the transformed program.
Since the defunctionalization is driven by type information,
it can also easily perform a specialization of higher-order
functions with respect to the values of their functional arguments, hence gaining a real run-time improvement of the
transformed program.
1 Introduction

Reactive Functional Programming
Richard B. Kieburtz
Oregon Graduate Institute of Science & Technology
P.O. Box 91000, Portland, OR 97291-1000 USA
October 13, 1997
Abstract
Reactive systems respond to concurrent, possibly unsynchronized streams of input events. Programming
reactive systems is challenging without language support for event-triggered actions. It is even more
challenging to reason about reactive systems. This paper explores a new conceptual basis for applying
functional programming techniques to the design and formal verification of reactive systems. The
mathematical foundation for this approach is based upon signature coalgebras and derived proof rules
for coinduction. The concepts are illustrated with an example that has been used with the language
Esterel.
1 Introduction

Location Independent Names for Nomadic Computers
David C. Steere, Mark Morrissey, Peter Geib, Calton Pu, and Jonathan Walpole
Department of Computer Science and Engineering
Oregon Graduate Institute
Abstract
Recent advances in the Domain Name System (DNS) and the Dynamic Host Configuration
Protocol (DHCP) have enabled a new approach to supporting mobile users: location independent
naming. In this approach, machines use the same hostname from any internet location, but use an
IP address that corresponds to their current location. We describe a protocol that implements
location independent naming for nomadic computers, i.e., machines that do not need transparent
mobility. Our protocol allows hosts to move across security domains, uses existing protocols, and
preserves existing trust relationships. Therefore, it preserves the performance and security of
normal IP for nomadic computers at the expense of not providing the transparent mobility of
Mobile IP. We contend that this is a reasonable tradeoff for nomadic computing.
1 Introduction

Comparison of Statistical and Neural Classifiers
and Their Applications to
Optical Character Recognition and Speech Classification
Ethem Alpaydn, Fikret Gurgen
Department of Computer Engineering
Bogazi~ci University
TR-80815   _ Istanbul Turkey
falpaydin,gurgeng@boun.edu.tr
Neural Network Systems Techniques and Applications (in print)
C. T. Leondes (Ed.), c flACADEMIC Press
October 24, 1996
Abstract
We give a review of basic statistical and neural techniques for classification. Statistical techniques are based on the idea of estimating class-conditional likelihoods and using Bayes rule
to convert these to posterior class probabilities whereas neural techniques estimate directly the
posteriors. Statistical techniques include (i) Parametric (Gaussian) Bayes classifiers, (ii) Non-parametric kernel-based density estimators like k-nearest neighbor and Parzen windows, and
(iii) mixtures of (Gaussian) densities (a special case of which is the Learning Vector Quantization). As neural classifiers, we include simple perceptrons and multilayer perceptrons with
sigmoid and Gaussian hidden units. The neural and statistical techniques are quite similar in
many respects and many approaches have been discovered independently twice, once in 1960s
by statisticians and once in 1980s by the neural network researchers. One of the aims of this article is to make this link more apparent. We also discuss two, most popular, pattern recognition
applications: Optical character recognition and speech recognition. Though they seem different,
in many respects, the two applications are similar and in the past, almost the same techniques
have been applied for their implementation. We implement the well known statistical and neural
classification techniques for two datasets of these applications and compare them in terms of
generalization accuracy, memory requirement and learning time. We especially advise to take
into account statistics of the sample even if a neural classifier is to be used. The similarity
between statistical and neural techniques is greater than generally agreed and simple statistical
methods like k-NN perform generally quite well and much of the functionality of neural networks like distributed parallel computation can be obtained by such methods without requiring
complicated computation and precise error minimization procedures.
Keywords| Statistical pattern recognition, artificial neural networks, optical character recog
nition, speech recognition, Bayes decision theory, nonparametric estimation.
+PAGE+

DIMACS Technical Report 96-47
October 1996
On independent domination number of graphs
with given minimum degree
by
N. I. Glebov 1
Institute of Mathematics
630090 Novosibirsk, Russia
A. V. Kostochka 2
Institute of Mathematics
630090 Novosibirsk, Russia
DIMACS is a partnership of Rutgers University, Princeton University, AT&T
Research, Bellcore, and Bell Laboratories.
DIMACS is an NSF Science and Technology Center, funded under contract
STC-91-19999; and also receives support from the New Jersey Commission
on Science and Technology.
+PAGE+

DIMACS Technical Report 97-15
April 1997
(Revised August 1997)
Crowds: Anonymity for Web Transactions
by
Michael K. Reiter 1 Aviel D. Rubin 2
AT&T Labs|Research,    Murray Hill, New Jersey, USA
freiter,rubing@research.att.com
1 Permanent Member
2 Permanent Member
DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs, Bellcore, and Bell Labs.
DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives
support from the New Jersey Commission on Science and Technology.
+PAGE+

DIMACS Technical Report 98-34
July 1998
On the dimension of the Hilbert-cubes
by
Norbert Hegyvari 1
1 Research partially supported by Hungarian National Foundation for Scientific Research, Grant No.
T025617 and by DIMACS (Center for Discrete Mathematics and Theoretical Computer Science)
NSF-STC-91-19999.
DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs-Research,
Bell Labs, Bellcore and NEC Research Institute.
DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999;
and also receives support from the New Jersey Commission on Science and Technology.
+PAGE+

Kit: A Study in
Operating System Verification
William R. Bevier
Technical Report 28   August, 1988
Computational Logic Inc.
1717 W. 6th St. Suite 290
Austin, Texas 78703
(512) 322-9951
This research was supported in part by the U.S.
Government. The views and conclusions contained in this
document are those of the author and should not be
interpreted as representing the official policies, either
expressed or implied, of the Defense Advanced Research
Projects Agency or the U.S. Government. This work was
sponsored in part at Computational Logic, Inc. by the
Defense Advanced Research Projects Agency, ARPA
Orders 6082 and 9151, and at the University of Texas at
Austin by the Defense Advanced Research Projects
Agency, ARPA Order 5246, issued by the Space and
Naval Warfare Systems Command under Contract
N00039-85-K-0085.
+PAGE+

Towards an Effective Calculus for Object Query Languages
Leonidas Fegaras David Maier
Department of Computer Science and Engineering
Oregon Graduate Institute of Science & Technology
20000 N.W. Walker Road P.O. Box 91000
Portland, OR 97291-1000
email: ffegaras,maierg@cse.ogi.edu
Abstract
We define a standard of effectiveness for a database calculus
relative to a query language. Effectiveness judges suitability
to serve as a processing framework for the query language,
and comprises aspects of coverage, manipulability and
efficient evaluation. We present the monoid calculus, and
argue its effectiveness for object-oriented query languages,
exemplified by OQL of ODMG-93. The monoid calculus
readily captures such features as multiple collection types,
aggregations, arbitrary composition of type constructors and
nested query expressions. We also show how to extend
the monoid calculus to deal with vectors and arrays in
more expressive ways than current query languages do, and
illustrate how it can handle identity and updates.
1 Introduction

LISP AND SYMBOLIC COMPUTATION: An International Journal, 5, 191-221, 1992
c 1992 Kluwer Academic Publishers Manufactured in The Netherlands
Callee-save Registers in Continuation-passing Style
ANDREW W. APPEL   (appel@princeton.edu)
ZHONG SHAO   (zsh@princeton.edu)
Department of Computer Science,   Princeton University,    Princeton, NJ 08544-2087
Keywords: Register Allocation, Continuation-passing Style, Procedure Call
Abstract. Continuation-passing style (CPS) is a good abstract representation to use
for compilation and optimization: it has a clean semantics and is easily manipulated.
We examine how CPS expresses the saving and restoring of registers in source-language
procedure calls. In most CPS-based compilers, the context of the calling procedure is
saved in a "continuation closure"|a single variable that is passed as an argument to the
function being called. This closure is a record containing bindings of all the free variables
of the continuation; that is, registers that hold values needed by the caller "after the call"
are written to memory in the closure, and fetched back after the call.
Consider the procedure-call mechanisms used by conventional compilers. In particular,
registers holding values needed after the call must be saved and later restored. The
responsibility for saving registers can lie with the caller (a "caller-saves" convention)
or with the called function ("callee-saves"). In practice, to optimize memory traffic,
compilers find it useful to have some caller-saves registers and some callee-saves.
"Conventional" CPS-based compilers that pass a pointer to a record containing all
the variables needed after the call (i.e., the continuation closure), are using a caller-saves
convention. We explain how to express callee-save registers in Continuation-Passing
Style, and give measurements showing the resulting improvement in execution time.
1. Introduction

Coordinated Resource Management
in a Replicated Object Server
Sanjay Ghemawat
Robert Gruber
James O'Toole
Liuba Shrira
Abstract
We propose several new techniques for resource management in a replicated object server. By coordinating cache
and disk usage among the replicas, these techniques increase throughput and reduce fetch latency. Cache splitting
speeds up fetches by avoiding redundant cache entries, effectively increasing the cache size. Coordinated writing
coordinates disk writes to ensure that one replica is always
available to service fetches. We investigate the performance
of a replicated server using these techniques, and we present
simulation results showing that these techniques provide
substantial performance improvements across a variety of
workloads.
1 Introduction

Discovering Compressive Partial Determinations
in Mixed Numerical and Symbolic Domains
Bernhard Pfahringer and Stefan Kramer
Austrian Research Institute for Artificial Intelligence
Schottengasse 3
A-1010 Vienna, Austria
fbernhard, stefang@ai.univie.ac.at
Abstract
Partial determinations are an interesting
form of dependency between attributes in a
relation. They generalize functional dependencies by allowing exceptions. We modify a known MDL formula for evaluating
such partial determinations to allow for its
use in an admissible heuristic in exhaustive
search. Furthermore we describe an efficient
preprocessing-based approach for handling
numerical attributes. An empirical investigation tries to evaluate the viability of the
presented ideas.
1 Introduction

Vertex heaviest paths and cycles in quasi-transitive
digraphs
Jtrgen Bang-Jensen
Gregory Gutin
Department of Mathematics and Computer Science
Odense University,   Denmark
Abstract
A digraph D is called a quasi-transitive digraph (QTD) if for any
triple x; ; of distinct vertices of D such that (x; ) and (; ) are
arcs of D there is at least one arc from x to or from to x. Solving
a conjecture by J. Bang-Jensen and J. Huang (J. Graph Theory, to
appear), G. Gutin (Australas. J. Combin., to appear) described polynomial algorithms for finding a Hamiltonian cycle and a Hamiltonian
path (if it exists) in a QTD. The approach taken in that paper cannot
be used to find a longest path or cycle in polynomial time. We present
a principally new approach that leads to polynomial algorithms for
finding vertex heaviest paths and cycles in QTD's with non-negative
weights on the vertices. This, in particular, provides an answer to a
question by N. Alon on longest paths and cycles in QTD's.
1 Introduction

A Knowledge-Sharing Strategy
David Goldstein and Albert Esterline
North Carolina A&T State University
Greensboro, North Carolina
goldstn @ncat.edu, esterlin@ncat.edu
Introduction

PORTS: Experiences with a Scheduler for Dynamic Real-Time
Systems
(Extended Abstract)
Kaushik Ghosh, Richard M. Fujimoto, and Karsten Schwan
College of Computing
Georgia Institute of Technology
Atlanta, GA, 30332.
June 24, 1994
Abstract
This paper describes several of our experiences with a real-time scheduler. Using a robot control application program, we motivate the importance of supporting multiple schedulers within the same application
program. We demonstrate the utility of speculative task execution in dynamic real-time systems, and describe the implementation of a scheduler for performing speculative execution and recovery. We show that
existing real-time scheduler interfaces have scope for improvement, especially when scheduling latency must
be low and when multiple schedulers used by a single application must co-exist on a single processor. A new
scheduler interface is specified and its basic costs are evaluated experimentally. Preliminary measurements on
a KSR-1 machine are quoted. The measurements demonstrate how the execution times of temporal queries
may be reduced by use of access structures to scheduler data structures. Finally, there are several overheads
associated with speculative execution, and multiple schedulers in a single application. We consider the problem of on-line reconfiguration of the several overheads associated with the speculative-execution paradigm
for optimal performance in the face of these overheads. Initial performance measurements of the PORTS
scheduler indicate that it is possible to perform real-time scheduling with latencies approximating those of
proposed specialized scheduling co-processors.
1 Introduction

Core Selection Methods for
Multicast Routing
Kenneth L. Calvert Ellen W. Zegura
Michael J. Donahoo
GIT-CC-95/15
Abstract
Multicast routing is an important topic of both theoretical and practical
interest. Some recently-proposed multicast routing algorithms involve the
designation of one or more network nodes as the "center" of the routing
tree for each multicast group address. The choice of this designated router
(which we refer to as the "core") influences the shape of the multicast routing
tree, and thus influences performance of the routing scheme. In this paper we
investigate the relationship between the choice of core and three performance
measures. Specifically, we compare various methods of selecting a core with
respect to their effect on bandwidth, delay, and traffic concentration. We
conclude that simple methods are adequate for widely distributed groups,
but that the addition of group information can be leveraged to improve
performance especially when the group is small or exhibits a high degree
of locality. We also conclude that core choice has a significant impact on
traffic concentration, in fact traffic concentration effects can be ameliorated
by appropriate core choice policies.
Keywords: Multicast routing, Scalability, Network modeling
College of Computing
Georgia Institute of Technology
Atlanta, Georgia 30332-0280
+PAGE+

Interactive Model-Based Image Understanding
Daryl T. Lawton Warren F. Gardner
Abstract
This paper describes a general architecture for an interactive model-based vision system. A
human specifies a limited amount of information which establishes a context for autonomous interpretation of images. Object models are described by constraints specifying necessary geometrical
properties and relationships between objects. The use of constraints allows for flexible object in-stantiation. A user can indicate an object in a scene and this directs perceptual processing routines
as well as constraining future object instantiations. This interactive model-based concept has been
applied to the domain of vehicle tracking, and this paper concludes with several processing examples
from this domain.
1 Introduction

Implementing Schema-theoretic Models of
Animal Behavior in Robotic Systems
Khaled S. Ali and Ronald C. Arkin
Mobile Robot Laboratory
College of Computing
Georgia Institute of Technology
Atlanta, GA, 30332-0281 USA
fkali,arking@cc.gatech.edu
Abstract
Formal models of animal sensorimotor behavior can
provide effective methods for generating robotic intelligence. In this paper we describe how schema-theoretic
models of the praying mantis are implemented on a
hexapod robot equipped with a real-time color vision
system. The model upon which the implementation
is based was developed by ethologists studying man-tids. This implementation incorporates a wide range
of behaviors, including obstacle avoidance, prey acquisition, predator avoidance, mating, and chantlitaxia
behaviors.
1 Introduction

Finding the Parts of Objects
in Range Images
Andre Lejeune and Frank P. Ferrie
CIM-93-8   August 1993
Center for Intelligent Machines
McGill University,    McConnell Engineering Building
3480 Universite, Montreal, Quebec, CANADA, H3A 2A7
Email: andre@lightning.mcrcim.mcgill.edu
ferrie@lightning.mcrcim.mcgill.edu
Tel: (514) 398-6042 Fax: (514) 398-7348
+PAGE+

Hypertextual Concurrent Control
of a Lisp Kernel
P. David Stotts Richard Furuta
Department of Computer and Department of Computer Science and
Information Sciences Institute for Advanced Computer Studies
University of Florida University of Maryland
Gainesville, FL 32611 College Park, MD 20742
Abstract
Using the Trellis human/computer interaction model as an implementation vehicle, we demonstrate
how to use concurrency-supporting hypertext to provide visual displays of the execution flows through
a parallel Lisp program. In addition to displays, the hypertext interface allows injection of control
flow into an otherwise functional computation, and therefore provides reader control over the order of
evaluation of expressions. The resulting system, termed Trellis, can be thought of as a concurrent control
flow browser for composing functional computations, providing a visual implementation of kernel-control
decomposition. The advantages of Trellis are ease of exploring program side effects; ease of debugging
parallel code; aid in teaching functional languages; and the ability to construct hypertext documents
that have parallel execution semantics and flexible browsing behaviors.
Key words: functional programming, parallelism, kernel-control decomposition, Lisp, hypertext, exe
cution visualization.
1 Introduction

A SIMPLE APPROACH TO DISTRIBUTED POOLS
BENEDICT M. RAFANELLO & THEODORE JOHNSON
University of Florida,   Department of Computer and information Sciences
January 1994
Computer networks hold the potential to coordinate the activities of multiple machines so that their combined computational abilities can be applied to solving a single problem. Several methods have been developed over the years to
harness the power of networked systems for solving certain classes of problems. One such class of problems is the distributed producer/consumer problem, in which a set of producer processes supply items to a set of consumer processes. Each
of the processes involved resides on a different machine, with the machines being connected by a network and the processes communicating via message passing. The problem, then, is how to coordinate the activities of the producers and consumers so that an acceptable level of throughput can be maintained with a minimal amount of overhead. This paper
presents a simple solution to the distributed producer/consumer problem. This solution, which is based upon the notion of
a distributed pool, is described in detail and its performance analyzed. As the analysis shows, the distributed pools algorithm presented herein is a simple, efficient solution to the distributed producer/consumer problem, and is capable of better than 90% efficiency under common conditions. Its major failing is that it needs the production rates of the producers
to be reasonably similar.
Key words: distributed computing, distributed queue, performance modeling, producer/consumer, simulation.
INTRODUCTION

Selection Predicate Indexing for Active Databases
Using Interval Skip Lists
Eric N. Hanson
Theodore Johnson
Computer and Information Sciences Department
University of Florida
Gainesville, FL 32611
fhanson,tedg@cis.ufl.edu
TR94-017
15 April 1994
(revised 13 October 1994)
Abstract
A new, efficient selection predicate indexing scheme for active database systems is introduced.
The selection predicate index proposed uses an interval index on an attribute of a relation or
object collection when one or more rule condition clauses are defined on that attribute. The
selection predicate index uses a new type of interval index called the interval skip list (IS-list).
The IS-list is designed to allow efficient retrieval of all intervals that overlap a point, while allowing
dynamic insertion and deletion of intervals. IS-list algorithms are described in detail. The IS-list
allows efficient on-line searches, insertions, and deletions, yet is much simpler to implement than
other comparable interval index data structures such as the priority search tree and balanced
interval binary search tree (IBS-tree). IS-lists require only one third as much code to implement
as balanced IBS-trees. The combination of simplicity, performance, and dynamic updateability
of the IS-list is unmatched by any other interval index data structure. This makes the IS-list a
good interval index structure for implementation in an active database predicate index.
1 Introduction

Shape Modeling with Front Propagation: A Level Set Approach
Ravikanth Malladi, 1 James A. Sethian, 1 and Baba C. Vemuri 2
1 Lawrence Berkeley Laboratory
and
Department of Mathematics
University of California,   Berkeley, CA 94720.
2 Department of Computer & Information Sciences
University of Florida,   Gainesville, FL 32611.
Abstract
Shape modeling is an important constituent of computer vision as well as computer graphics
research. Shape models aid the tasks of object representation and recognition. This paper
presents a new approach to shape modeling which retains some of the attractive features of
existing methods, and overcomes some of their limitations. Our techniques can be applied to
model arbitrarily complex shapes, which include shapes with significant protrusions, and to
situations where no a priori assumption about the object's topology is made. A single instance
of our model, when presented with an image having more than one object of interest, has the
ability to split freely to represent each object. This method is based on the ideas developed
by Osher and Sethian to model propagating solid/liquid interfaces with curvature-dependent
speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient
field with constant speed or a speed that depends on the curvature. It is moved by solving a
"Hamilton-Jacobi" type equation written for a function in which the interface is a particular
level set. A speed term synthesized from the image is used to stop the interface in the vicinity of
object boundaries. The resulting equation of motion is solved by employing entropy-satisfying
upwind finite difference schemes. We present a variety of ways of computing evolving front,
including narrow bands, reinitializations, and different stopping criteria. The efficacy of the
scheme is demonstrated with numerical experiments on some synthesized images and some low
contrast medical images.
fl1 Supported in part by the Applied Mathematical Sciences Subprogram of the Office of Energy Research, U.S.
Dept. of Energy under Contract DE-AC03-76SD00098 and by the NSF ARPA under grant DMS-8919074.
2 Supported in part by NSF grant ECS-9210648.
+PAGE+

Quality Management of Information Systems Development
Geoff Beckworth,
email: gbeck@deakin.edu.au
Brian Garner
email: brian@deakin.edu.au
School of Computing and Mathematics
Deakin University,   Geelong, Victoria, 3217, Australia.
Abstract
The role of the systems analyst in the implementation process has changed
dramatically in recent times because of changes to organisational boundaries, the
need to align IT with business objectives and the complexity of the systems now
required. Some organisations are finding themselves in a continually changing
environment and being involved in multi-organisational structures. Establishing
strategies and requirements for these organisations requires a new understanding
of the implementation process. Implementation is concerned with behavioural
phenomena since people are involved from the inception of the idea, as well as
being involved in the development process. They are also affected by the changes
which the new system brings to the organisation. The research is attempting to
understand the critical factors associated with the implementation process and
consequently develop an appropriate model.
1. Introduction

Times of Cryptographic Parameter Generation, and
Key Computation and Distribution for the Star-based
and Ring-based Conference Authentication Facilities 1
Damien De Paoli and Andrzej Goscinski
(ddp@deakin.edu.au, ang@deakin.edu.au)
School of Computing and Mathematics
Deakin University
Geelong, Victoria 3217
September 8, 1994
Abstract
Two-way authentication methods are inefficient when used to authenticate multiple users who wish to communicate securely with each other. M-way, also called conference authentication, is designed to efficiently authenticate many users and distribute a secure common conference key. Unfortunately, few if any conference
authentication schemes have been developed and/or performance tested in a real distributed system. This report
attempts to rectify this problem. Specifically, it presents the performance of both a star and a ring-based conference authentication scheme that has been developed for the RHODOS distributed operating system. As with most
systems, RHODOS does not have any special hardware, thus, a software based solution is utilised. This report
also attempts to shed some light upon how viable and secure a conference authentication scheme would be when
used in a distributed system.
Keywords: Conference Authentication, Distributed Systems.
1. This work was supported by the Australian Research Council under Grant A48831034, Australian

In press: The Neurobiology of Computation: Proceedings of the Annual Compu--tational Neuroscience Meeting. J.M. Bower, ed. Kluwer Academic Publishers,
Boston.
UNSUPERVISED LEARNING OF
INVARIANT REPRESENTATIONS OF FACES
THROUGH TEMPORAL ASSOCIATION
Marian Stewart Bartlett ;
Terrence J. Sejnowski ;
marni@salk.edu, terry@salk.edu
Departments of Cognitive Science and Psychology, UCSD
Howard Hughes Medical Institute
The Salk Institute,   La Jolla, CA, 92037
Abstract
The appearance of an object or a face changes continuously as the observer
moves through the environment or as a face changes expression or pose. Recognizing an object or a face despite these image changes is a challenging problem
for computer vision systems, yet we perform the task quickly and easily. This
simulation investigates the ability of an unsupervised learning mechanism to
acquire representations that are tolerant to such changes in the image. The
learning mechanism finds these representations by capturing temporal relationships between 2-D patterns. Previous models of temporal association learning
have used idealized input representations. The input to this model consists of
graylevel images of faces. A two-layer network learned face representations that
incorporated changes of pose up to 30 ffi . A second network learned representations that were independent of facial expression.
Introduction

Temporal Difference Learning of
Position Evaluation in the Game of Go
Nicol N. Schraudolph Peter Dayan Terrence J. Sejnowski
schraudo@salk.edu dayan@salk.edu terry@salk.edu
Computational Neurobiology Laboratory
The Salk Institute for Biological Studies
San Diego, CA 92186-5800
Abstract
The game of Go has a high branching factor that defeats the tree
search approach used in computer chess, and long-range spa-tiotemporal interactions that make position evaluation extremely
difficult. Development of conventional Go programs is hampered
by their knowledge-intensive nature. We demonstrate a viable
alternative by training networks to evaluate Go positions via temporal difference (TD) learning.
Our approach is based on network architectures that reflect the
spatial organization of both input and reinforcement signals on
the Go board, and training protocols that provide exposure to
competent (though unlabelled) play. These techniques yield far
better performance than undifferentiated networks trained by self-play alone. A network with less than 500 weights learned within
3,000 games of 9x9 Go a position evaluation function that enables
a primitive one-ply search to defeat a commercial Go program at
a low playing level.
1 INTRODUCTION

Displaying 3D Images: Algorithms for
Single Image Random Dot Stereograms
Harold W. Thimbleby, Stuart Inglis, and Ian H. Witten *
Abstract
This paper describes how to generate a single image which, when viewed in the
appropriate way, appears to the brain as a 3D scene. The image is a stereogram composed
of seemingly random dots. A new, simple and symmetric algorithm for generating such
images from a solid model is given, along with the design parameters and their influence
on the display. The algorithm improves on previously-described ones in several ways: it
is symmetric and hence free from directional (right-to-left or left-to-right) bias, it corrects
a slight distortion in the rendering of depth, it removes hidden parts of surfaces, and it
also eliminates a type of artifact that we call an echo.
Random dot stereograms have one remaining problem: difficulty of initial viewing. If
a computer screen rather than paper is used for output, the problem can be ameliorated by
shimmering, or time-multiplexing of pixel values. We also describe a simple
computational technique for determining what is present in a stereogram so that, if
viewing is difficult, one can ascertain what to look for.
Keywords: Single image random dot stereograms, SIRDS, autostereograms,
stereoscopic pictures, optical illusions
Department of Psychology,   University of Stirling,    Stirling, Scotland.   Phone (+44) 786-467679; fax
786-467641;   email hwt@compsci.stirling.ac.uk
Department of Computer Science,   University of Waikato,   Hamilton, New Zealand.   Phone (+64 7)
856-2889; fax 838-4155;   email singlis@waikato.ac.NZ.
Department of Computer Science,   University of Waikato,   Hamilton, New Zealand.   Phone (+64 7)
838-4246; fax 838-4155;   email ihw@waikato.ac.NZ.
* Please address all correspondence to Ian H. Witten
+PAGE+

Constructing a Configurable Group RPC Service
Matti A. Hiltunen and Richard D. Schlichting
Department of Computer Science
University of Arizona
Tucson, AZ 85721, USA
Abstract
Current Remote Procedure Call (RPC) services implement a variety of semantics, with many of the differences
related to how communication and server failures are handled. The list increases even more when considering group
RPC, a variant of RPC often used for fault-tolerance where
an invocation is sent to a group of servers rather than one.
This paper presents an approach to constructing group
RPC in which a single configurable system is used to build
different variants of the service. The approach is based on
implementing each property as a separate software module
called a micro-protocol, and then configuring the micro-protocols needed to implement the desired service together
using a software framework based on the x-kernel. The
properties of point-to-point and group RPC are identified
and classified, and the general execution model described.
An example consisting of a modular implementation of a
group RPC service is given to illustrate the approach. Dependency issues that restrict configurability are also addressed.
1 Introduction

Development of an Intelligent Monitoring and Control System for a
Heterogeneous Numerical Propulsion System Simulation
Abdollah A. Afjeh * , Patrick T. Homer , Henry Lewandowski ,
John A. Reed * , and Richard D. Schlichting
Cleveland State University , The University of Arizona , University of Toledo *
Abstract
The NASA Numerical Propulsion System Simulation
(NPSS) project is exploring the use of computer simulation
to facilitate the design of new jet engines. Several key issues
raised in this research are being examined in an NPSS-related research project: zooming, monitoring and control,
and support for heterogeneity. The design of a simulation
executive that addresses each of these issues is described.
In this work, the strategy of zooming, which allows codes
that model at different levels of fidelity to be integrated
within a single simulation, is applied to the fan component
of a turbofan propulsion system. A prototype monitoring
and control system has been designed for this simulation to
support experimentation with expert system techniques for
active control of the simulation. An interconnection system
provides a transparent means of connecting the heterogeneous systems that comprise the prototype.
1. Introduction

Domain-specific Information Browsers for Man Page, File, and Font *
Thomas A. Phelps
Department of Electrical Engineering and Computer Science
Computer Science Division
University of California,  Berkeley
Abstract
The task of information browsers is (1) to aid in navigating through a
large database to locate the item of interest and (2) to inspect or otherwise
manipulate this item once found. This document describes three experiments in constructing tools for information browsing: TkMan for UNIX
manual pages, NBT for files in a hierarchical file system, and FoSel for
bitmap fonts. Each exploits the large-scale natural organization of data
and the fine-grained structure of each datum with a graphical user interface to provide a powerful yet intuitive tool. The lessons learned in implementing the browsers point to general principles that should guide the
design of all information browsers.
1 Domain-specific Information Browsing

USENIX Summer Conference
June 11-15, 1990
Anaheim, California
Why Aren't
Operating Systems
Getting Faster As
Fast as Hardware?
John K. Ousterhout   University of California at Berkeley
ABSTRACT
This paper evaluates several hardware platforms and operating systems using a set of benchmarks
that stress kernel entry/exit, file systems, and other things related to operating systems. The
overall conclusion is that operating system performance is not improving at the same rate as the
base speed of the underlying hardware. The most obvious ways to remedy this situation are to
improve memory bandwidth and reduce operating systems' tendency to wait for disk operations to
complete.
1. Introduction

Evolution of Recursive Transition Networks for
Natural Language Recognition with Parallel
Distributed Genetic Programming
Riccardo Poli
School of Computer Science
The University of Birmingham
E-mail: R.Poli@cs.bham.ac.uk
Technical Report: CSRP-96-19
December 1996
Abstract
This paper describes the application of Parallel Distributed Genetic Programming (PDGP) to the problem of inducing programs for natural language processing.
PDGP is a new form of Genetic Programming (GP) which is suitable for the development of programs with a high degree of parallelism and an efficient and effective
reuse of partial results. Programs are represented in PDGP as graphs with nodes
representing functions and terminals, and links representing the flow of control and
results. PDGP allows the exploration of a large space of possible programs including standard tree-like programs, logic networks, neural networks, finite state
automata, Recursive Transition Networks (RTNs), etc. The paper describes the
representations, the operators and the interpreters used in PDGP, and illustrates
its behaviour on the problem of inducing RTN-based recognisers for natural lan
guage from positive and negative examples.
1 Introduction

Submitted to EuroGP-98, Paris, 16-17 April, 1998
Genetic Programming Bloat with Dynamic Fitness
W. B. Langdon and R. Poli
School of Computer Science,   University of Birmingham,   Birmingham B15 2TT, UK
fW.B.Langdon,R.Polig@cs.bham.ac.uk   http://www.cs.bham.ac.uk/~wbl, ~rmp
Tel: +44 (0) 121 414 4791, Fax: +44 (0) 121 414 4281
Technical Report: CSRP-97-29,   3 December 1997
Abstract
In artificial evolution individuals which perform as their parents are usually rewarded identically
to their parents. We note that Nature is more dynamic and there may be a penalty to pay for doing
the same thing as your parents. We report two sets of experiments where static fitness functions
are firstly augmented by a penalty for unchanged offspring and secondly the static fitness case
is replaced by randomly generated dynamic test cases. We conclude genetic programming, when
evolving artificial ant control programs, is surprisingly little effected by large penalties and program
growth is observed in all our experiments.
1 Introduction

Why Ants are Hard
W. B. Langdon and R. Poli
School of Computer Science,   The University of Birmingham,   Birmingham B15 2TT, UK
fW.B.Langdon,R.Polig@cs.bham.ac.uk   http://www.cs.bham.ac.uk/~wbl, ~rmp
Tel: +44 (0) 121 414 4791, Fax: +44 (0) 121 414 4281
Technical Report: CSRP-98-4
January 1998
Abstract
The problem of programming an artificial ant to follow the Santa Fe trail is used as an example
program search space. Analysis of shorter solutions shows they have many of the characteristics
often ascribed to manually coded programs. Enumeration of a small fraction of the total search
space and random sampling characterise it as rugged with many multiple plateaus split by deep
valleys and many local and global optima. This suggests it is difficult for hill climbing algorithms.
Analysis of the program search space in terms of fixed length schema suggests it is highly deceptive
and that for the simplest solutions large building blocks must be assembled before they have above
average fitness. In some cases we show solutions cannot be assembled using a fixed representation
from small building blocks of above average fitness. These suggest the Ant problem is difficult for
Genetic Algorithms.
Random sampling of the program search space suggests on average the density of global optima
changes only slowly with program size but the density of neutral networks linking points of the same
fitness grows approximately linearly with program length. This is part of the cause of bloat.
Previously reported genetic programming, simulated annealing and hill climbing performance is
shown not to be much better than random search on the Ant problem.
1 Introduction

Technical Report CSRP-98-13
School of Computer Science,   The University of Birmingham
GP-Music: An Interactive Genetic Programming System for
Music Generation with Automated Fitness Raters
Brad Johanson
Stanford University
Rains Apt. 9A
704 Campus Dr.
Stanford, CA. 94305
bjohanso@stanford.edu
650-497-7543
Riccardo Poli
University of Birmingham
School of Computer Science
The University of Birmingham
Birmingham B15 2TT
R.Poli@cs.bham.ac.uk
+44-121-414-3739
Abstract
In this paper we present the GP-Music System, an interactive system which allows users to evolve short musical sequences
using interactive genetic programming, and its extensions aimed at making the system fully automated. The basic GP-system works by using a genetic programming algorithm, a small set of functions for creating musical sequences, and a user
interface which allows the user to rate individual sequences. With this user interactive technique it was possible to generate
pleasant tunes over runs of 20 individuals over 10 generations. As the user is the bottleneck in interactive systems, the
system takes rating data from a users run and uses it to train a neural network based automatic rater, or auto rater, which
can replace the user in bigger runs. Using this auto rater we were able to make runs of up to 50 generations with 500
individuals per generation. The best of run pieces generated by the auto raters were pleasant but were not, in general, as
nice as those generated in user interactive runs.
1 Introduction

Low-Energy Asynchronous Memory Design
Jose A. Tierno Alain J. Martin
California Institute of Technology
Pasadena, CA 91125
Abstract
We introduce the concept of energy per operation as
a measure of performance of an asynchronous circuit.
We show how to model energy consumption based on
the high-level language specification. This model is independent of voltage and timing considerations. We
apply this model to memory design. We show first
how to dimension a memory array, and how to break
up this memory array into smaller arrays to minimize
the energy per access. We then show how to use cache
memory and pre-fetch mechanisms to further reduce
energy per access.
Keywords: Low-energy, low-power, asynchronous
design, memory design.
1 Introduction

A General Approach to
Performance Analysis and Optimization
of Asynchronous Circuits
Thesis by
Tak Kwan Lee
In Partial Fulfillment of the Requirements
for the Degree of
Doctor of Philosophy
California Institute of Technology
Pasadena, California
1995
(Submitted May 18, 1995)
+PAGE+

From the Proceedings of the Second International Conference on Coordination Models
and Languages, LNCS 1282, Springer, Berlin, 1997
46
Checking Assumptions in Component
Dynamics at the Architectural Level
Paola Inverardi 1 , Alexander L. Wolf 2 , and Daniel Yankelevich 3
1 Dipartimento di Matematica 2 Department of Computer Science
Universita di L'Aquila University of Colorado
I-67010 L'Aquila, Italy Boulder, CO 80309 USA
3 Departmento de Computacion
Universidad de Buenos Aires
Buenos Aires, Argentina
Abstract. A critical challenge faced by the developer of a software system is to understand whether the system's components correctly integrate. While type theory has provided substantial help in detecting and
preventing errors in mismatched static properties, much work remains
in the area of dynamics. In particular, components make assumptions
about their behavioral interaction with other components, but currently
we have only limited ways in which to state those assumptions and to
analyze those assumptions for correctness.
We have begun to formulate a method that addresses this problem. The
method operates at the architectural level so that behavioral integration
errors, such as deadlock, can be revealed early in development. For each
component, a specification is given both of its own interaction behavior
and of the assumptions that it makes about the interaction behavior of
the external context in which it expects to operate. We have defined an
algorithm that, given such specifications for a set of components, performs "adequacy" checks between the component context assumptions
and the component interaction behaviors. A configuration of a system is
possible if and only if a successful way of "matching" actual behaviors
with assumptions can be found. In effect, we are extending the usual no
tion of type checking to include the checking of behavioral compatibility.
1 Introduction

From the Proc. of the 1996 Inter. Conf. on Requirements Engineering, Colorado Springs, Colorado, April 15-18, 1996
A Facilitator Method for Upstream Design Activities
with Diverse Stakeholders
Regina M. Gonzales and Alexander L. Wolf
Software Engineering Research Laboratory
Department of Computer Science
University of Colorado
Boulder, CO 80309 USA
fregina.gonzales,alwg@cs.colorado.edu
Abstract
This paper presents a method that can be used for
the elicitation and specification of requirements and
high-level design. It supports stakeholder-based modeling, rapid feasibility feedback to marketing, and the
interpersonal dynamics that are necessary to develop
a product. The method centers on the role of the facilitator, an independent agent whose purpose is to build
the Integrated System Model (ISM). The ISM is the result of merging the independent system views from all
stakeholders at any given abstraction level. Formulation of this method was based on the real-world experience of developing a complex, high-technology medical
product with critical time-to-market pressures. It has
proven to be a practical approach to the evolution of
requirements definition and provides a necessary link
to the marketing aspect of a product.
1 Introduction

To appear in the Proc. of the 1998 Inter. Conf. on Software Maintenance, Bethesda, Maryland, USA, November 1998
Evaluating Software Deployment Languages and Schema
An Experience Report
Richard S. Hall, Dennis M. Heimbigner, Alexander L. Wolf
Department of Computer Science
University of Colorado
Boulder, CO 80309 USA
frickhall,dennis,alwg@cs.colorado.edu
Abstract
Software distribution is evolving from a physical media
approach to one where it is practical and advantageous to
leverage the connectivity of networks. Network distribution of software systems provides timeliness and continuity
of evolution not possible with physical media distribution
methods. To support network-based software distribution,
companies and organizations such as Microsoft, Marimba,
and the Desktop Management Task Force (DMTF) are
strengthening their efforts to package software systems in
a way that is conducive to network distribution and management. The result of these efforts has led to the creation
of software description languages and schema such as the
Open Software Description format created by Microsoft and
Marimba and the Management Information Format created
by DMTF. While these efforts are steps in the right direction, they do not address deployment issues in a complete
and systematic fashion. The contribution of this paper is to
evaluate these leading software description technologies.
1. Introduction

DIMACS Series in Discrete Mathematics
and Theoretical Computer Science
Volume 00, 19xx
Global Optimization Methods for Protein Folding Problems
Richard H. Byrd, Elizabeth Eskow, Andre van der Hoek, Robert B.
Schnabel, Chung-Shang Shao, and Zhihong Zou
Abstract. The problem of finding the naturally occurring structure of a protein is believed to correspond to minimizing the free, or potential, energy of
the protein. This is generally a very difficult global optimization problem, with
a large number of parameters and a huge number of local minimizers including many with function values near that of the global minimizer. This paper
presents a new global optimization method for such problems. The method
consists of an initial phase that locates some reasonably low local minimizers of
the energy function, followed by the main phase that progresses from the best
current local minimizers to even lower local minimizers. The method combines
portions that work on small subsets of the parameters, including small-scale
global optimizations using stochastic methods, with local minimizations involving all the parameters. In computational tests on the protein polyalanine
with up to 58 amino acids (116 internal parameters), the method appears to
be very successful in finding the lowest energy structures. The largest case
is particularly significant because the lowest energy structures that are found
include ones that exhibit interesting tertiary as opposed to just secondary
structure.
1. Introduction

Does Configuration Management Research Have a Future?
Andre van der Hoek, Dennis Heimbigner, and Alexander L. Wolf
Department of Computer Science, CB 430
University of Colorado
Boulder, Colorado 80309 USA
fandre,dennis,alwg@cs.colorado.edu
Abstract
In this position paper we raise the question of whether Configuration Management (CM)
research has a future. The new standard in CM systems|typified by commercial products
such as Adele, ADC, ClearCase, Continuus/CM, and CCC/Harvest|largely satisfies the CM
functionality requirements posed by Dart. This implies that research in the area of CM is either
unnecessary or that we must find new challenges in CM on which to focus. We believe that
these challenges indeed exist. Here we present some areas that we feel are good opportunities
for new or continued CM research, and therefore conclude that CM research does have a future.
Introduction

Markov Decision Processes in Large State Spaces
Lawrence K. Saul and Satinder P. Singh
lksaul@psyche.mit.edu, singh@psyche.mit.edu
Center for Biological and Computational Learning
Massachusetts Institute of Technology
79 Amherst Street, E10-243
Cambridge, MA 02139
Abstract
In this paper we propose a new framework for
studying Markov decision processes (MDPs),
based on ideas from statistical mechanics. The
goal of learning in MDPs is to find a policy
that yields the maximum expected return over
time. In choosing policies, agents must therefore weigh the prospects of short-term versus
long-term gains. We study a simple MDP in
which the agent must constantly decide between exploratory jumps and local reward mining in state space. The number of policies to
choose from grows exponentially with the size
of the state space, N . We view the expected returns as defining an energy landscape over policy space. Methods from statistical mechanics
are used to analyze this landscape in the thermodynamic limit N ! 1. We calculate the
overall distribution of expected returns, as well
as the distribution of returns for policies at a
fixed Hamming distance from the optimal one.
We briefly discuss the problem of learning optimal policies from empirical estimates of the
expected return. As a first step, we relate our
findings for the entropy to the limit of high-temperature learning. Numerical simulations
support the theoretical results.
1 Introduction

As appears in Neural Information Processing Systems 4, pp. 251-258, 1992.
The Efficient Learning of Multiple Task
Sequences
Satinder P. Singh
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
Abstract
I present a modular network architecture and a learning algorithm based
on incremental dynamic programming that allows a single learning agent
to learn to solve multiple Markovian decision tasks (MDTs) with significant transfer of learning across the tasks. I consider a class of MDTs,
called composite tasks, formed by temporally concatenating a number of
simpler, elemental MDTs. The architecture is trained on a set of composite and elemental MDTs. The temporal structure of a composite task is
assumed to be unknown and the architecture learns to produce a temporal decomposition. It is shown that under certain conditions the solution
of a composite MDT can be constructed by computationally inexpensive
modifications of the solutions of its constituent elemental MDTs.
1 INTRODUCTION

A Simple Algorithm for Nearest Neighbor Search
in High Dimensions
Sameer A. Nene and Shree K. Nayar
Department of Computer Science
Columbia University
New York, N.Y. 10027
October, 1995
Technical Report No. CUCS-030-95
+PAGE+

Telecentric Optics for Computational Vision
Masahiro Watanabe and Shree K. Nayar
Department of Computer Science,   Columbia University
New York, N.Y. 10027
Abstract
A novel approach to constant-magnification imaging is proposed. Magnification variations due to
changes in focus setting pose problems for important
vision techniques, such as, depth from defocus. It
is shown that magnification of a conventional lens
can be made invariant to defocus by simply adding
an aperture at an analytically derived location. The
resulting optical configuration is called "telecentric."
It is shown that most commercially available lenses
can be turned into telecentric ones. The procedure
for calculating the position of the additional aperture
and a detailed analysis of the photometric and geometric properties of telecentric lenses are discussed.
The magnification invariance of telecentric optics and
its application to the problem of depth from defocus
are experimentally demonstrated in [ Watanabe and
Nayar-1995 ] . The proposed optics was found to result in significantly improved depth maps than those
obtained using a conventional lens.
1 Introduction

The Extruded Generalized Cylinder: A Deformable Model for
Object Recovery
Thomas O'Donnell ? Xi-Sheng Fang ?
Terrence E. Boult ? Alok Gupta
? Dept. of Computer Science Siemens Corporate Research, Inc.
Columbia University   755 College Road East
New York, N.Y. 10027 Princeton, N.J. 08540
Email: odonnell@cs.columbia.edu alok@scr.siemens.com
February 12, 1997
Abstract
There is increasing interest in the recovery of generalized cylinders (GCs) with curved spines. However,
existing formulations of such GCs, for example those
based on the Frenet-Serret frame or the tube model,
suffer serious drawbacks: discontinuities, a lack of expressive power, "narrowing" in the plane normal to
the spine, non-intuitive twisting behavior, and/or off-axis nonorthogonality of their local coordinate systems.
We discuss some of the problems associated with the
non-orthogonality of the coordinate system based on
the Frenet-Serret frame. This non-orthogonality is induced by torsion effects and we show how to correct for
it. We then introduce a new model, the extruded GC
(EGC) model, which overcomes all the problems mentioned above. For complex axes, the EGC model is also
simpler to understand and use than existing models.
The EGC model is further extended by including local surface deformations. Recovery of the deformable
EGC via a physically-motivated paradigm is demonstrated on pre-segmented data from a human carotid
artery.
1 Introduction and Previous

The complexity of multivariate elliptic problems with analytic data
Technical Report CUCS-016-94
Arthur G. Werschulz
Division of Science and Mathematics,   Fordham University
College at Lincoln Center
New York, NY 10023
and
Department of Computer Science
Columbia University
New York, NY 10023
June 28, 1994
Abstract. Let F be a class of functions defined on a d-dimensional domain. Our task is
to compute H m -norm "-approximations to solutions of 2mth-order elliptic boundary-value
problems Lu = f for a fixed L and for f 2 F . We assume that the only information we
can compute about f 2 F is the value of a finite number of continuous linear functionals
of f, each evaluation having cost c(d). Previous work has assumed that F was the unit
ball of a Sobolev space H r of fixed smoothness r, and it was found that the complexity of
computing an "-approximation was comp("; d) = fi(c(d)(1=") d=(r+m) ). Since the exponent
of 1=" depends on d, we see that the problem is intractable in 1=" for any such F of fixed
smoothness r. In this paper, we ask whether we can break intractability by letting F be the
unit ball of a space of infinite smoothness. To be specific, we let F be the unit ball of a
Hardy space of analytic functions defined over a complex d-dimensional ball of radius greater
than one. We then show that the problem is tractable in 1=". More precisely, we prove that
comp("; d) = fi(c(d)(ln 1=") d ), where the fi-constant depends on d. Since for any p &gt; 0, there
is a function K() such that comp("; d) c(d)K(d)(1=") p for sufficiently small ", we see that
the problem is tractable, with (minimal) exponent 0. Furthermore, we show how to construct
a finite element p-method (in the sense of Babuska) that can compute an "-approximation
with cost fi(c(d)(ln 1=") d ). Hence this finite element method is a nearly optimal complexity
algorithm for d-dimensional elliptic problems with analytic data.
1. Introduction

Predictive Dynamic Load Balancing of Parallel and Distributed Rule
and Query Processing
Hasanat M. Dewan Salvatore J. Stolfo
Mauricio Hernandez Jae-Jun Hwang
Department of Computer Science
Columbia University,    New York, NY 10027
CUCS-025-94
(This paper appeared in the Proceedings of the 1994 ACM SIGMOD Conference.)
Abstract
Expert Databases are environments that support the processing of rule programs against a disk resident database.
They occupy a position intermediate between active and deductive databases, with respect to the level of abstraction
of the underlying rule language. The operational semantics
of the rule language influences the problem solving strategy,
while the architecture of the processing environment determines efficiency and scalability.
In this paper, we present elements of the PARADISER
architecture and its kernel rule language, PARULEL. The
PARADISER environment provides support for parallel and
distributed evaluation of rule programs, as well as static
and dynamic load balancing protocols that predictively
balance a computation at runtime. This combination of
features results in a scalable database rule and complex
query processing architecture. We validate our claims by
analyzing the performance of the system for two realistic
test cases. In particular, we show how the performance of a
parallel implementation of transitive closure is significantly
improved by predictive dynamic load balancing.
1 Introduction

A Foundation for Multi-Dimensional Databases
Marc Gyssens
Department WNI
University of Limburg (LUC)
B-3590 Diepenbeek, Belgium.
gyssens@charlie.luc.ac.be
Laks V.S. Lakshmanan
Department of Computer Science
Concordia University
Montreal, Quebec H3G 1M8, Canada
laks@cs.concordia.ca
Abstract
We present a multi-dimensional database model,
which we believe can serve as a conceptual model
for On-Line Analytical Processing (OLAP)-based
applications. Apart from providing the functionalities necessary for OLAP-based applications, the
main feature of the model we propose is a clear
separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably
simple, transparent way. In particular, we show
that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus
and show them to be equivalent. We conclude by
comparing our approach to related work.
The conceptual multi-dimensional database model
developed here is orthogonal to its implementa
tion, which is not a subject of the present paper.
1 Introduction

Submitted for review to Discrete and Computational Geometry, September 1997.
On the Area Bisectors of a Polygon
Karl-Friedrich Bohringer
Cornell University
Bruce Randall Donald
Dartmouth College
Dan Halperin x
Tel Aviv University
Abstract
We consider the family of lines that are area bisectors of a polygon (possibly with
holes) in the plane. We say that two bisectors of a polygon P are combinatorially
distinct if they induce different partitionings of the vertices of P . We derive an algebraic
characterization of area bisectors. We then show that there are simple polygons with n
vertices that have (n 2 ) combinatorially distinct area bisectors (matching the obvious
upper bound), and present an output-sensitive algorithm for computing an explicit
representation of all the bisectors of a given polygon. Our study is motivated by the
development of novel, flexible feeding devices for parts positioning and orienting. The
question of determining all the bisectors of polygonal parts arises in connection with
the development of efficient part positioning strategies when using these devices.
Work on this paper by Karl-Friedrich Bohringer and Bruce Randall Donald has been supported in
part by the National Science Foundation under grants No. IRI-8802390, IRI-9000532, IRI-9201699, and
by a Presidential Young Investigator award to Bruce Donald, in part by NSF/ARPA Special Grant for
Experimental Research No. IRI-9403903, and in part by the Air Force Office of Sponsored Research, the
Mathematical Sciences Institute, Intel Corporation, and AT&T Bell laboratories. Work on this paper by
Dan Halperin has been supported in part by an Alon Fellowship, by ESPRIT IV LTR Project No. 21957
(CGAL), by the USA-Israel Binational Science Foundation, and by the Hermann Minkowski - Minerva
Center for Geometry at Tel Aviv University. A preliminary and abridged version of the paper appeared in
proc. 13th ACM Symp. on Computational Geometry, Nice, 1997, pp. 457-459.
Robotics & Vision Laboratory,   Department of Computer Science,   Cornell University.   Author's current
address:   ALPHA laboratory,   Dept. of Ind. Eng. and Op. Research,   University of California, Berkeley.    Email
address: karl@IEOR.Berkeley.EDU.
Dept. of Computer Science,   Dartmouth College,   6211 Sudikoff Laboratory, Hanover, NH 03755-3510.
brd@cs.dartmouth.edu,   http://www.cs.dartmouth.edu/ brd/.
x Department of Computer Science,   Tel Aviv University,   Tel Aviv 69978, ISRAEL.   Email address:
halperin@math.tau.ac.il.   Part of the work on this paper was carried out while D.H. was at the Robotics
Laboratory, Department of Computer Science, Stanford University.
+PAGE+

Numerical conformal mapping using cross-ratios
and Delaunay triangulation
Tobin A. Driscoll Stephen A. Vavasis
January 23, 1996
Abstract
We propose a new algorithm for computing the Riemann mapping of the
unit disk to a polygon, also known as the Schwarz-Christoffel transformation.
The new algorithm, CRDT, is based on cross-ratios of the prevertices, and also
on cross-ratios of quadrilaterals in a Delaunay triangulation of the polygon.
The CRDT algorithm produces an accurate representation of the Riemann
mapping even in the presence of arbitrary long, thin regions in the polygon,
unlike any previous conformal mapping algorithm. We believe that CRDT can
never fail to converge to the correct Riemann mapping, but the correctness and
convergence proof depend on conjectures that we have so far not been able to
prove. We demonstrate convergence with computational experiments.
The Riemann mapping has applications to problems in two-dimensional
potential theory and to finite-difference mesh generation. We use CRDT to
produce a mapping and solve a boundary value problem on long, thin regions
for which no other algorithm can solve these problems.
1 Conformal mapping

Asymptotically Tight Bounds for Performing
BMMC Permutations on Parallel Disk Systems
Thomas H. Cormen
Thomas Sundquist
Leonard F. Wisniewski
Department of Mathematics and Computer Science
Dartmouth College
Abstract
We give asymptotically equal lower and upper bounds for the number of parallel I/O operations required to perform bit-matrix-multiply/complement (BMMC) permutations on parallel
disk systems. In a BMMC permutation on N records, where N is a power of 2, each (lg N )-bit
source address x maps to a corresponding (lg N)-bit target address by the matrix equation
= A x c, where matrix multiplication is performed over GF (2). The characteristic matrix A
is (lg N )fi(lg N ) and nonsingular over GF (2). Under the Vitter-Shriver parallel-disk model with
N records, D disks, B records per block, and M records of memory, we show a universal lower
bound of
BD
1 + rank
lg(M=B)
parallel I/Os for performing a BMMC permutation, where
is the lower left lg(N=B) fi lg B submatrix of the characteristic matrix. We also present an algo
rithm that uses at most 2N
BD
rank
lg(M=B)
+ 2
parallel I/Os, which asymptotically matches the
lower bound and improves upon the BMMC and bit-permute/complement (BPC) algorithms in
[4]. When rank is low, this method is an improvement over the general-permutation bound of
fi
N
lg(N=B)
We introduce a new subclass of BMMC permutations, called memoryload-dispersal (MLD)
permutations, which can be performed in one pass. This subclass, which is used in the BMMC
algorithm, extends the catalog of one-pass permutations appearing in [4].
Although many BMMC permutations of practical interest fall into subclasses that might be
explicitly invoked within the source code, we show how to detect in at most N=BD+
l
D
parallel I/Os whether a given vector of target addresses specifies a BMMC permutation. Thus,
one can determine efficiently at run time whether a permutation to be performed is BMMC and
then avoid the general-permutation algorithm and save parallel I/Os by using our algorithm.
1 Introduction

TIAS: A Transportable
Intelligent Agent System
Kenneth E. Harker
Senior Thesis
Department of Computer Science
Dartmouth College
Hanover, NH 03755-3510
Dartmouth Technical Report: PCS-TR95-258
iago@cs.dartmouth.edu
5 June 1995
Abstract
In recent years, there has been an explosive growth in the amount of
information available to our society. In particular, the amount of
information available online through vast networks like the global
Internet has been growing at a staggering rate. This growth rate has by far
exceeded the rate of growth in network speeds, as has the number of
individuals and organizations seeking access to this information. There is
thus a motivation to find abstract methods of manipulating this online
data in ways that both serve the needs of end users efficiently and use
network resources intelligently. In lieu of a traditional clientserver model
of information processing, which is both inflexible and potentially very
inefficient, a Transportable Intelligent Agent system has the potential to
achieve a more efficient and flexible network system. An intelligent agent
is a program that models the information space for a user, and allows the
user to specify how the information is to be processed. A transportable
agent can suspend its execution, transport itself to a new location on a
network, and resume execution at the new location. This is a particularly
attractive model for both wireless and dialup networks where a user might
not be able to maintain a permanent network connection, as well as for
situations where the amount of information to be processed is large
relative to the network bandwidth. Preliminary work in the field has
shown that such agent systems are possible and deserve further study.
This thesis describes a prototype transportable intelligent agent system that
extends work already done in the field. Agents are written in a modified
version of the Tcl programming language and transported using TCP/IP
connections. Several simple examples demonstrate the properties of the
system.
1. Introduction

To appear in Parallel Computing, 1997.
Available at URL   ftp://ftp.cs.dartmouth.edu/kotz/papers/nieuwejaar:jgalley.ps.Z
The Galley Parallel File System
Nils Nieuwejaar, David Kotz
fnils,dfkg@cs.dartmouth.edu
Department of Computer Science,   Dartmouth College,   Hanover, NH 03755-3510
Most current multiprocessor file systems are designed to use multiple disks
in parallel, using the high aggregate bandwidth to meet the growing I/O
requirements of parallel scientific applications. Many multiprocessor file
systems provide applications with a conventional Unix-like interface, allowing the application to access multiple disks transparently. This interface conceals the parallelism within the file system, increasing the ease
of programmability, but making it difficult or impossible for sophisticated programmers and libraries to use knowledge about their I/O needs
to exploit that parallelism. In addition to providing an insufficient interface, most current multiprocessor file systems are optimized for a different
workload than they are being asked to support. We introduce Galley, a
new parallel file system that is intended to efficiently support realistic
scientific multiprocessor workloads. We discuss Galley's file structure and
application interface, as well as the performance advantages offered by
that interface.
Key words: Parallel I/O. Multiprocessor file system. Performance evaluation. IBM
SP-2. Scientific Computing.
1 Introduction

Increasing the Resilience of
Distributed and Replicated Database Systems
Idit Keidar Danny Dolev
Institute of Computer Science,
The Hebrew University of Jerusalem,
Jerusalem, Israel, 91904
E-mail: fidish,dolevg@cs.huji.ac.il
Url: http://www.cs.huji.ac.il/f~idish,~dolevg
Abstract
This paper presents a new atomic commitment protocol, enhanced three phase commit
(E3PC ), that always allows a quorum in the system to make progress. Previously suggested
quorum-based protocols (e.g., the quorum-based three phase commit (3PC) [Ske82]) allow a
quorum to make progress in case of one failure. If failures cascade, however, and the quorum
in the system is "lost" (i.e., at a given time no quorum component exists), a quorum can later
become connected and still remain blocked. With our protocol, a connected quorum never
blocks. E3PC is based on the quorum-based 3PC [Ske82], and it does not require more time
or communication than 3PC. We describe how this protocol can be exploited in a replicated
database setting, making the database always available to a majority of the sites.
1 Introduction

Relating Comprehension and Production
in the Acquisition of Morphology
Michael Gasser
Indiana University
Abstract
Most theories of language processing and acquisition make the assumption that
perception and comprehension are related to production, but few have anything say
about how. This paper describes a performance-oriented connectionist model of
the acquisition of morphology in which production builds on representations which
develop during the learning of word recognition. Using artificial language stimuli
embodying simple suffixation, prefixation, and template rules, I demonstrate that
the model generalizes to novel combinations of roots and inflections for both word
recognition and production. I argue that the capacity of connectionist networks to
develop intermediate distributed representations which not only enable the solving
of the task at hand but also facilitate another task offers a plausible account of how
comprehension and production come to share phonological knowledge as words are
learned.
Introduction

An Algebra for List-Oriented Applications
Latha S. Colby
Department of Computer Science
Indiana University
Bloomington, IN, 47405
colby@cs.indiana.edu
February 23, 1992
Abstract
Most data models and query languages, provide mechanisms for dealing with sets
of objects. Many applications nowadays, however, are list-oriented, i.e., deal with
collections or aggregates of objects in which their order is important. A formal model
and an algebra for representing and manipulating list-oriented data are presented in
this paper. We also give the criteria that were used in the design of the algebra and
show how the algebra satisfies these criteria.
The author was supported by a grant from the Indiana Corporation for Science and Technology.
+PAGE+

Coir: A Thread-Model for Supporting Task- and Data- Parallelism
in Object-Oriented Parallel Languages
Neelakantan Sundaresan Dennis Gannon
nsundare@cs.indiana.edu gannon@cs.indiana.edu
Computer Science Department
215 Lindley Hall
Indiana University
Bloomington, IN 47405
Abstract
Data- and task-parallelism are two important parallel programming models. Object-oriented paradigm
in parallelism provides a good way of abstracting out various aspects of computations and computing resources. Using an object-oriented language like C++, one can compose data and control representations
into a single active object.
We propose a thread model of parallelism that addresses both data and task parallelism. Computation
and communication can be overlapped by suspending a thread of computation which is waiting for an
event and running an eligible thread of computation in its place. Threads naturally subsume task-parallelism. Threads are encapsulated into thread objects may be grouped into rope objects [22, 20], that
span the parallel machine domain, for collective computation and communication. Thus data-parallelism
can be supported. Since rope objects are parallel objects, they can be customized, interestingly, in a
serial or a parallel manner. Spatial transparency of objects is achieved by global pointer templates.
We present results from a prototype system running on the SGI Challenge and the Intel Paragon.
keywords: task-parallelism, data-parallelism, thread, rope, object-oriented paradigm
+PAGE+

Detecting Global Predicates in Distributed Systems with Clocks
Scott D. Stoller
Dept. of Computer Science, Indiana University,   Bloomington, IN 47405, USA
29 June 1997
Abstract
This paper proposes a framework for predicate detection in systems of processes with approximately-synchronized real-time clocks. Timestamps from these clocks are used to define two orderings on
events: "definitely occurred before" and "possibly occurred before". These orderings lead naturally
to definitions of 3 distinct detection modalities, i.e., 3 meanings of "predicate held during a computation", namely: Poss T (" possibly held"), Def T (" definitely held"), and Inst ("
definitely held at a specific instant"). This paper defines these modalities and gives efficient algorithms for detecting them; the algorithms are based on algorithms of Cooper and Marzullo, Garg
and Waldecker, and Fromentin and Raynal.
Keywords: global predicate detection, consistent global states, partially-synchronous systems, distributed debugging, real-time monitoring
1 Introduction

Finding Genes in DNA with a Hidden Markov Model
John Henderson Steven Salzberg Kenneth H. Fasman
Jan. 31, 1996, revised Aug. 28, 1996
Abstract
This study describes a new Hidden Markov Model (HMM) system for segmenting uncharacterized genomic DNA sequences into exons, introns, and intergenic
regions. Separate HMM modules were designed and trained for specific regions of
DNA: exons, introns, intergenic regions, and splice sites. The models were then
tied together to form a biologically feasible topology. The integrated HMM was
trained further on a set of eukaryotic DNA sequences, and tested by using it to
segment a separate set of sequences. The resulting HMM system, which is called
VEIL (Viterbi Exon-Intron Locator), obtains an overall accuracy on test data of
92% of total bases correctly labelled, with a correlation coefficient of 0.68. Using the more stringent test of exact exon prediction, VEIL correctly located both
ends of 46% of the exons. Moreover, more than 50% of the exons it predicts are
exactly correct. These results compare favorably to the best previous results for
gene structure prediction, and demonstrate the benefits of using HMMs for this
problem.
1 Introduction

A Feedback Mechanism for Query by Navigation
F.C. Berger
Th.P. van der Weide n
Published as:   F.C. Berger and Th.P. van der
Weide.   A Feedback Mechanism for Query by
Navigation.   Technical Report CSI-R9413,   Computing Science Institute, University of Nijmegen,
Nijmegen, The Netherlands,   October 1994.
Abstract
The Two-Level Hypermedia Paradigm sees an
Information Retrieval System as consisting of
a document network (the Hyperbase) and a
descriptor (term) network (the Hyperindex).
Query by Navigation is a process whereby the
searcher gives a description of the Information Need by travelling through the descriptor
network. This paper presents a formalism for
expressing the effects of traversing the Hyper-index on the elements of the Hyperindex. This
formalism makes use of probabilities for mod-elling the searcher's behavious. The events
which can occur during the search process
are discussed and modelled. Some important
properties, which are reasonable to demand of
a retrieval system, can be proven to be valid
if this formalism is adopted. A mechanism
for assigning a measure of relevance to documents is presented. This uses the formalism
mentioned above. An example will show the
effectiveness of The aspect of relevance feedback and its role in Query by Navigation is
introduced by examining the different level on
which the searcher can offer information for
weeding out unwanted sections of the search
space. In order to illustrate the workings of
Query by Navigation a small example is included.
Dept. of Information Systems, Faculty of Mathematics and
Informatics, University of Nijmegen,   Toernooiveld 1, 6525 ED
Nijmegen, The Netherlands
Keywords: information retrieval, relevance feedback, user modelling, query formulation
Classification: AMS 68P20; CR H.3.3, H.5.1
1 Introduction

A Unifying Framework
for
Conceptual Data Modelling Concepts
P.J.M. Frederiks, A.H.M. ter Hofstede, E. Lippe
Department of Information Systems
University of Nijmegen
Toernooiveld 1
NL-6525 ED Nijmegen
The Netherlands
fpaulf,arthur,ernstlg@cs.kun.nl
Published as:   P.J.M. Frederiks, A.H.M. ter Hofstede, and E. Lippe.   A Unifying Framework
for Conceptual Data Modelling Concepts.   Technical Report CSI-R9410,   Computing Science
Institute, University of Nijmegen,   Nijmegen, The Netherlands,   September 1994.
Abstract
For succesful information systems development, conceptual data modelling is essential.
Nowadays many techniques for conceptual data modelling exist, examples are NIAM, FORM,
PSM, many (E)ER variants, IFO, and FDM. In-depth comparisons of concepts of these techniques is very difficult as the mathematical formalisations of these techniques, if existing at
all, are very different. As such there is a need for a unifying formal framework providing a
sufficiently high level of abstraction. In this paper the use of category theory for this purpose
is addressed. Well-known conceptual data modelling concepts are discussed from a category
theoretic point of view. Advantages and disadvantages of the approach chosen will be outlined.
Keywords: Conceptual Data Modelling, Category Theory, Meta Modelling
Classification: 68P99 (AMS-1991), H.1.0. (CR-1991)
1 Introduction

Autonomous Acquisition of
Sensor-Motor Couplings in Robots
Ulrich Nehmzow
Technical Report UMCS-94-11-1
+PAGE+

Middle Scale Robot Navigation A Case Study
Carl Owen and Ulrich Nehmzow
Department of Computer Science
University of Manchester
Manchester M13 9PL
United Kingdom
owenc@cs.man.ac.uk
u.nehmzow@cs.man.ac.uk
7/4/97
Abstract
In this paper we present results of experiments carried out with a route learning system for a
mobile robot, conducted in a `real world' environment covering distances of several hundred metres.
The system uses no odometry and is based on a self-organising mapbuilding process using perceptual
landmarks.
A performance metric is defined and used to measure the robot's ability to traverse the route.
1 Introduction

Animal and Robot Navigation
Ulrich Nehmzow
Department of Computer Science
Manchester University
Manchester M13 9PL
United Kingdom
u.nehmzow@cs.man.ac.uk
Abstract
It is argued that the following three properties are foundations of robust robot
navigation:
* The use of landmarks (and, in particular, the use of a compass sense),
* the use of canonical paths, and
* the use of topological rather than geometrical maps.
Some examples of successful animal navigation are presented that support this view.
We have performed initial experiments with mobile robots to investigate mechanisms
suitable to implement such navigational architectures. Experiments concerning navigation by dead reckoning are presented, and a differential light compass is introduced
to aid robot navigation.
1 Introduction

Error-Correcting Output Codes:
A General Method for Improving
Multiclass Inductive Learning Programs
Thomas G. Dietterich and Ghulum Bakiri
Department of Computer Science
Oregon State University
Corvallis, OR 97331-3202
Abstract
Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a
discrete set containing k &gt; 2 values (i.e., k "classes").
The definition is acquired by studying large collections
of training examples of the form hx i ; f(x i )i. Existing
approaches to this problem include (a) direct application of multiclass algorithms such as the decision-tree
algorithms ID3 and CART, (b) application of binary
concept learning algorithms to learn individual binary
functions for each of the k classes, and (c) application
of binary concept learning algorithms with distributed
output codes such as those employed by Sejnowski and
Rosenberg in the NETtalk system. This paper compares these three approaches to a new technique in
which BCH error-correcting codes are employed as a
distributed output representation. We show that these
output representations improve the performance of ID3
on the NETtalk task and of backpropagation on an
isolated-letter speech-recognition task. These results
demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass
problems.
Introduction

Bezier Nets, Convexity and Subdivision on Higher
Dimensional Simplices
Tim Goodman
Department of Mathematics
and Computer Sciences
The University of Dundee
Dundee DD1 4HN, Scotland
Jorg Peters 1
Department of Computer Sciences
Purdue University
W-Lafayette, IN 47907-1398
USA
October 26, 1994
1 supported by NSF grant 9396164-CCR
+PAGE+

Analysis of Algorithms Generalizing B-Spline
Subdivision
Jorg Peters Ulrich Reif
January 27, 1997
Abstract
A new set of tools for verifying smoothness of surfaces generated by stationary
subdivision algorithms is presented. The main challenge here is the verification of
injectivity of the characteristic map. The tools are sufficiently versatile and easy
to wield to allow, as an application, a full analysis of algorithms generalizing bi-quadratic and bicubic B-spline subdivision. In the case of generalized biquadratic
subdivision the analysis yields a hitherto unknown sharp bound strictly less than
one on the second largest eigenvalue of any smoothly converging subdivision.
Keywords: subdivision, arbitrary topology, characteristic map, Doo-Sabin Algorithm, Catmull-Clark algorithm, B-spline
AMS subject classification: 65D17, 65D07, 68U07
Abbreviated title: Generalized B-Spline Subdivision
1 Introduction

Performance of Temporal Reasoning Systems
Ed Yampratoom James F. Allen
The University of Rochester
Computer Science Department
Rochester, New York 14627
TRAINS Technical Note 93-1
May 1993
Abstract
This paper describes the performance evaluation of six temporal reasoning systems.
We show that if you are working with large temporal datasets where information
is added incrementally throughout the execution of the program, systems using incompletely connected graphs (i.e., TMM, TimeGraph and TimeGraph-II) seem the
best option. While they do not offer the constant query time of systems using fully
connected graphs (i.e. the systems based on constraint satisfaction), the savings at
assertion time are so substantial that the relatively small performance penalty for
queries is a reasonable tradeoff. Of course, these systems do not offer the expressiv
ity of the interval-based systems as they only handle point-based relations. Of the
three, TimeGraph-II offers a wider range of qualitative relations as it handles point
inequality. It does not currently handle metric information, however, as do TMM
and TimeGraph. Thus decisions between these three may be more determined by the
reasoning capabilities required rather than raw performance.
This material is based upon work supported in part by U.S. Air Force-Rome Laboratory research
contract no. F30602-91-C-0010.
+PAGE+

Utterance Units in Spoken Dialogue
David R. Traum 1 and Peter A. Heeman 2
Abstract. In order to make spoken dialogue systems more sophisticated, designers need to better understand the conventions that people
use in structuring their speech and in interacting with their fellow con-versants. In particular, it is crucial to discriminate the basic building
blocks of dialogue and how they affect the way people process language. Many researchers have proposed the utterance unit as the
primary object of study, but defining exactly what this is has remained a difficult issue. To shed light on this question, we consider
grounding behavior in dialogue, and examine co-occurrences between
turn-initial grounding acts and utterance unit signals that have been
proposed in the literal, namely prosodic boundary tones and pauses.
Preliminary results indicate high correlation between grounding and
boundary tones, with a secondary correlation for longer pauses. We
also consider some of the dialogue processing issues which are impacted by a definition of utterance unit.
1 INTRODUCTION

Tally NP Sets and Easy Census Functions
Judy Goldsmith 1
Department of Computer Science
University of Kentucky
Lexington, KY 40506, USA
goldsmit@cs.engr.uky.edu
Mitsunori Ogihara 2
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
ogihara@cs.rochester.edu
Jorg Rothe 3
Institut fur Informatik
Friedrich-Schiller-Universitat Jena
07740 Jena, Germany
rothe@informatik.uni-jena.de
March 19, 1998
1 Supported in part by NSF grant CCR-9315354.
2 Supported in part by NSF CAREER Award CCR-9701911.
3 Supported in part by grants NSF-INT-9513368/DAAD-315-PRO-fo-ab and NSF-CCR-9322513
and by a NATO Postdoctoral Science Fellowship from the Deutscher Akademischer Austausch-dienst ("Gemeinsames Hochschulsonderprogramm III von Bund und Landern"). Current address:
Department of Computer Science, University of Rochester, Rochester, NY 14627, USA. Work done
in part while visiting the University of Kentucky and the University of Rochester.
+PAGE+

Scalable Atomic Primitives for Distributed
Shared Memory Multiprocessors
(Extended Abstract)
Maged M. Michael
Department of Computer Science
University of Rochester
Rochester, NY 14627-0226
USA
Michael L. Scott
Department of Computer Science
University of Rochester
Rochester, NY 14627-0226
USA
Abstract
Our research addresses the general topic of atomic update of shared data
structures on large-scale shared-memory multiprocessors. In this paper
we consider alternative implementations of the general-purpose single-address atomic primitives fetch and , compare and swap, load linked,
and store conditional. These primitives have proven popular on small-scale bus-based machines, but have yet to become widely available on
large-scale, distributed shared memory machines. We propose several alternative hardware implementations of these primitives, and then analyze
the performance of these implementations for various data sharing patterns. Our results indicate that good overall performance can be obtained
by implementing compare and swap in the cache controllers, and by pro
viding an additional instruction to load an exclusive copy of a cache line.
1 INTRODUCTION

THE COMPLEXITY OF COMPUTING
MAXIMAL WORD FUNCTIONS
Eric Allender, Danilo Bruschi
and Giovanni Pighizzini
Abstract. Maximal word functions occur in data retrieval applications
and have connections with ranking problems, which in turn were first
investigated in relation to data compression [21]. By the "maximal word
function" of a language L , we mean the problem of finding, on
input x, the lexicographically largest word belonging to L that is smaller
than or equal to x.
In this paper we present a parallel algorithm for computing maximal
word functions for languages recognized by one-way nondeterministic
auxiliary pushdown automata (and hence for the class of context-free
languages).
This paper is a continuation of a stream of research focusing on the
problem of identifying properties others than membership which are
easily computable for certain classes of languages. For a survey, see [24].
Subject classifications. 68Q15,68Q25,68Q45.
1. Introduction

Priors for Infinite Networks
Radford M. Neal
Technical Report CRG-TR-94-1
Department of Computer Science
University of Toronto
10 King's College Road
Toronto, Canada M5S 1A4
E-mail: radford@cs.toronto.edu
1 March 1994
Abstract
Bayesian inference begins with a prior distribution for model parameters that is
meant to capture prior beliefs about the relationship being modeled. For multilayer
perceptron networks, where the parameters are the connection weights, the prior
lacks any direct meaning | what matters is the prior over functions computed
by the network that is implied by this prior over weights. In this paper, I show
that priors over weights can be defined in such a way that the corresponding
priors over functions reach reasonable limits as the number of hidden units in the
network goes to infinity. When using such priors, there is thus no need to limit the
size of the network in order to avoid "overfitting". The infinite network limit also
provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions, which can be
smooth, Brownian, or fractional Brownian, depending on the hidden unit activation
function and the prior for input-to-hidden weights. Quite different effects can be
obtained using priors based on non-Gaussian stable distributions. In networks with
more than one hidden layer, a combination of Gaussian and non-Gaussian priors
appears most interesting.
+PAGE+

Generative Models for Discovering Sparse Distributed
Representations
Geoffrey E. Hinton and Zoubin Ghahramani
Department of Computer Science
University of Toronto
Toronto, Ontario, M5S 1A4, Canada
hinton@cs.toronto.edu, zoubin@cs.toronto.edu
May 9, 1997
A modified version to appear in Philosophical Transactions of the Royal Society B, 1997.
Abstract
We describe a hierarchical, generative model that can be viewed as a non-linear generalization of factor analysis and can be implemented in a neural network. The model uses
bottom-up, top-down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated
using a very simple learning rule that only requires locally available information. We demon
strate that the network learns to extract sparse, distributed, hierarchical representations.
1 Introduction

To appear in Jordan, MI, Kearns MJ, and Solla, SA Advances in Neural Information
Processing Systems 10. MIT Press: Cambridge, MA, 1998.
Hierarchical Non-linear Factor Analysis
and Topographic Maps
Zoubin Ghahramani and Geoffrey E. Hinton
Dept. of Computer Science, University of Toronto
Toronto, Ontario, M5S 3H5, Canada
http://www.cs.toronto.edu/neuron/
fzoubin,hintong@cs.toronto.edu
Abstract
We first describe a hierarchical, generative model that can be
viewed as a non-linear generalisation of factor analysis and can
be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using
top-down, bottom-up and lateral connections. These connections
can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse,
distributed, hierarchical representation of depth from simplified
random-dot stereograms and the localised disparity detectors in
the first hidden layer form a topographic map. When presented
with image patches from natural scenes, the model develops topo
graphically organised local feature detectors.
1 Introduction

Attentive Object Recognition in the Selective Tuning
Network
by
David C. Dolson
A thesis submitted in conformity with the requirements
for the degree of Master of Science
Graduate Department of Computer Science
University of Toronto
c Copyright by David C. Dolson 1997
+PAGE+

Gap-Definable Counting Classes
Stephen A. Fenner
Computer Science Department
University of Southern Maine
96 Falmouth Street
Portland, Maine 04103
Lance J. Fortnow
Stuart A. Kurtz
Computer Science Department
University of Chicago
1100 East Fifty-eighth Street
Chicago, Illinois 60637
July 12, 1992
Work done while the first author was a graduate student at the University of Chicago Computer Science Depart
ment, supported in part by a University of Chicago Fellowship.
Supported by NSF Grant CCR-9009936
+PAGE+

Evaluating Weak Memories with Maya
Divyakant Agrawal Manhoi Choy Hong Va Leong Ambuj K. Singh
Department of Computer Science
University of California at Santa Barbara
Santa Barbara, CA 93106
Abstract
Maya is a simulation platform for evaluating the performance of parallel programs on parallel architectures with different memory coherence protocols. Rapid prototyping of different memory protocols
supporting varying degrees of coherence is possible and the impact of these protocols on the performance
of application programs can be studied. Implementations of existing weak memories along with some
new primitives using Maya are presented. The results of running some user applications are summarized
and the impact of weak memories on the efficiency of parallel programs is discussed.
Keywords: distributed shared memory, memory consistency, parallel programming, weak
memories
+PAGE+

Q-Learning for Bandit Problems
Michael O. Duff
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
duff@cs.umass.edu
Abstract
Multi-armed bandits may be viewed as
decompositionally-structured Markov decision processes (MDP's) with potentially very-large state sets. A particularly elegant
methodology for computing optimal policies
was developed over twenty ago by Gittins
[Gittins & Jones, 1974]. Gittins' approach
reduces the problem of finding optimal policies for the original MDP to a sequence of
low-dimensional stopping problems whose solutions determine the optimal policy through
the so-called "Gittins indices." Katehakis
and Veinott [Katehakis & Veinott, 1987] have
shown that the Gittins index for a process
in state i may be interpreted as a particular
component of the maximum-value function
associated with the "restart-in-i" process,
a simple MDP to which standard solution
methods for computing optimal policies, such
as successive approximation, apply. This paper explores the problem of learning the Git-tins indices on-line without the aid of a process model; it suggests utilizing process-state-specific Q-learning agents to solve their respective restart-in-state-i subproblems, and
includes an example in which the online reinforcement learning approach is applied to
a problem of stochastic scheduling|one instance drawn from a wide class of problems
that may be formulated as bandit problems.
1 INTRODUCTION

Intra-Option Learning about Temporally Abstract Actions
Richard S. Sutton
Department of Computer Science
University of Massachusetts
Amherst, MA 01003-4610
rich@cs.umass.edu
Doina Precup
Department of Computer Science
University of Massachusetts
Amherst, MA 01003-4610
dprecup@cs.umass.edu
Satinder Singh
Department of Computer Science
University of Colorado
Boulder, CO 80309-0430
baveja@cs.colorado.edu
Abstract
Several researchers have proposed modeling
temporally abstract actions in reinforcement
learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of
options can be learned using methods designed
for semi-Markov decision processes (SMDPs).
However, all these methods require an option to
be executed to termination. In this paper we explore methods that learn about an option from
small fragments of experience consistent with
that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience
within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously
about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the
consequences of options. We present computational examples in which these new methods
learn much faster than SMDP methods and learn
effectively when SMDP methods cannot learn at
all. We also sketch a convergence proof for intra
option value learning.
1 Introduction

Humans Plus Agents
Maintain Schedules Better
than Either Alone
Tim Oates and Paul R. Cohen
Computer Science Technical Report 94-03
Experimental Knowledge Systems Laboratory
Department of Computer Science, Box 34610
Lederle Graduate Research Center
University of Massachusetts
Amherst, MA 01003-4610
Abstract
Tracking and evaluating the progress of large, complex plans or
schedules as they unfold in real time is extremely difficult for humans.
In this paper we present a mixed-initiative system for the task of schedule maintenance in a simulated shipping network. A schedule maintenance agent monitors the network, predicting the occurrence of states
that may result in reduced throughput and formulating schedule modifications to avoid those states. The goal is to maximize throughput
while minimizing disruptions to the original schedule. We present results of experiments in which human subjects attempt to obtain that
goal both with and without the aid of the agent. We found that the human and the agent working together are able to achieve better results
than either one working alone. In addition to looking at global performance measures such as throughput, we analyze individual schedule
modification decisions made by subjects in an attempt to assign credit
for the improvements in performance.
This research is supported by ARPA-AFOSR contract F30602-91-C-0076.
+PAGE+

Design-to-time Real-Time Scheduling
Alan Garvey Victor Lesser
Department of Computer Science
University of Massachusetts
Amherst, Massachusetts 01003
CSNET: GARVEY@CS.UMASS.EDU
April 14, 1994
Abstract
Design-to-time is an approach to problem-solving in resource-constrained domains where:
multiple solution methods are available for tasks, those solution methods make tradeoffs in
solution quality versus time, and satisficing solutions are acceptable. Design-to-time involves
designing a solution to a problem that uses all available resources to maximize the solution
quality within the available time. This paper defines the design-to-time approach in detail,
contrasting it to the anytime algorithm approach, and presents a heuristic algorithm for design-to-time real-time scheduling.
Our blackboard architecture that implements the design-to-time approach is discussed and
an example problem and solution from the Distributed Vehicle Monitoring Testbed (DVMT) is
described in detail. Experimental results, generated using a simulation, show the effects of
various parameters on scheduler performance. Finally we discuss future research goals and
plans.
1 This work was partly supported by the Office of Naval Research under a University Research Initiative

Issues in Design-to-time Real-time Scheduling
Alan Garvey
Department of Computer Science
Pacific Lutheran University
Tacoma, WA 98447
Email: garveyaj@plu.edu
Victor Lesser
Computer Science Department
University of Massachusetts
Amherst, MA 01003
Email: lesser@cs.umass.edu
Abstract
Design-to-time real-time scheduling is an alternative to the many flexible computation approaches that are based on anytime algorithms.
It builds schedules at runtime that dynamically
combine solutions to subproblems, taking advantage of the time available to achieve the best
results it can. In this paper we look in detail at
a few issues related to design-to-time, including where the approximations we rely on come
from, how uncertainty affects the scheduling
process and the interface between the sched-uler and its invoker.
Introduction

Interprocedural Transformations for Parallel Code Generation
Mary W. Hall Ken Kennedy Kathryn S. M c Kinley
Department of Computer Science, Rice University,   Houston, TX 77251-1892
Abstract
We present a new approach that enables compiler
optimization of procedure calls and loop nests containing procedure calls. We introduce two inter-procedural transformations that move loops across procedure boundaries, exposing them to traditional optimizations on loop nests. These transformations are
incorporated into a code generation algorithm for a
shared-memory multiprocessor. The code generator relies on a machine model to estimate the expected benefits of loop parallelization and parallelism-enhancing
transformations. Several transformation strategies are
explored and one that minimizes total execution time is
selected. Efficient support of this strategy is provided
by an existing interprocedural compilation system. We
demonstrate the potential of these techniques by applying this code generation strategy to two scientific
applications programs.
1 Introduction

Integrated Signal Processing
and Signal Understanding 1
Victor Lesser, Hamid Nawab ,
Malini Bhandaru, Norman Carver,
Zarko Cvetanovic, Izaskun Gallastegi,
Frank Klassner
COINS Technical Report 91-34
November 1991
Electrical and Computer Engineering Dept.
Boston University
44 Cummington Street
Boston, Massachusetts 02125
Abstract
This report outlines the IPUS paradigm, named for Integrated Processing and Understanding of Signals, which permits sophisticated interaction between theory-based problem
solving in signal processing and heuristic problem-solving in signal interpretation. The need
for such a paradigm arises in signal understanding domains that require the processing of
complicated interacting signals under variable signal-to-noise ratios. One such application is
sound understanding, in the context of which we report on a testbed experiment illustrating
the functionality of key IPUS architecture components.
1 This work was supported by the Office of Naval Research under University Research Initiative

An Application of Distributed Solid Modeling: Feature Recognition
William C. Regli
National Institute of Standards and Technology
Manufacturing Systems Integration Division
Building 220, Room A-127
Gaithersburg, MD 20899
regli@cme.nist.gov
Satyandra K. Gupta
Mechanical Engineering Department
Institute for Systems Research
University of Maryland
College Park, MD 20742 USA
skgupta@src.umd.edu
Dana S. Nau
Computer Science Department
Institute for Advanced Computer Studies
Institute for Systems Research
University of Maryland
College Park, MD 20742 USA
nau@cs.umd.edu
Available as CS-TR-3375, UMIACS-TR-94-126, ISR-TR 94-82.
Abstract
The availability of low-cost computational power is a driving force behind the growing sophistication
of CAD software. Tools designed to reduce time-consuming build-test-redesign iterations are essential
for increasing engineering quality and productivity. However, automation of the design process poses
many difficult computational problems. As more downstream engineering activities are being considered
during the design phase, guaranteeing reasonable response times within design systems becomes problematic. Design is an interactive process and speed is a critical factor in systems that enable designers to
explore and experiment with alternative ideas during the design phase. Achieving interactivity requires
an increasingly sophisticated allocation of computational resources in order to perform realistic design
analyses and generate feedback in real time.
This paper presents our initial efforts to develop techniques to apply distributed algorithms to the
problem of recognizing machining features from solid models. Existing work on recognition of features
has focused exclusively on serial computer architectures. Our objective is to show that distributed
algorithms can be employed on realistic parts with large numbers of features and many geometric and
topological entities to obtain significant improvements in computation time using existing hardware and
software tools. Migrating solid modeling applications toward a distributed computing framework enables
interconnection of many of the autonomous and geographically diverse software tools used in the modern
manufacturing enterprise.
This has been implemented on a network of SUN workstations using the ACIS solid modeler and the
NIH C++ class library; inter-processor communication is handled with TCP/IP-based network communication tools.
Keywords: Multiprocessor Solid Modeling, Feature Recognition, Feature-Based Modeling, Distributed
Computing.
This work was supported in part by NSF Grants IRI9306580, DDM-9201779, EEC 94-02384 and a forgivable loan from
General Electric Corporation awarded to the first author. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
Also affiliated with: Computer Science Department and Institute for Systems Research, University of Maryland, College
Park.
+PAGE+

DTP: An Efficient Transport Protocol
Dheeraj Sanghi and Ashok K. Agrawala
Department of Computer Science, University of Maryland,   College Park,
MD 20742, USA
Abstract
We recently introduced a new flow control scheme, called send-time control, which is
based on a deterministic model of virtual circuits in a computer network. In this scheme,
the time at which a packet is sent by a source is computed from estimates of round-trip
time, traffic in the network and bottleneck service time. In this paper, we describe a new
transport protocol, called DTP, which uses send-time control as its flow control scheme.
Preliminary measurements of coast-to-coast connections over the Internet show significant
performance improvement over TCP, which is the most commonly used transport protocol
in the Internet today.
Keyword Codes: C.2.2
Keywords: Computer-Communication Networks, Network Protocols
1. Introduction

TR-1923   September 1987
Invariant Subspaces
and
Capital Punishment
(A Participatory Paper)
G. W. Stewart
abstract
The notion of invariant subspaces is useful in a number of theoretical and practical applications. In this paper we give an
elementary treatment of invariant subspaces that stresses their
connection with simple eigenvalues and their eigenvectors.
Department of Computer Science and Institute for Physical Science and Technology,
University of Maryland,   College Park, MD 20742.
+PAGE+

A SERVER OF DISTRIBUTED DISK PAGES
USING A CONFIGURABLE SOFTWARE BUS
Charles Falkenberg, Paul Hagger and Steve Kelley
Institute for Advanced Computer Studies and
The Department of Computer Science
University of Maryland
College Park, MD 20742
ABSTRACT
As network latency drops below disk latency, access time to a remote disk will begin
to approach local disk access time. The performance of I/O may then be improved
by spreading disk pages across several remote disk servers and accessing disk pages
in parallel. To research this we have prototyped a data page server called a Page
File. This persistent data type provides a set of methods to access disk pages stored
on a cluster of remote machines acting as disk servers. The goal is to improve the
throughput of database management system or other I/O intensive application by
accessing pages from remote disks and incurring disk latency in parallel. This report
describes the conceptual foundation and the methods of access for our prototype.
With oversight by Office of Naval Research, this research is supported by ARPA/SISTO in
conjunction with the Domain Specific Software Architectures project.
+PAGE+

Hierarchical Inter-Domain Routing Protocol
with On-Demand ToS and Policy Resolution
Cengiz Alaettinoglu, A. Udaya Shankar
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, Maryland 20742
CS-TR-3299
June 20, 1994
Abstract
Traditional inter-domain routing protocols based on superdomains maintain either "strong"
or "weak" ToS and policy constraints for each visible superdomain. With strong constraints,
a valid path may not be found even though one exists. With weak constraints, an invalid
domain-level path may be treated as a valid path.
We present an inter-domain routing protocol based on superdomains, which always finds
a valid path if one exists. Both strong and weak constraints are maintained for each visible
superdomain. If the strong constraints of the superdomains on a path are satisfied, then the
path is valid. If only the weak constraints are satisfied for some superdomains on the path, the
source uses a query protocol to obtain a more detailed "internal" view of these superdomains,
and searches again for a valid path. Our protocol handles topology changes, including node/link
failures that partition superdomains. Evaluation results indicate our protocol scales well to large
internetworks.
Categories and Subject Descriptors: C.2.1 [Computer-Communication Networks]: Network Archi
tecture and Design|packet networks; store and forward networks; C.2.2 [Computer-Communication Net
works]: Network Protocols|protocol architecture; C.2.m [Routing Protocols]; F.2.m [Computer Network
Routing Protocols].
This work is supported in part by ARPA and Philips Labs under contract DASG60-92-0055 to Department
of Computer Science, University of Maryland, and by National Science Foundation Grant No. NCR 89-04590. The
views, opinions, and/or findings contained in this report are those of the author(s) and should not be interpreted as
representing the official policies, either expressed or implied, of the Advanced Research Projects Agency, PL, NSF,
or the U.S. Government. Computer facilities were provided in part by NSF grant CCR-8811954.
+PAGE+

Finite State Machines and Recurrent Neural Networks -
Automata and Dynamical Systems Approaches
Peter Tino a;b , Bill G. Horne b , C. Lee Giles b;c
a Department of Computer Science and Engineering
Slovak Technical University
Ilkovicova 3, 812 19 Bratislava, Slovakia
Email: tino@decef.elf.stuba.sk
b NEC Research Institute
4 Independence Way
Princeton, NJ 08540
Email:
ftino,horne,gilesg@research.nj.nec.com
c Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742
Technical Report
UMIACS-TR-95-1 and CS-TR-3396
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742
+PAGE+

University of Maryland College Park
Institute for Advanced Computer Studies   TR-95-93
Department of Computer Science   TR-3535
On the Perturbation of
LU and Cholesky Factors
G. W. Stewart
October, 1995
ABSTRACT
In a recent paper, Chang and Paige have shown that the usual perturbation bounds for Cholesky factors can systematically overestimate
the errors. In this note we sharpen their results and extend them to
the factors of the LU decomposition. The results are based on a new
formula for the first order terms of the error in the factors.
This report is available by anonymous ftp from thales.cs.umd.edu in the directory
pub/reports.
Department of Computer Science and Institute for Advanced Computer Studies, University
of Maryland,   College Park, MD 20742.   This work was supported in part by the National Science
Foundation under grant CCR 95503126.
+PAGE+

The Relative Importance of
Concurrent Writers and Weak Consistency Models
Pete Keleher
Department of Computer Science
University of Maryland
College Park, MD 20742-3255
keleher@cs.umd.edu
Abstract
This paper presents a detailed comparison of the relative importance of allowing concurrent writers
versus the choice of the underlying consistency model. Our comparison is based on single- and multiple-writer versions of a lazy release consistent (LRC) protocol, and a single-writer sequentially consistent
protocol, all implemented in the CVM software distributed shared memory system.
We find that in our environment, which we believe to be representative of distributed systems today
and in the near future, the consistency model has a much higher impact on overall performance than the
choice of whether to allow concurrent writers. The multiple writer protocol performs an average of 9%
better than the single writer LRC protocol, but 34% better than the single-writer sequentially consistent
protocol. Set against this, MW-LRC required an average of 72% memory overhead, compared to 10%
overhead for the single-writer protocols.
1 Introduction

On Hybrid Synthesis for Hierarchical Structured Petri Nets
Hong Liu Jun-Cheol Park Raymond E. Miller
Department of Computer Science
University of Maryland,   College Park, MD 20742
flhong, jcpark, millerg@cs.umd.edu
April 23, 1996
Abstract
We propose a hybrid method for synthesis of hierarchical structured Petri nets. In a top-down manner, we decompose a system into a set of subsystems at each level of abstraction, each
of these is specified as a blackbox Petri net that has multiple inputs and outputs. We stipulate
that each subsystem satisfies the following I/O constraints: (1) At any instance of time, at
most one of the inputs can be activated; and (2) If one input is activated, then the subsystem
must consume the input and produce exactly one output within a finite length of time. We
give a stepwise refinement procedure which starts from the initial high-level abstraction of the
system and expands an internal place of a blackbox Petri net into a more detailed subnet at
each step. By enforcing the I/O constraints of each subsystem in each intermediate abstraction,
our refinement maintains the sequencing of transitions prescribed by the initial abstraction of
the system. Next, for the bottom-up synthesis, we present interconnection rules for sequential,
parallel, and loop structures and prove that each rule maintains the I/O constraints. Thus, by
incorporating these interconnection rules into our refinement formulation, our approach can be
regarded as a hybrid Petri net synthesis technique that employs both top-down and bottom-up
methods. The major advantage of the method is that the modeling details can be introduced
incrementally and naturally, while the important logical properties of the resulting Petri net are
guaranteed.
This research was supported by NASA Grant No. NAG 5-2648.
+PAGE+

Exploiting the Temporal Structure of MPEG Video
for the Reduction of Bandwidth Requirements
Marwan Krunz and Satish Tripathi
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
Email: krunz@cs.umd.edu
Abstract
We propose a new bandwidth allocation scheme for VBR video traffic in ATM networks.
The scheme is tailored to MPEG-coded video sources that require stringent and deterministic
quality-of-service guarantees. By exploiting the temporal structure of MPEG sources, we show
that our scheme results in an effective bandwidth which, in most cases, is less than the source
peak rate. The reduction in the bandwidth requirement is achieved without sacrificing any
perceived QoS. Efficient procedures are provided for the computation of the effective bandwidth
under heterogeneous MPEG sources. The effective bandwidth strongly depends on the arrangement of the multiplexed streams which is a measure of the degree of synchronization between the
GOP patterns of different streams. Assuming that all possible arrangements are equi-probable,
we derive an expression for the asymptotic tail distribution of the effective bandwidth. From
the tail distribution, we compute several performance measures for the call blocking probability
when the allocation is made based on the effective bandwidth. In the case of homogeneous
sources, we give a closed-form expression for the `best' arrangement that results in the `optimal'
effective bandwidth. Numerical examples based on real MPEG traces are used to demonstrate
the advantages of our scheme.
Keywords: bandwidth allocation, MPEG, statistical multiplexing, CAC.
This research was partially supported by the NSF grant # CCR 9318933.
+PAGE+

CS-TR-3692   Sept. 1996
Putting Visualization to Work:
ProgramFinder for Youth Placement
Jason Ellis, Anne Rose, Catherine Plaisant
Human-Computer Interaction Laboratory
Institute for Advanced Computer Studies
University of Maryland,   College Park, MD 20742-3255
http://www/cs.umd.edu/projects/hcil/
jellis, rose, plaisant-@cs.umd.edu
Abstract
The Human-Computer Interaction Laboratory (HCIL) and the Maryland Department of Juvenile Justice (DJJ) have
been working together to develop the ProgramFinder, a tool for choosing programs for a troubled youth from drug
rehabilitation centers to secure residential facilities. The seemingly straightforward journey of the ProgramFinder
from an existing user interface technique to a product design required the development of five different prototypes
which involved user interface design, prototype implementation, and selecting search criterion. While HCILs effort
focused primarily on design and implementation, DJJs attribute selection process was the most time consuming and
difficult task. We also found that a direct link to DJJs workflow was needed in the prototypes to generate the
necessary buy-in. This paper analyzes the interaction between the efforts of HCIL and DJJ and the amount of buy-in by DJJ staff and management. Lesson learned are presented for developers.
+PAGE+

In: Multimedia Systems, Volume 2, Number 6, (January 1995) pages 267-279.
An Empirical Study of Delay Jitter Management Policies *
Donald L. Stone Kevin Jeffay
University of North Carolina at Chapel Hill
Department of Computer Science
Chapel Hill, NC 27599-3175 USA
stone, jeffay-@cs.unc.edu
July 1994
Abstract: This paper presents an empirical study of several policies for managing the effect
of delay jitter on the playout of audio and video in computer-based conferences. The problem
addressed is that of managing the fundamental tradeoff between display with low latency and
display with few gaps. We describe a particular policy called queue monitoring which
observes delay jitter over time and dynamically adjusts display latency in order to support
low-latency conferences with an acceptable gap rate. Queue monitoring is evaluated by
comparing it with two policies from the literature in a study based on measurements from a
computer-based conferencing system. Our results show that queue monitoring performs as
well or better than the other policies over the range of observed network loads. More
importantly, we show that queue monitoring performs better on those network loads for
which the other policies exhibit poor performance.
1. Introduction

Real-Time Computing with Lock-Free Shared Objects
James H. Anderson Srikanth Ramamurthy Kevin Jeffay
Department of Computer Science, University of North Carolina,   Chapel Hill, NC 27599-3175
Abstract
This paper considers the use of lock-free shared objects
within hard real-time systems. As the name suggests,
lock-free shared objects are distinguished by the fact
that they are not locked. As such, they do not give
rise to priority inversions, a key advantage over conventional, lock-based object-sharing approaches. Despite this advantage, it is not immediately apparent
that lock-free shared objects can be employed if tasks
must adhere to strict timing constraints. In particular,
lock-free object implementations permit concurrent operations to interfere with each other, and repeated interferences can cause a given operation to take an arbitrarily long time to complete.
The main contribution of this paper is to show that
such interferences can be bounded by judicious scheduling. This work pertains to periodic, hard real-time
tasks that share lock-free objects on a uniprocessor. In
the first part of the paper, scheduling conditions are derived for such tasks, for both static and dynamic priority schemes. Based on these conditions, it is formally
shown that lock-free object-sharing approaches can be
expected to incur much less overhead than approaches
based on wait-free objects or lock-based schemes. In
the last part of the paper, this conclusion is validated experimentally through work involving a real-time desktop videoconferencing system.
1 Introduction

Sync: A System for Mobile Collaborative Applications
Jonathan P. Munson and Prasun Dewan
Department of Computer Science, University of North Carolina at Chapel Hill
March 14, 1997
ABSTRACT
Sync is a new Java-based framework for developing collaborative applications for wireless mobile
systems. Sync is based on objectoriented replication and offers high-level synchronization-aware classes
based on existing Java classes. Programmers may also extend the Sync-provided classes to create new
replicated classes, either to add functionality or to modify a classs merge policy. Sync supports fully
disconnected operation and employs centralized, asynchronous synchronization. Application programmers
use the Sync framework to define conflicts and specify conflict resolution on the basis of the applications
structure and semantics.
We discuss the general needs of wireless mobile applications, and present a high-function example
application that would be useful to mobile users, to be used for illustration throughout the paper. Next we
discuss related work, and evaluate each work relative to its ability to support the example application. We
then present the Sync framework, motivating each feature with its use in the example application.
INTRODUCTION

Pages 61 to 70 of W. Daelemans, A. van den Bosch, and A. Weijters (Editors),
Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural
Language Processing Tasks, April 26, 1997, Prague, Czech Republic
Automatic Phonetic Transcription of Words
Based On Sparse Data
Maria Wolters (i) and Antal van den Bosch (ii)
(i) Institut fur Kommunikationsforschung und Phonetik, Universitat Bonn
Poppelsdorfer Allee 47, 53113 Bonn, Germany
mwo@asl1.ikp.uni-bonn.de
(ii) Department of Computer Science, Universiteit Maastricht
PO Box 616, 6200 MD Maastricht, The Netherlands
antal@cs.unimaas.nl
Abstract
The relation between the orthography and the phonology of a language has
traditionally been modelled by hand-crafted rule sets. Machine-learning (ML)
approaches offer a means to gather this knowledge automatically. Problems
arise when the training material is sparse. Generalising from sparse data
is a well-known problem for many ML algorithms. We present experiments
in which connectionist, instance-based, and decision-tree learning algorithms
are applied to a small corpus of Scottish Gaelic. instance-based learning in the
ib1-ig algorithm yields the best generalisation performance, and that most
algorithms tested perform tolerably well. Given the availability of a lexicon,
even if it is sparse, ML is a valuable and efficient tool for automatic phonetic
transcription of written text.
1 The Problem

Strange Bedfellows: Issues in Object Naming Under Unix
Douglas B. Orr, Robert W. Mecklenburg and Ravindra Kuramkote
Department of Computer Science
University of Utah
Salt Lake City, UT 84112 USA
E-mail: dbo@cs.utah.edu, mecklen@cs.utah.edu, kuramkot@cs.utah.edu
Abstract
Naming plays a key role in the design of any system that exports services or resources. Object systems
may export many different categories of names: instances, components of records, types, etc. Operating
systems export the names of files, devices, and services. Integrating an object base with existing operating system facilities can improve accessibility of the
object base resources. We consider the benefits and
pitfalls of integrating an object base namespace with
the Unix namespace. 1
1 Introduction

Persistence is Hard, Then You Die!
or
Compiler and Runtime Support for a Persistent
Common Lisp
J. H. Jacobs
M. R. Swanson
R. R. Kessler
UUCS-94-003
Department of Computer Science
University of Utah
Salt Lake City, UT 84112 USA
January 26, 1994
Abstract
Integrating persistence into an existing programming language is a serious undertaking. Preserving the essence of the existing language, adequately supporting persistence, and maintaining efficiency require low-level support from the compiler and runtime systems. Pervasive,
low-level changes were made to a Lisp compiler and runtime system to introduce persistence.
The result is an efficient language which is worthy of the name Persistent Lisp. 1
1  This research was sponsored by the Advanced Research Projects Agency (DOD), monitored by the

Testing the FM9001 Microprocessor
Kenneth L. Albin, Bishop C. Brock,
Warren A. Hunt, Jr., Lawrence M. Smith
Technical Report 90   January 6, 1995
Computational Logic, Inc.
1717 West Sixth Street, Suite 290
Austin, Texas 78703-4776
TEL: +1 512 322 9951
EMAIL: hunt@cli.com
This work was supported in part at Computational Logic, Inc. and by the
Defense Advanced Research Projects Agency, ARPA Orders 6082 and 9151. The
views and conclusions contained in this document are those of the author(s) and
should not be interpreted as representing the official policies, either expressed or
implied, of Computational Logic, Inc.
Copyright c 1995 Computational Logic, Inc.
+PAGE+

On Automatically Generating and Using
Examples in a Computational Logic System
@shortTitle(Generating and Using Examples )
@authorbox(Myung Won Kim)
@reportnumbox(Technical Report #57   March 1987)
The contents of this technical report originally
appeared as the author's dissertation.
+PAGE+

Secure Group Communications Using Key Graphs
Chung Kei Wong Mohamed Gouda Simon S. Lam
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712-1188
fckwong,gouda,lamg@cs.utexas.edu
Abstract
Many emerging applications (e.g., teleconference, real-time
information services, pay per view, distributed interactive
simulation, and collaborative work) are based upon a group
communications model, i.e., they require packet delivery
from one or more authorized senders to a very large number
of authorized receivers. As a result, securing group communications (i.e., providing confidentiality, integrity, and authenticity of messages delivered between group members)
will become a critical networking issue.
In this paper, we present a novel solution to the scalability problem of group/multicast key management. We
formalize the notion of a secure group as a triple (U; K; R)
where U denotes a set of users, K a set of keys held by the
users, and R a user-key relation. We then introduce key
graphs to specify secure groups. For a special class of key
graphs, we present three strategies for securely distributing rekey messages after a join/leave, and specify protocols
for joining and leaving a secure group. The rekeying strategies and join/leave protocols are implemented in a prototype
group key server we have built. We present measurement
results from experiments and discuss performance comparisons. We show that our group key management service, using any of the three rekeying strategies, is scalable to large
groups with frequent joins and leaves. In particular, the
average measured processing time per join/leave increases
linearly with the logarithm of group size.
1 Introduction

Integrating Explanation-Based and Inductive Learning Techniques
to Acquire Search-Control for Planning
Tara A. Estlin
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712
estlin@cs.utexas.edu
Technical Report AI96-250
September 1996
Abstract
Planning systems have become an important tool for automating a wide variety
of tasks. Control knowledge guides a planner to find solutions quickly and is crucial
for efficient planning in most domains. Machine learning techniques enable a planning
system to automatically acquire domain-specific search-control knowledge for different
applications. Past approaches to learning control information have usually employed
explanation-based learning (EBL) to generate control rules. Unfortunately, EBL alone
often produces overly complex rules that actually decrease rather than improve overall
planning efficiency. This paper presents a novel learning approach for control knowledge
acquisition that integrates explanation-based learning with techniques from inductive
logic programming. In our learning system Scope, EBL is used to constrain an inductive search for control heuristics that help a planner choose between competing plan
refinements. Scope is one of the few systems to address learning control information for newer, partial-order planners. Specifically, this proposal describes how Scope
learns domain-specific control rules for the UCPOP planning algorithm. The resulting
system is shown to produce significant speedup in two different planning domains, and
to be more effective than a pure EBL approach. Future research will be performed
in three main areas. First, Scope's learning algorithm will be extended to include
additional techniques such as constructive induction and rule utility analysis. Second,
Scope will be more thoroughly tested; several real-world planning domains have been
identified as possible testbeds, and more in-depth comparisons will be drawn between
Scope and other competing approaches. Third, Scope will be implemented in a different planning system in order to test its portability to other planning algorithms. This
work should demonstrate that machine-learning techniques can be a powerful tool in
the quest for tractable real-world planning.
This research was supported by the NASA Graduate Student Researchers Program, grant number NGT
51332.
+PAGE+

Products of Domain Models
Don Batory
Department of Computer Sciences
The University of Texas
Austin, Texas 78712
batory@cs.utexas.edu
Abstract
We argue that domain models should produce four basic products: identification of reusable software components, definition of software architectures that explain how components can be composed, a demonstration of architecture scalability, and a direct relationship of these results to
software generation of target systems.
1 Introduction

A Family of 2-process Mutual Exclusion Algorithms
Notes on UNITY: 13-90
Jayadev Misra
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712
(512) 471-9547
misra@cs.utexas.edu
March 7, 1994
1 Introduction

Verifying adder circuits using powerlists
William Adams
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712-1188
USA
e-mail: will@cs.utexas.edu
March 29, 1994
Abstract
We define the ripple-carry and the carry-lookahead adder circuits in the
powerlist notation and we use the powerlist algebra to prove that these
circuits correctly implement addition for natural numbers represented as
bit vectors.
0 Introduction

Loop Optimizations for Acyclic Object-Oriented Queries
Vasilis Samoladas Daniel P. Miranker
The University of Texas at Austin
Department of Computer Sciences
Taylor Hall 2.124
Austin, TX 78712-1188
fvsam,mirankerg@cs.utexas.edu
Tel: (512)-471-9541
Abstract
Nested loop execution of object-oriented queries retains
the promise of maintaining the full generality of the object paradigm, independent of the specifics of any single
object model. Thus, from this starting point we have
developed an object-oriented query optimizer and execution engine. The methods, developed to date for only
acyclic queries, augment nested loops structures with a
simple marking mechanism such that unnecessary loop
iterations are not repeated. In the case of acyclic queries,
the executions are asymptotically optimal. In contrast to
optimal query methods based on semijoin reductions our
method involves no preprocessing step and thus avoids
the extra I/O associated with semijoins and prevents the
formal benefits of semijoin reduction from appearing as a
practical improvement. Empirical results comparing our
query environment with a commercially available product
demonstrate significant performance improvement.
1 Introduction

Algorithms for Fence Design
Robert-Paul Berretty   University of Utrecht,    Utrecht, The Netherlands
Ken Goldberg   University of California at Berkeley,   CA, USA
Mark H. Overmars   University of Utrecht,    Utrecht, The Netherlands
A. Frank van der Stappen   University of Utrecht,   Utrecht, The Netherlands
Abstract
A common task in automated manufacturing processes is to orient parts prior to
assembly. We address sensorless orientation of a polygonal part on a conveyor belt by
a sequence of stationary fences across this belt. Since fences can only push against the
motion of the belt, it is a challenging problem to compute fence designs which orients
a given part. In this paper, we give several polynomial-time, algorithms to compute
fence designs which are optimal with respect to various criteria. We address both
frictionless and frictional fences. We also compute modular fence designs in which
the fence angles are restricted to a discrete set of angles instead of an interval.
1 Introduction

To Appear: Proc. IEEE ICC'96 Conference, Dallas, June 1996.
A Bandwidth Control Scheme for Connectionless ATM
Traffic with Multiple Traffic Classes
Jorg Liebeherr Ian F. Akyildiz Debapriya Sarkar ?
Computer Science Department, University of Virginia,   Charlottesville, VA 22903.
School of ECE, Georgia Institute of Technology,   Atlanta, GA 30332.
? Hughes Network Systems, Hughes Network Systems,   Germantown, MD 20876.
Abstract
A bandwidth control mechanism is proposed for ATM
networks that can control the usage of bandwidth in the
presence of both connection-oriented and connection-less traffic, as well as multiple classes of connectionless
traffic. The bandwidth control mechanism operates at
three levels. At the topmost level, bandwidth is dynamically regulated between connection-oriented and
connectionless traffic based on the utilization of each
traffic type. At the next level, bandwidth is controlled
between different classes of connectionless traffic, such
as real-time traffic, bulk data traffic, and so on. At the
lowest level, bandwidth is distributed among flows belonging to the same connectionless traffic class.
1 Introduction

Software Engineering
Beginning In
The First Computer Science Course 1
Jane C. Prey James P. Cohoon Greg Fife
Department of Computer Science
School of Engineering and Applied Sciences
University of Virginia
Charlottesville, VA 22903
Abstract. The demand for computing and computing power is increasing at a rapid pace. With this demand,
the ability to develop, enhance and maintain software is a top priority. Educating students to do competent
work in software development, enhancement and maintenance has become a complex problem. Software
engineering concepts are typically not introduced in beginning computer science courses. Students do not see
software engineering until the third or fourth year of the curriculum. We do not believe students can acquire
an adequate software engineering foundation with the present approach. We believe an emphasis on software
engineering should begin in the very first course and continue throughout the curriculum. We are redesigning
our curriculum to reect this. The first course of the new curriculum is complete. This article focuses on two
of the laboratory activities we have developed which deal with specific software engineering concepts.
Introduction

Isotach Networks
Paul F. Reynolds, Jr.
Craig Williams
Raymond R. Wagner, Jr.
Abstract We introduce a class of networks called isotach networks designed to reduce the
cost of concurrency control in asynchronous computations. Isotach networks support several
properties important to the correct execution of parallel and distributed computations: atomi-city, causal message delivery, sequential consistency, and memory coherence in systems in
which shared data can replicate and migrate. They allow processes to execute atomic actions
without locks and to pipeline memory accesses without sacrificing sequential consistency. Iso-tach networks can be implemented in a wide variety of configurations, including NUMA (nonuniform memory access) multiprocessors and distributed as well as parallel systems. Networks
that implement isotach time systems are characterized not by their topology, but by the guarantees they make about the relative order in which messages appear to be delivered. These
guarantees are expressed in logical time, not physical time. Physical time guarantees would be
prohibitively expensive, whereas logical time guarantees can be enforced cheaply, using purely
local knowledge, and yet are powerful enough to support efficient techniques for coordinating
asynchronously executing processes. Empirical and analytic studies of isotach systems show
that they outperform conventional systems under realistic workloads, in some cases by an order
of magnitude or more.
1. INTRODUCTION

Shade: A Fast Instruction-Set Simulator
for Execution Profiling
Bob Cmelik
Sun Microsystems, Inc.
rfc@eng.sun.com
David Keppel
University of Washington
pardo@cs.washington.edu
Abstract
Tracing tools are used widely to help analyze, design, and tune
both hardware and software systems. This paper describes a tool
called Shade which combines efficient instruction-set simulation
with a flexible, extensible trace generation capability. Efficiency
is achieved by dynamically compiling and caching code to simulate and trace the application program. The user may control the
extent of tracing in a variety of ways; arbitrarily detailed application state information may be collected during the simulation, but
tracing less translates directly into greater efficiency. Current
Shade implementations run on SPARC systems and simulate the
SPARC (Versions 8 and 9) and MIPS I instruction sets. This
paper describes the capabilities, design, implementation, and performance of Shade, and discusses instruction set emulation in
general.
1. Introduction

Appears in "Proceedings of the First Symposium on Operating Systems Design and Implementation," Usenix Association, November 1994.
Dynamic Page Mapping Policies for Cache Conflict Resolution on
Standard Hardware
Theodore H. Romer
Dennis Lee
Brian N. Bershad
Department of Computer Science
and Engineering
University of Washington
Seattle, WA 98195
fromer,dlee,bershadg@cs.washington.edu
J. Bradley Chen
Division of Applied Sciences
29 Oxford Street
Harvard University
Cambridge MA 02138
bchen@das.harvard.edu
Abstract
In computer systems with large, physically-indexed,
direct-mapped caches, a poor mapping from virtual to
physical pages causes excessive cache conflict misses.
In a previous paper we proposed a simple hardware device, the Cache Miss Lookaside (CML) Buffer, which
identifies pages that are suffering from conflict misses.
The operating system can use this information to implement a dynamic page mapping policy that resolves
conflicts by performing an in-memory copy of one of
the conflicting pages, and updating the virtual to physical mappings. In this paper, we propose several dynamic page mapping policies that detect and resolve
cache conflicts using hardware available in existing systems, such as a TLB and cache miss counter, to locate
possible cache conflicts. We evaluate the simulated
performance of a variety of mapping policies, and show
that a dynamic page mapping policy using standard
hardware can improve upon the performance of a static
policy, but is not as effective as special-purpose hardware such as an associative cache or a CML buffer.
We also describe the implementation and performance
of a software-based dynamic policy on a DEC Alpha
workstation running DEC OSF/1.
1 Introduction

A Portable Parallel N-body Solver
E Christopher Lewis Calvin Lin Lawrence Snyder George Turkiyyah
Abstract
We present parallel solutions for direct and fast n-body solvers written in the ZPL
language. We describe the direct solver, compare its performance against a sequential
C program, and show performance results for two very different parallel machines: the
KSR-2 and the Paragon. We also discuss the implementation of the fast solver in ZPL,
including factors pertinent to data movement.
1 Introduction

Timing Analysis of Concurrent Systems
An Algorithm for Determining Time Separation of Events
Tod Amon, Henrik Hulgaard, Gaetano Borriello, Steve Burns
Department of Computer Science and Engineering, FR-35
University of Washington
Seattle, WA 98195 USA
Abstract
A fundamental problem in the synthesis and optimization of concurrent systems is the determination of the separation time between system events. We present a theoretical framework
for solving this problem for arbitrary process graphs without conditional behavior and develop an efficient and exact algorithm based on this theoretical foundation. Examples are
used to demonstrate the operation and generality of the algorithm.
1  Introduction

User-Level Threads and Interprocess Communication
Michael J. Feeley, Jeffrey S. Chase, and Edward D. Lazowska
Department of Computer Science and Engineering, FR-35
University of Washington
Seattle, WA 98195
Technical Report 93-02-03
Abstract
User-level threads have performance and flexibility advantages over both Unix-like processes
and kernel threads. However, the performance of user-level threads may suffer in multipro-grammed environments, or when threads block in the kernel (e.g., for I/O). These problems
can be particularly severe in tasks that communicate frequently using IPC (e.g., multithreaded
servers), due to interactions between the user-level thread scheduler and the operating system
IPC primitives. Efficient IPC typically involves processor handoff that blocks the caller and
unblocks a thread in the callee; when combined with user-level threads, this can cause problems
for both caller and callee, particularly if the caller thread should subsequently block.
In this paper we describe a new user-level thread package, called OThreads, designed to
support blocking and efficient IPC for a system based on traditional kernel threads. We discuss
the extent to which these problems can be solved at the user level without kernel changes
such as scheduler activations. Our conclusion is that problems caused by application-controlled
blocking and IPC can be resolved in the user-level thread package, but that problems due
to multiprogramming workload and unanticipated blocking such as page faults require kernel
changes such as scheduler activations.
1 Introduction

Utility Models for Goal-Directed
Decision-Theoretic Planners
Peter Haddawy 1 , Steve Hanks
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
Technical Report 93-06-04
June 15, 1993
1  Department of EE & CS, University of Wisconsin-Milwaukee,   Milwaukee WI 53201
+PAGE+

Interface Timing Verification with
Combined Max and Linear Constraints
Elizabeth Walkup, Gaetano Borriello
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
Technical Report 94-03-04
June 3, 1994
+PAGE+

Hierarchical Image Caching for Accelerated
Walkthroughs of Complex Environments
Jonathan Shade Dani Lischinski David H. Salesin
Tony DeRose John Snyder
Department of Computer Science and Engineering
University of Washington
Microsoft Research
Technical Report UW-CSE-96-01-06
January 1996
Abstract
We present a new method for accelerating walkthroughs of geometrically complex static
scenes. As a preprocessing step, our method constructs a BSP-tree that hierarchically partitions the geometric primitives in the scene. In the course of a walkthrough, images of nodes
at various levels of the hierarchy are cached for reuse in subsequent frames. A cached image is applied as a texture map to a single quadrilateral that is drawn instead of the geometry
contained in the corresponding node. Visual artifacts are kept under control by using an error metric that quantifies the discrepancy between the appearance of the geometry contained
in a node and the cached image. The new method is shown to achieve significant speedups
for a walkthrough of a complex outdoor scene, with little or no loss in rendering quality.
+PAGE+

Fast Rendering of Subdivision Surfaces
Kari Pulli
University of Washington
Seattle, WA
Mark Segal
Silicon Graphics Inc.
Abstract
Subdivision surfaces provide a curved surface representation that is useful in a number of applications, including modeling surfaces of arbitrary topological type [5] , fitting scattered data [6] , and geometric compression
and automatic level-of-detail generation using wavelets [8] . Subdivision surfaces also provide an attractive representation for fast rendering, since they can directly represent complex surfaces of arbitrary topology. This direct
representation contrasts with traditional approaches such as trimmed NURBS, in which tesselating trim regions
dominates rendering time, and algebraic implicit surfaces, in which rendering requires resultants, root finders, or
other computationally expensive techniques.
We present a method for subdivision surface triangulation that is fast, uses minimum memory, and is simpler in
structure than a naive rendering method based on direct subdivision. These features make the algorithm amenable
to implementation on dedicated geometry engine processors, allowing high rendering performance on appropri
ately equipped graphics hardware.
CR Categories and Subject Descriptors: I.3.6 [Computer Graphics]: Methodology and Techniques.
Additional Key Words: subdivision surfaces, surface rendering.
1 Introduction

Emerging Opportunities for Theoretical Computer Science
Alfred V. Aho
Columbia University
David S. Johnson
AT & T Research
Richard M. Karp (Chair)
University of Washington
S. Rao Kosaraju
Johns Hopkins University
Catherine C. McGeoch
Amherst College
Christos H. Papadimitriou
University of California at Berkeley
Pavel Pevzner
University of Southern California
October 15, 1996
Abstract
The principles underlying this report can be summarized as follows:
1. A strong theoretical foundation is vital to computer science.

Random Striping for
News on Demand Servers
Juan Alemany and Jayram S. Thathachar
Technical Report UW-CSE-97-02-02
February, 1997
Department of Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195
+PAGE+

The Cassowary Linear Arithmetic Constraint Solving
Algorithm: Interface and Implementation
Greg J. Badros Alan Borning
Technical Report UW-CSE-98-06-04
Department of Computer Science and Engineering
University of Washington
Box 352350, Seattle, WA 98195-2350 USA
fgjb,borningg@cs.washington.edu
29 June 1998
Abstract
Linear equality and inequality constraints arise naturally in specifying many aspects of user
interfaces, such as requiring that one window be to the left of another, requiring that a pane
occupy the leftmost 1/3 of a window, or preferring that an object be contained within a rectangle if possible. Current constraint solvers designed for UI applications cannot efficiently
handle simultaneous linear equations and inequalities. This is a major limitation. We describe
Cassowary|an incremental algorithm based on the dual simplex method that can solve such
systems of constraints efficiently.
This informal technical report describes the latest version of the Cassowary algorithm. It is
derived from the paper "Solving Linear Arithmetic Constraints for User Interface Applications"
by Alan Borning, Kim Marriott, Peter Stuckey, and Yi Xiao [7], published in the UIST'97
Proceedings. The UIST paper also contains a description of QOCA, a closely related solver
that finds least-squares solutions to linear constraints. This technical report, which is intended
to be self-contained, includes material on Cassowary from the UIST paper, plus a description
of the Java, C++, and Smalltalk implementations and their interfaces. along with additional
details, corrections, and clarifications.
An earlier technical report also discussed QOCA and the similarities between Cassowary and
that algorithm [6].
1 Introduction

The Error in Polynomial Tensor-Product,
and Chung-Yao, Interpolation
Carl de Boor
Abstract. A formula for the error in Chung-Yao interpolation announced earlier is proved (by induction). In the process, a bivariate divided difference identity of independent interest is proved. Also, an inductive proof of an error formula for polynomial interpolation by tensor-products is given. The main tool is a (convenient notation for a) multi-variate divided difference.
Surface Fitting and Multiresolution Methods 35
A. Le Mehaute, C. Rabut, and L. L. Schumaker (eds.), pp. 35-50.
Copyright o c 1997 by Vanderbilt University Press, Nashville, TN.
ISBN 0-8265-1294-1.
All rights of reproduction in any form reserved.
+PAGE+

On multivariate polynomial interpolation
Carl de Boor 1 & Amos Ron
ABSTRACT
We provide a map fi 7! fi which associates each finite set fi of points in C s with a polynomial
space fi from which interpolation to arbitrary data given at the points in fi is possible and uniquely
so. Among all polynomial spaces Q from which interpolation at fi is uniquely possible, our fi
is of smallest degree. It is also D- and scale-invariant. Our map is monotone, thus providing a
Newton form for the resulting interpolant. Our map is also continuous within reason, allowing us to
interpret certain cases of coalescence as Hermite interpolation. In fact, our map can be extended to
the case where, with each 2 fi, there is associated a polynomial space P , and, for given smooth
f , a polynomial q 2 Q is sought for which
p(D)(f q)() = 0; all p 2 P ; 2 fi:
We obtain fi as the "scaled limit at the origin" (Exp fi ) # of the exponential space Exp fi with
frequencies fi, and base our results on a study of the map H 7! H # defined on subspaces H of
the space of functions analytic at the origin. This study also allows us to determine the local
approximation order from such H and provides an algorithm for the construction of H # from any
basis for H.
AMS (MOS) Subject Classifications: primary 41A05, 41A63, 41A10; secondary 65D05, 41A30
Key Words: exponentials, polynomials, multivariate, interpolation, Newton form, Birkhoff interpolation
Authors' affiliation and address:
Center for Mathematical Sciences
University of Wisconsin-Madison
610 Walnut St.
Madison WI 53705
1 supported in part by the National Science Foundation under Grant No. DMS-8701275 and by
the United States Army under Contract No. DAAL03-87-K-0030
+PAGE+

Recovering Shape by Purposive Viewpoint Adjustment
Kiriakos N. Kutulakos Charles R. Dyer
Computer Sciences Department
University of Wisconsin
Madison, Wisconsin 53706
Technical Report #1035
August 1991
Abstract
We present an approach for recovering surface shape from the occluding contour using an active (i.e., moving) observer. It is based on a relation between the geometries of
a surface in a scene and its occluding contour: If the viewing direction of the observer
is along a principal direction for a surface point whose projection is on the contour,
surface shape (i.e., curvature) at the surface point can be recovered from the contour.
Unlike previous approaches for recovering shape from the occluding contour, we use an
observer that purposefully changes viewpoint in order to achieve a well-defined geometric relationship with respect to a 3D shape prior to its recognition. We show that there
is a simple and efficient viewing strategy that allows the observer to align their viewing
direction with one of the two principal directions for a point on the surface. This strategy depends on only curvature measurements on the occluding contour and therefore
demonstrates that recovering quantitative shape information from the contour does not
require knowledge of the velocities or accelerations of the observer. Experimental results
demonstrate that our method can be easily implemented and can provide reliable shape
information from the occluding contour.
The support of the National Science Foundation under Grant No. IRI-9002582 is gratefully acknowledged.
+PAGE+

Appears in Working Notes, Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms
Workshop, Thirteenth National Conference on Artificial Intelligence, Portland, OR: AAAI Press (1996).
Human Expert-Level Performance on a Scientific Image Analysis Task
by a System Using Combined Artificial Neural Networks
Kevin J. Cherkauer
Department of Computer Sciences
University of Wisconsin-Madison
1210 West Dayton Street
Madison, WI 53706, USA
cherkauer@cs.wisc.edu
Abstract
This paper presents the Plannett system, which
combines artificial neural networks to achieve expert-
level accuracy on the difficult scientific task of recognizing volcanos in radar images of the surface of the
planet Venus. Plannett uses ANNs that vary along
two dimensions: the set of input features used to train
and the number of hidden units. The ANNs are combined simply by averaging their output activations.
When Plannett is used as the classification module
of a three-stage image analysis system called JAR-
tool, the end-to-end accuracy (sensitivity and specificity) is as good as that of a human planetary geologist on a four-image test suite. JARtool-Plannett
also achieves the best algorithmic accuracy on these
images to date.
Introduction

Team Learning of Formal Languages
Sanjay Jain
Dept. of Info. Systems & Computer Science
National University of Singapore
Singapore 0511, Republic of Singapore
sanjay@iscs.nus.sg
Arun Sharma
School of Computer Science and Engineering
The University of New South Wales
Sydney, NSW 2052, Australia
arun@cse.unsw.edu.au
Abstract
A team of learning machines is a multiset of
learning machines. A team is said to successfully learn a concept just in case each member
of some nonempty subset, of predetermined
size, of the team learns the concept.
Team learning of computer programs for
computable functions from their graphs has
been studied extensively. However, team
learning of languages turns out to be a
more suitable theoretical model for studying
computational limits on multi-agent machine
learning. The main reason for this is that
language learning can model both learning
from positive data and learning from positive
and negative data, whereas function learning
models only learning from positive and negative data.
Some theoretical results about learnability of
formal languages by teams of algorithmic machines are surveyed. Some new results about
restricted classes of languages are presented.
These results are mainly about two issues: redundancy and aggregation. The issue of redundancy deals with the impact of increasing
the size of a team and increasing the number
of machines required to be successful. The
issue of aggregation deals with conditions under which a team may be replaced by a single
machine without any loss in learning ability.
Scenarios which can be modeled by team
learning are also presented.
1 INTRODUCTION

Mathematical Programming in Machine Learning
O. L. Mangasarian
Mathematical Programming Technical Report 95-06
April 1995 - Revised July 1995
Abstract
We describe in this work a number of central problems of machine learning and
show how they can be modeled and solved as mathematical programs of various
complexity.
1 Introduction

Clustering via Concave Minimization
P. S. Bradley and O. L. Mangasarian W. N. Street
Computer Sciences Department Computer Science Department
University of Wisconsin Oklahoma State University
1210 West Dayton Street 205 Mathematical Sciences
Madison, WI 53706 Stillwater, OK 74078
email: paulb@cs.wisc.edu, olvi@cs.wisc.edu email:nstreet@cs.okstate.edu
Abstract
The problem of assigning m points in the n-dimensional real space
R n to k clusters is formulated as that of determining k centers in
R n such that the sum of distances of each point to the nearest
center is minimized. If a polyhedral distance is used, the problem
can be formulated as that of minimizing a piecewise-linear concave
function on a polyhedral set which is shown to be equivalent to
a bilinear program: minimizing a bilinear function on a polyhedral set. A fast finite k-Median Algorithm consisting of solving
few linear programs in closed form leads to a stationary point of
the bilinear program. Computational testing on a number of real-world databases was carried out. On the Wisconsin Diagnostic
Breast Cancer (WDBC) database, k-Median training set correctness was comparable to that of the k-Mean Algorithm, however its
testing set correctness was better. Additionally, on the Wisconsin
Prognostic Breast Cancer (WPBC) database, distinct and clinically important survival curves were extracted by the k-Median
Algorithm, whereas the k-Mean Algorithm failed to obtain such
distinct survival curves for the same database.
1 Introduction

MATHEMATICAL PROGRAMMING
APPROACHES TO MACHINE LEARNING
AND DATA MINING
By
Paul S. Bradley
A dissertation submitted in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy
(Computer Sciences)
at the
UNIVERSITY OF WISCONSIN - MADISON
1998
+PAGE+

High-Bandwidth Address Translation
for Multiple-Issue Processors
Todd M. Austin Gurindar S. Sohi
Computer Sciences Department
University of Wisconsin-Madison
1210 W. Dayton Street
Madison, WI 53706
faustin,sohig@cs.wisc.edu
Abstract
In an effort to push the envelope of system performance, microprocessor designs are continually exploiting higher levels of
instruction-level parallelism, resulting in increasing bandwidth demands on the address translation mechanism. Most current microprocessor designs meet this demand with a multi-ported TLB. While
this design provides an excellent hit rate at each port, its access latency and area grow very quickly as the number of ports is increased.
As bandwidth demands continue to increase, multi-ported designs
will soon impact memory access latency.
We present four high-bandwidth address translation mechanisms
with latency and area characteristics that scale better than a multi-ported TLB design. We extend traditional high-bandwidth memory
design techniques to address translation, developing interleaved and
multi-level TLB designs. In addition, we introduce two new designs
crafted specifically for high-bandwidth address translation. Piggyback ports are introduced as a technique to exploit spatial locality in
simultaneous translation requests, allowing accesses to the same virtual memory page to combine their requests at the TLB access port.
Pretranslation is introduced as a technique for attaching translations
to base register values, making it possible to reuse a single translation many times.
We perform extensive simulation-based studies to evaluate our
designs. We vary key system parameters, such as processor model,
page size, and number of architected registers, to see what effects
these changes have on the relative merits of each approach. A number of designs show particular promise. Multi-level TLBs with as
few as eight entries in the upper-level TLB nearly achieve the performance of a TLB with unlimited bandwidth. Piggyback ports
combined with a lesser-ported TLB structure, e.g., an interleaved or
multi-ported TLB, also perform well. Pretranslation over a single-ported TLB performs almost as well as a same-sized multi-level
TLB with the added benefit of decreased access latency for physically indexed caches.
1 Introduction

PARALLEL CONSTRAINT DISTRIBUTION
M. C. FERRIS AND O. L. MANGASARIAN
Abstract. Constraints of a mathematical program are distributed among parallel processors together with an appropriately constructed augmented Lagrangian for each processor, which contains
Lagrangian information on the constraints handled by the other processors. Lagrange multiplier information is then exchanged between processors. Convergence is established under suitable conditions
for strongly convex quadratic programs and for general convex programs.
Key words. Parallel Optimization, Augmented Lagrangians, Quadratic Programs, Convex Programs
1. Introduction. We are concerned with the problem

OPTIMAL PROCESSOR ASSIGNMENT FOR PARALLEL
DATABASE DESIGN
SHAHRAM GHANDEHARIZADEH , ROBERT R. MEYER , GARY L. SCHULTZ AND
JONATHAN YACKEL
Abstract. The computing time benefits of parallelism in database systems (achieved by using multiple processors to execute a query) must be weighed against communication, startup, and
termination overhead costs that increase as a function of the number of processors used. We consider problems of minimizing overhead subject to allocating data among the processors according
to specified loads. We present lower bounds for these combinatorial problems and demonstrate how
processors may be optimally assigned for some problem classes.
1. Introduction. In highly-parallel database machines (e.g., Gamma [2], Bubba

PIECEWISE LINEAR HOMOTOPIES AND
AFFINE VARIATIONAL INEQUALITIES
By
Menglin Cao
A thesis submitted in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy
(Computer Sciences)
at the
UNIVERSITY OF WISCONSIN - MADISON
1994
+PAGE+

1995 Computer Science Department MQP
Review
Robert E. Kinicki
Craig E. Wills
Computer Science Department
Worcester Polytechnic Institute
Worcester, MA 01609
WPI-CS-TR-95-01
August 1, 1995
Abstract
This report presents results of a peer review of MQPs conducted within
the Computer Science Department during the Summer of 1995 as part of a
campus-wide MQP review. The goal of the report is to assess whether the
department MQPs are accomplishing their educational goals. The report
identifies problems that need to be addressed and trends that need to be
continued to make the MQPs a worthwhile learning experience. It reflects
data and evaluations for 27 MQPs, involving 43 computer science students,
that were completed between the Summer of 1994 and the Spring of 1995.
The report also makes comparisons to similar reviews done in 1991 and 1993.
Overall, the large majority of the projects are meeting the educational
goals of the department as good learning experiences. The reviews indicate
the overall quality of the projects is good, about the same as in 1993 and
a little better than 1991. The report draws a number of conclusions about
the success of the projects based upon the data collected and evaluations
done for this review. The report concludes with recommendations for future
projects.
+PAGE+

What Tasks Can Be Performed with an Uncalibrated
Stereo Vision System?
J. P. Hespanha, Z. Dodds, G. D. Hager, and A. S. Morse
Center for Computational Vision and Control
c/o Computer Science Department
P.O. Box 208285
Yale University
New Haven, CT, 06520
Phone: (203) 432-6432
E-mail: (gregory.hager, joao.hespanha, zachary.dodds, as.morse)@yale.edu
Abstract
This article studies the following question: "When is it possible to decide, on the basis of images of point features observed by an imprecisely modeled two-camera stereo
vision system, whether or not a prescribed robot positioning task has been accomplished with precision?" It is shown that for a stereo vision system with known epipo-lar geometry, whether or not such a positioning task has been accomplished can be
decided with available data, just in case the task function which specifies the task is a
projective invariant.
Submitted to IJCV special issue on vision research at Yale.
This research was supported by the National Science Foundation, the Army Research Office, and the
Air Force Office of Scientific Research
+PAGE+

In Proc. 8th IASTED Int'l Conf. on Parallel and Distributed Computing and Systems (Chicago, IL, USA),
c IASTED/ACTA Press (Anaheim/Calgary/Z urich), pp. 144-148, Oct. 1996. [ISBN: 0-88986-213-3]
HPF and MPI Implementation of the NAS Parallel Benchmarks
Supported by Integrated Program Engineering Tools
Christian Cl emencon Karsten M. Decker Vaibhav R. Deshpande
Akiyoshi Endo Josef Fritscher Paulo A. R. Lorenzo
Norio Masuda Andreas Muller Roland R uhl
William Sawyer Brian J. N. Wylie Frank Zimmermann
Centro Svizzero di Calcolo Scientifico (CSCS/SCSC) and
NEC European Supercomputer Systems, Swiss Branch
CH-6928 Manno, Switzerland
http://www.cscs.ch/Official/Project CSCS-NEC.html
Abstract: High Performance Fortran (HPF) compilers
and communication libraries with the standardized Message Passing Interface (MPI) are becoming widely available, easing the development of portable parallel applications on distributed-memory parallel processor systems.
The recently developed Annai tool environment supports
programming, debugging and tuning of both HPF- and
MPI-based applications. Considering code development
and subsequent maintenance time to be as important as ultimate performance, we address how sequential Fortran-77
versions of the familiar NAS Parallel Benchmark kernels
can be expediently parallelized with appropriate tool support. While automatic parallelization of scientific applications written in traditional sequential languages remains
largely impractical, Annai provides users with high-level
language extensions and integrated program engineering
support tools. In this paper, Annai support is demonstrated primarily focusing on the MG (multigrid) kernel,
with complementary examples selected from the other four
kernels. Respectable performance and good scalability
in most cases are obtained with this straightforward par-allelization strategy, even without recourse to platform-specific optimizations or major program transformations.
Keywords: HPF & MPI parallelization; parallel program
engineering tools.
1 Introduction

Explaining Anomalies as a Basis for Knowledge Base Refinement
Neli P. Zlatareva
Department of Computer Science
Central Connecticut State University
New Britain, CT 06050
Abstract
Explanations play a key role in operationalization-based anomaly detection techniques. In this paper
we show that their role is not limited to anomaly detection; they can also be used for guiding automated
knowledge base refinement. We introduce a refinement procedure which takes: (i) a small number of
refinement rules (rather than test cases), and (ii) explanations constructed in an attempt to reveal the
cause (or causes) for inconsistencies detected during
the verification process, and returns rule revisions
aiming to recover the consistency of the KB-theory.
Inconsistencies caused by more than one anomaly
are handled at the same time, which improves the
efficiency of the refinement process.
1 Introduction

Confluent Preorder Parsing
CS-TR-95-03
HO, Kei Shiu Edward and CHAN, Lai Wan
Department of Computer Science
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
email : ho052@cs.cuhk.hk and lwchan@cs.cuhk.hk
KEYWORDS: Neural Networks, Connectionist Syntactic Parsing, RAAM, Holistic Transformation, Confluent
Preorder Parser, Linearization of a Hierarchical Parse Tree, Parsing Erroneous Sentences, Syntactic
Disambiguation
Abstract
In this paper, syntactic parsing is discussed in the context of connectionism. A new model - the Confluent
Preorder Parser (CPP), is proposed which exemplifies the holistic parsing paradigm. Holistic parsing has the
advantage that little assumption has to be made concerning the detailed parsing algorithm, which is often
unknown or debatable, especially when human language understanding is concerned. In the CPP, syntactic
parsing is achieved by transforming in a oneshot manner, from the connectionist representation of the sentence
to the connectionist representation of the preorder traversal of its parse tree, instead of the parse tree itself. As
revealed by the simulation experiments, generalization performance is excellent (as high as 90%). Besides, the
CPP is also capable of parsing erroneous sentences and resolving syntactic ambiguities. A systematic study is
conducted to explore the range of factors which can affect the effectiveness of it. This error-recovery capability
is especially useful in natural language processing when incomplete or even ungrammatical sentences are to be
dealt with.
1. Introduction

Assessing Responses to Situated Cognition
Tim Menzies
Dept. of Artifical Intelligence, School of Computer Science and Engineering,
The University of New South Wales,   Sydney, Australia, 2052
timm@cse.unsw.edu.au;   http://www.cse.unsw.edu.au/~timm
September 17, 1996
Abstract
Situated cognition (SC) claims that knowledge is mostly context-dependent and that symbolic descriptions elicited prior to direct experience are less important than functional units developed via direct
experience with the current problem. If this were true, then we would need to modify the knowledge
modeling approaches of KA which assume that re-using old symbolic descriptions are a productivity tool
for new applications. There are numerous tools which, if added to conventional knowledge modeling,
could be said to handle SC (e.g. machine learning, abduction, verification & validation tools, repertory
grids, certain frameworks for decision support systems, expert critiquing systems, and ripple-down-rules).
However, we require an experiment to assess the effectiveness of these tools as a response to SC.
1 Introduction

DRAFT   June 2, 1996:
Learning stable concepts in domains with hidden changes in context
Michael Harries
Department of Artificial Intelligence
School of Computer Science and Engineering
University of NSW,   Australia
mbh@cse.unsw.edu.au
Kim Horn
Predictive Strategies Unit
Australian Gilt Securities Limited
Australia
kim@ags.com.au
Abstract
This paper presents Splice, a batch meta-learning system, designed to learn locally stable concepts in domains with hidden changes
in context. The majority of machine learning
algorithms assume that target concepts remain stable over time. In many domains this
assumption is invalid. For example, financial prediction, medical diagnosis, and network performance are domains in which target concepts may not remain stable. Unstable target concepts are often due to changes
in a hidden context. Existing works on learning in the presence of hidden changes in con
text use an incremental learning approach.
1 INTRODUCTION

Minimum Encoding Approaches for Predictive Modeling
Peter Grunwald
CWI
Dept. of Algorithms and Architectures
P.O.Box 94079
NL-1090 GB Amsterdam, The Netherlands
http://www.cwi.nl/~pdg/
Petri Kontkanen, Petri Myllymaki,
Tomi Silander, Henry Tirri
Complex Systems Computation Group (CoSCo)
P.O.Box 26, Department of Computer Science
FIN-00014 University of Helsinki, Finland
http://www.cs.Helsinki.FI/research/cosco/
To appear in the Proceedings of the Fourteenth International Conference on Uncertainty in Artificial Intelligence
(Madison, WI, July 1998).
Abstract
We analyze differences between two
information-theoretically motivated approaches to statistical inference and model
selection: the Minimum Description Length
(MDL) principle, and the Minimum Message
Length (MML) principle. Based on this
analysis, we present two revised versions of
MML: a pointwise estimator which gives
the MML-optimal single parameter model,
and a volumewise estimator which gives
the MML-optimal region in the parameter
space. Our empirical results suggest that
with small data sets, the MDL approach
yields more accurate predictions than the
MML estimators. The empirical results
also demonstrate that the revised MML
estimators introduced here perform better
than the original MML estimator suggested
by Wallace and Freeman.
1 INTRODUCTION

A Space of Presentation Emphasis Techniques for Visualizing Graphs
Emanuel G. Noik
Computer Systems Research Institute
University of Toronto
6 King's College Road
Toronto, Ontario, Canada m4s 1a1
e-mail: noik@db.toronto.edu
Telephone: (416) 978 8609
Abstract
The graph topo-visual formalism has been shown to
be well-suited to the task of visualizing complex relations on a set of elements. Unfortunately, most visual
formalisms do not scale very well. This observation is
particularly true of graphs, which even when hand-drawn
by an artist, are seldom meaningful when the number of
nodes or links exceeds a very modest threshold typically only a few hundred elements. This severe limitation
has prompted many researchers to seek alternative visualization techniques that may eliminate, or, at the very
least, raise this threshold.
In this paper we analyze these recent efforts, describe
an abstract space of presentation emphasis techniques,
and locate the current approaches within this space. The
contributions of this paper are several: (1) a significant
portion of recent work is collected and reviewed; (2) a
common set of criteria and a taxonomy of graph views
are proposed; these, (3) permit a more direct comparison
of previous work; which helps to, (4) identify common
shortcomings and limitations; which in turn, (5) suggest
future directions.
Keywords: presentation emphasis techniques, fisheye
views, relational data visualization, graphs, nested
graphs.
1 Introduction

Proving Properties of Logic Programs by
Abstract Diagnosis
Marco Comini 1 , Giorgio Levi 1 , Maria Chiara Meo 2 , and Giuliana Vitiello 3
1 Dipartimento di Informatica, Universita di Pisa,   Corso Italia 40, 56125 Pisa, Italy,

The Semantics of the C Programming Language
Yuri Gurevich and James K. Huggins
EECS Department, University of Michigan,   Ann Arbor, MI 48109-2122, USA
February 19, 1993
This paper first appeared in [GH2], and incorporates the corrections indicated in [GH3].
0 Introduction

An Evolving Algebra Abstract Machine
Giuseppe Del Castillo 1 , Igor D - urd -anovic 2 , Uwe Glasser 1
1 Heinz Nixdorf Institut, Universitat-GH Paderborn,    Furstenallee 11,
33102 Paderborn, Germany,   fgiusp,glaesserg@uni-paderborn.de
2 FB Mathematik-Informatik, Universitat-GH Paderborn,   Warburger Str. 100,
33098 Paderborn, Germany,   igor@uni-paderborn.de
Abstract. Evolving algebras (EAs) as defined by Yuri Gurevich constitute the basis of a powerful and elegant specification and verification
method which has successfully been applied to the design and analysis of
various kinds of discrete dynamic systems. Aiming at the development
of a comprehensive EA-based specification and design environment, we
introduce the concept of an evolving algebra abstract machine (EAM ) as
a platform for the systematic development of EA tools; for instance, as
required for machine based analysis and execution of EA specifications.
We give a formal definition of the EAM ground model in terms of a
universal evolving algebra, where we validate the correctness of the relation between evolving algebras (their theoretical foundations) and their
EAM representation and interpretation. Our approach covers sequential
as well as distributed evolving algebras.
Introduction

Defining the Java Virtual Machine as Platform
for Provably Correct Java Compilation ?
Egon Borger 1 and Wolfram Schulte 2
1 Universita di Pisa, Dipartimento di Informatica,   I-56125 Pisa, Italy
boerger@di.unipi.it
2 Universitat Ulm, Fakultat fur Informatik,   D-89069 Ulm, Germany
wolfram@informatik.uni-ulm.de
Abstract. We provide concise abstract code for running the Java Virtual Machine (JVM) to execute compiled Java programs, and define a
general compilation scheme of Java programs to JVM code. These definitions, together with the definition of an abstract interpreter of Java
programs given in our previous work [3], allow us to prove that any
compiler that satisfies the conditions stated in this paper compiles Java
code correctly. In addition we have validated our JVM and compiler
specification through experimentation.
The modularity of our definitions for Java, the JVM and the compilation
scheme exhibit orthogonal language, machine and compiler components,
which fit together and provide the basis for a stepwise and provably correct design-for-reuse. As a by-product we provide a challenging realistic
case study for mechanical verification of a compiler correctness proof.
1 Introduction

Refining Abstract Machine Specifications of the
Steam Boiler Control to Well Documented
Executable Code
Christoph Beierle, Egon Borger, Igor D - urd -anovic, Uwe Glasser, Elvinia
Riccobene
1 Fernuniversitat-GH Hagen,   Germany,   christoph.beierle@fernuni-hagen.de
2 Universita di Pisa,   Italy,   boerger@di.unipi.it
3 Universitat-GH Paderborn,   Germany,   igor@uni-paderborn.de
4 Universitat-GH Paderborn,   Germany,   glaesser@uni-paderborn.de
5 Universita di Catania,   Italy,   riccobene@dipmat.unict.it
Abstract. We use the steam boiler control specification problem to illustrate how the evolving algebra approach to the specification and the
verification of complex systems can be exploited for a reliable and well
documented development of executable, but formally inspectable and
systematically modifiable code. A hierarchy of stepwise refined abstract
machine models is developed, the ground version of which can be checked
for whether it faithfully reflects the informally given problem. The sequence of machine models yields various abstract views of the system,
making the various design decisions transparent, and leads to a C++
program. This program has been demonstrated during the Dagstuhl-Meeting on Methods for Semantics and Specification, in June 1995, to
control the Karlsruhe steam boiler simulator satisfactorily.
The abstract machines are evolving algebras and thereby have a rigorous
semantical foundation, allowing us to formalize and prove, under precisely stated assumptions, some typical sample properties of the system.
This provides insight into the structure of the system which supports
easily maintainable extensions and modifications of both the abstract
specification and the implementation.
1 Introduction

Reasoning about Other Agents: Philosophy,
Theory, and Implementation.
Piotr J. Gmytrasiewicz and Edmund H. Durfee
Department of Computer Science
Hebrew University,   Jerusalem, Israel
Department of Electrical Engineering and Computer Science
University of Michigan,   Ann Arbor, Michigan 48109
piotr@cs.huji.ac.il, durfee@engin.umich.edu
Abstract
Drawing on on our work in the area of Distributed Artificial Intelligence, we propose the rudiments of a view of multiagent reasoning that relates current philosophical
intuitions, theoretical foundations, and preliminary implementation. The philosophical
position we take is a combination of Daniel Dennett's philosophy of the ladder of per-sonhood (consisting of rationality, intentionality, stance, reciprocity, communication,
and consciousness) on one hand, and the utilitarian philosophy of selfish utility maximization on the other hand. The theories we incorporate are logics of knowledge and
belief, which in addressing the multiagent issues can be developed based on a recursive
version of the Kripke structure, and the related fields of utility, decision and game
theories. Our preliminary implementation, the Recursive Modeling Method (RMM),
lets an agent coordinate its actions with the actions of other agents, cooperate with
them when appropriate, and rationally choose an optimal form of communication with
them.
1 Introduction

Observational Uncertainty in Plan Recognition Among Interacting
Robots
Marcus J. Huber
Edmund H. Durfee
Distributed Intelligent Agents Group (DIAG)
Artificial Intelligence Laboratory
The University of Michigan
Ann Arbor, Michigan 48109-2110
marcush@engin.umich.edu, durfee@engin.umich.edu
May 16, 1994
Abstract
Plan recognition is the process of observing another agent's behavior(s) and inferring what, and
possibly why, the agent is acting as it is. Plan recognition becomes a very important means of acquiring
such information about other agents in situations and domains where explicit communication is either
very costly, dangerous, or impossible. Performing plan recognition in a physical domain (i.e. the real
world) forces the world's ubiquitous uncertainty upon the observing agent because of the necessity to
use real sensors to make the observations. We have developed a multiple resolution, hierarchical plan
recognition system to coordinate the motion of two interacting mobile robots. Uncertainty arises in the
system from dead reckoning errors that accumulate while the robots are moving, as well as by errors
in the computer vision system that is used to detect the other agent's behaviors. Based upon belief
networks, the plan recognition system gracefully degrades in performance as the level of uncertainty
about observations increase.
1 Introduction

Using MICE to Study Intelligent Dynamic Coordination
Edmund H. Durfee and Thomas A. Montgomery
Department of Electrical Engineering and Computer Science
University of Michigan
Ann Arbor, Michigan 48109
(313) 936-1563
durfee@caen.engin.umich.edu
Abstract
We describe a flexible experimental testbed, called MICE, for distributed artificial intelligence
research. We argue that the adoption of MICE (or some other standard testbed) by the distributed
artificial intelligence community can draw together the community and permit a much greater level
of exchange of ideas, formalisms, and techniques. MICE allows an experimenter to specify the
constraints and characteristics of an environment in which agents are simulated to act and interact,
and does not assume any particular implementation of an agent's reasoning architecture. MICE
therefore provides a platform for investigating and evaluating alternative reasoning architectures
and coordination mechanisms in many different simulated environments. We outline the design
of MICE and illustrate its flexibility by describing simulated environments that model the coordination issues in domains such as predators chasing prey, predators attacking each other, agents
fighting a fire, and diverse robots that are working together. In addition, we note that MICE's
ability to simulate multi-agent environments makes it an ideal platform for studying reasoning in
dynamic worlds; we can associate functionality to arbitrary objects in order to trigger changes in
the environment. We conclude by discussing the status of MICE and how we are using MICE in
our current research.
0 This research was sponsored, in part, by the University of Michigan under a Rackham Faculty Research
Grant, and by a Bell Northern Research Postgraduate Award.
+PAGE+

PENALIZED LIKELIHOOD EMISSION IMAGE RECONSTRUCTION
WITH UNCERTAIN BOUNDARY INFORMATION
Stephen R. Titus, Alfred O. Hero III, Jeffrey A. Fessler
4401 EECS, University of Michigan, Ann Arbor, MI 48109
ABSTRACT
In this paper, a method is introduced for incorporating perfectly registered MRI boundary information into
a penalized likelihood emission reconstruction scheme.
The boundary curve is modeled as a periodic spline
whose coefficients are estimated from the MRI image.
The resulting boundary estimate is mapped to a spatially variant set of Gibbs weights. When incorporated into a quadratic roughness penalty, these weights
improve emission reconstruction bias/variance performance by preventing smoothing across the estimated
boundary.
1. INTRODUCTION

Observational Uncertainty in Plan Recognition Among Interacting
Robots
Marcus J. Huber
Edmund H. Durfee
Distributed Intelligent Agents Group (DIAG)
Artificial Intelligence Laboratory
The University of Michigan
Ann Arbor, Michigan 48109-2110
marcush@engin.umich.edu, durfee@engin.umich.edu
May 16, 1994
Abstract
Plan recognition is the process of observing another agent's behavior(s) and inferring what, and
possibly why, the agent is acting as it is. Plan recognition becomes a very important means of acquiring
such information about other agents in situations and domains where explicit communication is either
very costly, dangerous, or impossible. Performing plan recognition in a physical domain (i.e. the real
world) forces the world's ubiquitous uncertainty upon the observing agent because of the necessity to
use real sensors to make the observations. We have developed a multiple resolution, hierarchical plan
recognition system to coordinate the motion of two interacting mobile robots. Uncertainty arises in the
system from dead reckoning errors that accumulate while the robots are moving, as well as by errors
in the computer vision system that is used to detect the other agent's behaviors. Based upon belief
networks, the plan recognition system gracefully degrades in performance as the level of uncertainty
about observations increase.
1 Introduction

Route Guidance Support in Intelligent Transportation
Systems: An Encoded Path View Approach
University of Michigan Technical Report 1995.
Yun-Wu Huangy, Ning Jingz, and Elke A. Rundensteinery
e-mail: [ywh j jning j rundenst] @eecs.umich.edu
Department of Electrical Engineering and Computer Science
University of Michigan,   Ann Arbor, MI 48109
Department of Electrical Engineering
Changsha Institute of Technology,   Changsha, Hunan, P.R. China
Abstract
Efficient path computation necessary for route guidance has been identified as one of the key requirements
for Intelligent Transportation Systems (ITS) applications. While the current ITS literature has focused on
the application of search algorithms (typically, heuristic A* algorithms) to provide for compute-on-demand
path finding, we propose an encoded path view approach that precomputes optimal paths. Advantages of
our approach include (1) route search is efficient and less dependent on system load, (2) alternative paths
are materialized in addition to the optimal paths, simplifying the process of global optimization, (3) the
storage overhead is manageable and less than for the full enumeration of all possible paths. In this paper,
we present algorithms for incrementally updating the encoded path view structure in response to weight
changes on the traffic links of the underlying network. Despite non-optimal paths also being materialized, our
algorithms are designed to operate on cyclic planar graphs | given that ITS maps typically correspond to
highly interconnected grid structures. In this paper, we show that while our approach does not encode all
paths, it omits some non-optimal paths to resolve cycle ambivalence and will recover them once they become
optimal. Proofs of correctness and of complexity are also given. We demonstrate the potential of our approach
by presenting experimental results of evaluating our approach both on randomly generated as well as on real
city map data. Our experiments furthermore compare the proposed approach against more conventional path
searching algorithms, which correspond to the state-of-the-art for route guidance in ITS.
Index Terms | View Materialization, Encoded Path Structure, Route Guidance, Path Retrieval, Map Databases,
Cycle Detection.
This work was supported in part by the University of Michigan ITS Center of Excellence grant (DTFH61-93-X-00017-Sub)
sponsored by the U.S. Department of Transportation and by the Michigan Department of Transportation. Ning Jing, on leave
from the Changsha Institute of Technology, is currently visiting the University of Michigan and likes to thank the State Education
Commission of P.R. China.
+PAGE+

Hierarchical Path Views: A Model Based on Fragmentation and
Transportation Road Types
Yun-Wu Huangy, Ning Jingz, and Elke A. Rundensteinery
e-mail: [ywh j jning j rundenst] @eecs.umich.edu
Dept. of Electrical Engineering and Computer Science, Univ. of Michigan,   Ann Arbor, MI48109
Dept. of Electrical Engineering, Changsha Institute of Technology,   Changsha, Hunan, China
Abstract
Efficient path query processing necessary for route guidance has been identified as one of the key requirements
for Intelligent Transportation Systems (ITS) applications.
While precomputing the view of all shortest paths provides
the most efficient path retrieval, the view maintenance and
storage costs become unrealistic for large ITS networks. Based
on ITS road type classification, we propose a hierarchical
path view approach, in which the path view maintenance
and storage costs are dramatically reduced at the cost of
negligible loss of path optimality. Comparing with the traditional ITS path finding approaches that use A or hierarchical A , our hierarchical approach is superior in three
areas: 1) path search is more efficient, 2) the connecting
point from the low-level roads to the high-level roads and
vice versa are dynamically determined based on the most
recent traffic, 3) within one region, the high-level traffic can
be dynamically rerouted through the low-level roads. In
this paper, we conduct experiments to gain insight into the
performance of our proposed algorithms and model, as well
as to contrast the difference in computational resource requirements between the hierarchical path view and the A
algorithms.
1 INTRODUCTION

Hierarchical Optimization of Optimal Path Finding for
Transportation Applications
Ning Jing
Changsha Institute of Technology
jning@eecs.umich.edu
Yun-Wu Huang
University of Michigan
ywh@eecs.umich.edu
Elke A. Rundensteiner
University of Michigan
rundenst@eecs.umich.edu
Abstract
Efficient path query processing is a key requirement for advanced
database applications including GIS (Geographic Information Systems) and ITS (Intelligent Transportation Systems). We study the
problem in the context of automobile navigation systems where a
large number of path requests can be submitted over the transportation network within a short period of time. To guarantee efficient re-sponsefor path queries, we employ a path view materialization strategy for precomputing the best paths. We tackle the following three
issues: (1) memory-resident solutions quickly exceed current computer storage capacity for networks of thousands of nodes, (2) disk-based solutions have been found inefficient to meet the stringent
performance requirements, and (3) path views become too costly
to update for large graphs. We propose the HEP V (Hierarchical
Encoded Path View) approach that addresses these problems while
guaranteeing the optimality of path retrieval. Our experimental results reveal that HEP V is more efficient than previously known
path finding approaches.
1 Introduction

In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96),
Portland, OR, USA, August 1996
Optimal Factory Scheduling using Stochastic Dominance A*
Peter R. Wurman and Michael P. Wellman
University of Michigan
Artificial Intelligence Laboratory
1101 Beal Avenue
Ann Arbor, MI, 48109-2110
fpwurman, wellmang@umich.edu
Abstract
We examine a standard factory scheduling
problem with stochastic processing and setup
times, minimizing the expectation of the
weighted number of tardy jobs. Because
the costs of operators in the schedule are
stochastic and sequence dependent, standard
dynamic programming algorithms such as
A* may fail to find the optimal schedule.
The SDA* (Stochastic Dominance A*) algorithm remedies this difficulty by relaxing the
pruning condition. We present an improved
state-space search formulation for these problems and discuss the conditions under which
stochastic scheduling problems can be solved
optimally using SDA*. In empirical testing
on randomly generated problems, we found
that in 70%, the expected cost of the optimal stochastic solution is lower than that of
the solution derived using a deterministic ap
proximation, with comparable search effort.
1 INTRODUCTION

Eaton's Markov Chain, its Conjugate Partner
and P-admissibility
James P. Hobert
Department of Statistics
University of Florida
Gainesville, FL 32611
jhobert@stat.ufl.edu
C. P. Robert
Laboratoire de Statistique
CREST, INSEE
75675 Paris cedex 14, France
robert@ensae.fr
August 1997
The first author acknowledges partial support from the Center for Research in Economics and
Statistics (CREST) at the French National Institute of Statistics and Economic Studies (INSEE),
Paris, France.
AMS 1991 subject classifications. Primary 62C15; secondary 60J05
Key words and phrases. Bilinear model, Branching process with immigration, Exponential
family, Improper prior, Null recurrence, Random walk, Stochastic difference equation, Transience
Abbreviated title. Markov Chains and P-admissibility
+PAGE+

Dynamic Generation and Refinement of Concept Hierarchies for
Knowledge Discovery in Databases
Jiawei Han and Yongjian Fu
School of Computing Science
Simon Fraser University
Burnaby, B.C., Canada V5A 1S6
fhan, yongjiang@cs.sfu.ca
Abstract
Concept hierarchies organize data and concepts in hierarchical forms or in certain partial order, which
helps expressing knowledge and data relationships in databases in concise, high level terms, and thus, plays
an important role in knowledge discovery processes. Concept hierarchies could be provided by knowledge
engineers, domain experts or users, or embedded in some data relations. However, it is sometimes desirable to automatically generate some concept hierarchies or adjust some given hierarchies for particular
learning tasks. In this paper, the issues of dynamic generation and refinement of concept hierarchies are
studied. The study leads to some algorithms for automatic generation of concept hierarchies for numerical attributes based on data distributions and for dynamic refinement of a given or generated concept
hierarchy based on a learning request, the relevant set of data and database statistics. These algorithms
have been implemented in the DBLearn knowledge discovery system and tested against large relational
databases. The experimental results show that the algorithms are efficient and effective for knowledge
discovery in large databases.
Keywords: Knowledge discovery in large databases, discovery methods, KDD system implementation, al
gorithms, dynamic generation and refinement of concept hierarchies.
1 Introduction

Multi-Agent Planning as a Dynamic Search for Social Consensus
Eithan Ephrati
Jeffrey S. Rosenschein
Computer Science Department
Hebrew University
Givat Ram, Jerusalem, Israel
Abstract
When autonomous agents attempt to coordinate action, it is often necessary that they reach
some kind of consensus. Reaching consensus
has traditionally been dealt with in the Distributed Artificial Intelligence literature via negotiation. Another alternative is to have agents
use a voting mechanism; each agent expresses
its preferences, and a group choice mechanism
is used to select the result. Some choice mechanisms are better than others, and ideally we
would like one that cannot be manipulated by
untruthful agents.
Coordination of actions by a group of agents
corresponds to a group planning process. We
here introduce a new multi-agent planning
technique, that makes use of a dynamic, iterative search procedure. Through a process of
group constraint aggregation, agents incrementally construct a plan that brings the group to
a state maximizing social welfare. At each step,
agents vote about the next joint action in the
group plan (i.e., what the next transition state
will be in the emerging plan). Using this technique agents need not fully reveal their preferences, and the set of alternative final states
need not be generated in advance of a vote.
With a minor variation, the entire procedure
can be made resistant to untruthful agents.
1 Introduction

SUPPORTING TECHNOLOGY TRANSFER OF FORMAL TECHNICAL REVIEW
THROUGH A COMPUTER SUPPORTED COLLABORATIVE REVIEW SYSTEM
Philip M. Johnson
Department of Information and Computer Sciences
University of Hawaii
Honolulu, HI 96822
(808) 956-3489
johnson@hawaii.edu
Abstract
Formal technical review (FTR) is an essential component of all modern software quality assessment, assurance,
and improvement techniques, and is acknowledged to be
the most cost-effective form of quality improvement when
practiced effectively. However, traditional FTR methods
such as inspection are very difficult to adopt in organizations: they introduce substantial new up-front costs,
training, overhead, and group process obstacles. Sustained commitment from high-level management along
with substantial resources is often necessary for successful
technology transfer of FTR.
Since 1991, we have been designing and evaluating
a series of versions of a system called CSRS: an instrumented, computer-supported cooperative work environment for formal technical review. The current version of
CSRS includes an FTR method definition language, which
allows organizations to design their own FTR method,
and to evolve it over time. This paper describes how our
approach to computer supported FTR can address some
of the issues in technology transfer of FTR.
1 Introduction

Experiences with CLARE: a Computer-Supported
Collaborative Learning Environment
Dadong Wan
Center for Information Technology & Management
Walter A. Haas School of Business
University of California
Berkeley, CA 94720-1900, USA
Philip M. Johnson
Department of Information and Computer Sciences
University of Hawaii
Honolulu, HI 96822, USA
September 2, 1994
Abstract
Current collaborative learning systems focus on maximizing shared information.
However, meaningful learning is not simply information sharing but also knowledge
construction. CLARE is a computer-supported learning environment that facilitates
meaningful learning through collaborative knowledge construction. It provides a semiformal representation language called RESRA and an explicit process model called
SECAI. Experimental evaluation through 300 hours of classroom usage indicates that
CLARE does support meaningful learning. It also shows that a major bottleneck to
computer-mediated knowledge construction is summarization. Lessons learned through
the design and evaluation of CLARE provide new insights into both collaborative learning
systems and collaborative learning theories.
This paper is a revised and expanded version of one appearing in the Proceedings of the 1994 ACM
Conference on Computer Supported Cooperative Work, October 22-26, 1994, Chapel Hill, North Carolina,
U.S.A.
+PAGE+

Combinatory Differential Fields:
An Algebraic Approach to
Approximate Computation and
Constructive Analysis
Karl Aberer
TR-91-061
November 1991
Abstract
The algebraic structure of combinatory differential fields is constructed to provide a semantics for computations in analysis. In this setting programs, approximations, limits and operations of analysis are represented
as algebraic terms. Analytic algorithms can be derived by algebraic methods. The main tool in this construction are combinatory models which are inner algebras of Engeler graph models. As an universal domain
of denotational semantics the lattice structure of the graph models allows to give a striking simple semantics
for computations with approximations. As models of combinatory algebra they provide all essential computational constructs, including recursion. Combinatory models are constructed as extensions of first order
theories. The classical first order theory to describe analysis is the theory of differential fields. It turns out
that two types of computational constructs, namely composition and piecewise definition of functions, are
preferably introduced as extensions of the differential fields theory. Combinatory differential fields are then
the combinatory models of these enriched differential fields. We show for basic algorithms of computational
analysis how their combinatory counterparts are derived in the algebraic setting. We illustrate how these
algorithms are suitable to be implemented in a computer algebra environment like mathematica.
Part of this work was done while the author was at ETH Zurich. Submitted to Journal of Symbolic
Computation.
International Computer Science Institute, Berkeley, CA 94704.   email: aberer@icsi.berkeley.edu.   Supported by Schweizerische Gesellschaft zur Forderung der Informatik und ihrer Anwendungen
+PAGE+

A Study of Perceptually
Grounded Polysemy in a Spatial
Microdomain
Jordan Zlatev
TR-92-048
August 1992
Abstract
This paper attempts to exemplify the advantages of perceptually grounded semantics with respect to traditional formalist approaches in elucidating the nature of the
controversial notion of linguistic polysemy, or multiplicity of meaning. It is also suggested how some aspects of language typically associated with compositionality could
be modeled, without there being a strictly "compositional semantics".
This is done through a series of experiments, using modifications of Terry Regier's
connectionist system for learning spatial relations [Regier, 1992] which constitutes a
part of the L 0 project concerned with associating descriptions in an arbitrary language
with an analog environment, (sequences of) pictures of simple 2-dimensional scenes.
The emphasis is above all on the English preposition `over', famous for its poly-semy, and analyzed in detail by [Brugman, 1981] and [Lakoff, 1987], but some modeling has been also done of the meaning of `under', as well as some rudimentary
semantics for simple verbs such as `be', `go' and `fly' that combine with the two
prepositions.
The author has been supported by a scholarship from The Swedish Institute and may be reached by
e-mail as zlatev@icsi.Berkeley.EDU or jordan@ling.su.se.
+PAGE+

An Efficient Parallel Algorithm
for Computing a Maximal
Independent Set in a
Hypergraph of Dimension 3
Elias Dahlhaus 1
Marek Karpinski 2
Peter Kelsen 3
TR-92-071
October, 1992
Abstract
The paper considers the problem of computing a maximal independent set
in a hypergraph (see [3] and [7]). We present an efficient deterministic NC
algorithm for finding a maximal independent set in a hypergraph of dimension
3: the algorithm runs in time O(log 4 n) time on n + m processors of an
EREW PRAM and is optimal up to a polylogarithmic factor. Our algorithm
adapts the technique of Goldberg and Spencer ([5]) for finding a maximal
independent set in a graph (or hypergraph of dimension 2). It is the first
efficient NC algorithm for finding a maximal independent set in a hypergraph
of dimension greater than 2.
1  Department of Computer Science, University of Bonn,   5300 Bonn 1.

Efficient PRAM Simulation on a
Distributed Memory Machine
Richard M. Karp
University of California at Berkeley and
International Computer Science Institute,   Berkeley, CA
Michael Luby
International Computer Science Institute,   Berkeley, CA
and UC Berkeley
Friedhelm Meyer auf der Heide
Heinz Nixdorf Institute and Computer Science Department,
University of Paderborn,    Germany
TR-93-040
August 1993
Abstract
We present algorithms for the randomized simulation of a shared memory machine
(PRAM) on a Distributed Memory Machine (DMM). In a PRAM, memory conflicts
occur only through concurrent access to the same cell, whereas the memory of a
DMM is divided into modules, one for each processor, and concurrent accesses to
the same module create a conflict. The delay of a simulation is the time needed to
simulate a parallel memory access of the PRAM. Any general simulation of an m
processor PRAM on a n processor DMM will necessarily have delay at least m=n. A
randomized simulation is called time-processor optimal if the delay is O(m=n) with
high probability. Using a novel simulation scheme based on hashing we obtain a
time-processor optimal simulation with delay O(loglog(n)log (n)). The best previous
simulations use a simpler scheme based on hashing and have much larger delay:
fi(log(n)= loglog(n)) for the simulation of an n processor PRAM on an n processor
DMM, and fi(log(n)) in the case where the simulation is time-processor optimal.
Research partially supported by NSF/DARPA Grant CCR-9005448
Research partially supported by NSF operating grant CCR-9016468 and by grant No. 89-00312 from
the United States-Israel Binational Science Foundation (BSF), Jerusalem, Israel.
Part of work was done during a visit at the International Computer Science Institute at Berkeley;
supported in part by DFG-Forschergruppe "Effiziente Nutzung massiv paralleler Systeme, Teilprojekt 4",
and by the Esprit Basic Research Action Nr. 7141 (ALCOM II).
+PAGE+

Modeling a Copier Paper Path:
A Case Study in Modeling
Transportation Processes
Vineet Gupta Peter Struss
TR-95-019
Abstract
We present a compositional model of paper transportation in a photocopier that is meant to
support different problem solving tasks like simulation and diagnosis, and to be applicable to
a variety of configurations. Therefore, we try to avoid making hard-wired implicit assumptions
about design principles and possible scenarios. In order to simplify our analysis, the model
abstracts away from the physical forces and reasons only about velocities. Nonetheless, it
succeeds in determining essential features of the motion of the sheet of paper like buckling
and tearing. The framework provided is quite generic and can be used as a starting point for
developing models of other transportation domains.
Xerox Palo Alto Research Center,   3333 Coyote Hill Road, Palo Alto CA 94304 USA.   (vgupta@parc.xerox.com)
Technical University of Munich,   Orleansstr. 34, D-81667 Munich, Germany.   (struss@informatik.tu-muenchen.de)
+PAGE+

Parallel Sorting With Limited
Bandwidth
Micah Adler John W Byers Richard M Karp
TR-TR-95-031
July 1995
Abstract
We study the problem of sorting on a parallel computer with limited communication
bandwidth. By using the recently proposed PRAM(m) model, where p processors
communicate through a small, globally shared memory consisting of m bits, we focus
on the trade-off between the amount of local computation and the amount of inter-processor communication required for parallel sorting algorithms. We prove a lower
bound of ( n log m
m ) on the time to sort n numbers in an exclusive-read variant of
the PRAM(m) model. We show that Leighton's Columnsort can be used to give
an asymptotically matching upper bound in the case where m grows as a fractional
power of n. The bounds are of a surprising form, in that they have little dependence
on the parameter p. This implies that attempting to distribute the workload across
more processors while holding the problem size and the size of the shared memory
fixed will not improve the optimal running time of sorting in this model. We also
show that both the upper and the lower bound can be adapted to bridging models
that address the issue of limited communication bandwidth: the LogP model and
the BSP model. The lower bounds provide convincing evidence that efficient parallel
algorithms for sorting rely strongly on high communication bandwidth.
Supported by a Schlumberger Foundation Graduate Fellowship.
Supported by a GAANN Graduate Fellowship.
Supported by NSF grant number CCR-9005448
+PAGE+

Smoothing and Multiplexing
Tradeoffs for Deterministic
Performance Guarantees to VBR
Video
Edward W. Knightly and Paola Rossaro
Also with EECS Department, U.C. Berkeley
TR-95-033
Abstract
The burstiness of variable bit rate traffic makes it difficult to both efficiently utilize network resources and provide end-to-end network performance guarantees to the traffic sources.
Generally, smoothing or shaping traffic sources at the entrance of the network reduces their
burstiness to allow higher utilization within the network. However, this buffering introduces
an additional delay so that, in effect, lossless smoothing trades queueing delay inside the
network for smoothing delay at the network edge. In this paper, we consider the net effect
of smoothing on end-to-end performance guarantees where a no-loss, no-delay-violation deterministic guarantee is provided with the D-BIND traffic model. We analytically quantify
these tradeoffs and provide a set of general rules for determining under which conditions
smoothing provides a net gain. We also empirically investigate these tradeoffs using traces
of MPEG compressed video.
+PAGE+

INTERNATIONAL COMPUTER SCIENCE INSTITUTE
I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198   * (510) 643-9153 * FAX (510) 643-7684
Elementary Proofs of some
Results on Representations of
p-groups
M.A. Shokrollahi
TR-95-054
September 1995
Abstract
A result of Roquette [3] states that if D is an absolutely irreducible representation
of a p-group G over the field of complex numbers, then D can be realized in K((g) j
g 2 G), where is the character of D and K = Q or K = Q(i) according to whether
p 6= 2 or p = 2. Based on Baum and Clausen's [1] algorithm for computing the
irreducible representations of supersolvable groups, we give an elementary proof of a
theorem which, among other well-known facts on representations of p-groups, implies
Roquette's result.
+PAGE+

INTERNATIONAL COMPUTER SCIENCE INSTITUTE
I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198 *   (510) 643-9153 * FAX (510) 643-7684
Managing ABR Capacity in
Reservation-based Slotted
Networks
Roya Ulrich and Pieter Kritzinger
fulrich@icsi.berkeley.edu, psk@cs.uct.ac.zag
The Networks Group
International Computer Science Institute, and
The Computer Science Depatrment
University of Cape Town
TR-96-006
January 1996
Abstract
For slotted networks carrying full multi-media traffic to work successfully, it is essential that connection setup and management is done well under all traffic conditions.
Major challenges remain with the current state of the technology, however, particularly on how one copes with traffic bursts. Existing reservation-based networks do not
allow the user to dynamically adjust his bandwidth requirements on demand. In this
paper we propose a new scheme, called the reservoir scheme, which allows dynamic
and distributed resource allocation. The basic idea behind the scheme is to reserve
bandwidth with a guaranteed bit rate for each virtual circuit. The user is allowed to
decentrally allocate additional bandwidth from an Available Bit Rate (ABR) reservoir to satisfy dynamic changes of Variable Bit Rate (VBR) traffic. The duration and
bandwidth of this dynamic access are negotiated in the call setup phase and do not
require any renegotiation with the service provider so that this solution overcomes the
rigidity of current static bandwidth reservation schemes. The additional management
requirements are low compared to other dynamic bandwidth reservation schemes. We
also describe an analytic model and simulation which we used to determine whether
it would be practical to apply the proposed scheme in a slotted network.
Pieter Kritzinger is in the Computer Science Depatrment, University of Cape Town, private
Bag, Rondbosch 7700
+PAGE+

INTERNATIONAL COMPUTER SCIENCE INSTITUTE
I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198   * (510) 643-9153 * FAX (510) 643-7684
Structural Grobner Basis
Detection
Bernd Sturmfels and Markus Wiegelmann
TR-96-017
Department of Mathematics, UC Berkeley
Berkeley, California 94720, USA
bernd@math.berkeley.edu
Fachbereich Mathematik, Universitat Trier
Universitatsring 15, D-54286 Trier, Germany
wiegelm@uni-trier.de
Abstract
We determine the computational complexity of deciding whether m polynomials in n variables have relatively prime leading terms with respect to some
term order. This problem is NP-complete in general, but solvable in polynomial time for m fixed and for nm fixed. Our new algorithm for the latter case
determines a candidate set of leading terms by solving a maximum matching
problem. This reduces the problem to linear programming.
+PAGE+

INTERNATIONAL COMPUTER SCIENCE INSTITUTE
I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198    * (510) 643-9153 * FAX (510) 643-7684
A Management Platform for
Global Area ATM Networks
Roya Ulrich
ulrich@icsi.berkeley.edu
TR-96-018
Abstract
Technological progress has made providing numerous new services to large number
of users possible. Concurrently, we also experience an increased interest in real-time
and interactive applications, e. g. teleseminaring, video conferencing and application
sharing, in particular, because of the worldwide and decentralized character of today's
research and development organizations.
The International Computer Science Institute (ICSI) is a participant of the first
transatlantic ATM link which is an integral part of the Multimedia Applications
on Intercontinental Highways (MAY) Project. Additionally, ICSI is attached to the
Bay Area Gigabit Network (BAGNet) providing ATM connectivity at the best-effort
basis. Both projects provide platforms to identify the key research and development
topics in cooperative real-time communication.
The technical report gives a brief introduction to the ATM infrastructure at ICSI and
addresses challenging management issues of multimedia applications in such global
area ATM networks. We explore three management areas: performance, configuration,
and fault management with respect to the user's point of view. Finally, we introduce
a management platform and tools we have been developing which help the user to
better predict the quality of service provided and to recover from faults occurred in
the system or during a transmission.
+PAGE+

Design and Implementation
of a Web-based Tool for
ATM Connection Management
Martin Bernhardt
mbhard@icsi.berkeley.edu
TR-96-041
August 1996
Abstract
At the International Computer Science Institute (ICSI), there is an ongoing effort
to gain experience on ATM and multi-media applications. ICSI is participating in
two ATM pilots called Bay Area Gigabit Network (BAGNet) and Multimedia Applications on Intercontinental Highway (MAY). Beside these wide-area trial ICSI's
ATM network is used for local multi-media experiments. The ATM environment at
ICSI is heterogeneous. Both, local and long distance traffic is based on permanent
virtual connections. The management of this environment has often been cumbersome and time-consuming for a number of reasons: The ATM devices have to be
accessed separately in an unintegrated manner. Different vendor-specific tools with
different user interfaces are used. Configuration data is stored unstructured, redundant and not centralized. Users cannot setup or verify a connection without knowing
device-specific details. Hence, the need for a software tool arose that can minimize the
administrative work spent on connection management. This technical report contains
my master's thesis which is about the design and implementation of TOMCAD a
tool for monitoring and configuration of ATM devices. Being a web-based software
tool it can support local and wide-area connectivity and provide access for local and
remote users.
+PAGE+

On-line Load Balancing for
Related Machines
Piotr Berman
Moses Charikar
Marek Karpinski
TR-97-007
January 1997
Abstract
We consider the problem of scheduling permanent jobs on related machines in an
on-line fashion. We design a new algorithm that achieves the competitive ratio of 3 +
p
8 5:828 for the deterministic version, and 3:31= ln 2:155 4:311 for its randomized
variant, improving the previous competitive ratios of 8 and 2e 5:436. We also prove
lower bounds of 2:4380 on the competitive ratio of deterministic algorithms and 1:8372
on the competitive ratio of randomized algorithms for this problem.
Dept. of Computer Science & Eng., Pennsylvania State University,   University Park, PA16802, USA
Email:berman@cse.psu.edu
Department of Computer Science, Stanford University,   Stanford, CA 94305-9045.   Supported by Stanford
School of Engineering Groswith Fellowship, an ARO MURI Grant DAAH04-96-1-0007 and NSF Award
CCR-9357849, with matching funds from IBM, Schlumberger Foundation, Shell Foundation, and Xerox
Corporation.   E-mail: moses@cs.stanford.edu.
Dept. of Computer Science, University of Bonn,    53117 Bonn,   and International Computer Science Institute,     Berkeley.     This research was partially supported by the DFG Grant KA 673/4-1, by the ESPRIT BR
Grants 7097 and EC-US 030.   Email:marek@cs.uni-bonn.de
+PAGE+

Towards the Assessment of Logics for
Concurrent Actions
Choong-Ho Yi
Department of Computer and Information Science
Linkoping University
581 83 Linkoping, Sweden
E-mail: choyi@ida.liu.se
Abstract
We have introduced concurrency into the framework of Sandewall. The resulting formalism is capable of reasoning about interdependent as well
as independent concurrent actions. Following
Sandewall's systematical method, we have then
applied the entailment criterion PCM to selecting
intended models of common sense theories where
concurrent actions are allowed, and proved that
the criterion leads to only intended models for a
subset of such theories.
Introduction

On Learning Soccer Strategies
Rafa l Sa lustowicz, Marco Wiering, Jurgen Schmidhuber
IDSIA,   Corso Elvezia 36, 6900 Lugano, Switzerland
e-mail: frafal, marco, juergeng@idsia.ch
In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, editors,
Proceedings of the Seventh International Conference on Artificial
Neural Networks (ICANN'97), volume 1327 of Lecture Notes in Computer
Science, pages 769-774. Springer-Verlag Berlin Heidelberg, 1997.
Abstract. We use simulated soccer to study multiagent learning. Each
team's players (agents) share action set and policy but may behave differently due to position-dependent inputs. All agents making up a team
are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare two learning algorithms:
TD-Q learning with linear neural networks (TD-Q) and Probabilistic
Incremental Program Evolution (PIPE). TD-Q is based on evaluation
functions (EFs) mapping input/action pairs to expected reward, while
PIPE searches policy space directly. PIPE uses an adaptive probability
distribution to synthesize programs that calculate action probabilities
from current inputs. Our results show that TD-Q has difficulties to learn
appropriate shared EFs. PIPE, however, does not depend on EFs and
finds good policies faster and more reliably.
1 Introduction

PMoct: A Policy management tool for OCT based Design Systems for Multiple Domains
ISI Research Report
ISI/RR-93-387
October, 1993
PMoct: A Policy management tool for OCT
based Design Systems for Multiple Domains
John Granacki and Tauseef Kazi
ISI/RR-93-387
October, 1993
University of Southern California
Information Science Institute
4676 Admiralty Way, Marina del Rey, CA 90292
Unclassified/Unlimited
UNIVERSITY OF SOUTHERN CALIFORNIA INFORMATION SCIENCES INSTITUTE
4676 Admiralty Way Marina del Rey, CA 90292
+PAGE+

Induction as Knowledge Integration
Benjamin D. Smith
Jet Propulsion Laboratory
California Institute of Technology
4800 Oak Grove Drive M/S 525-3660
Pasadena, CA 91109-8099
smith@aig.jpl.nasa.gov
Paul S. Rosenbloom
Information Sciences Institute & Computer Science Dept.
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
rosenbloom@isi.edu
Abstract
Two key issues for induction algorithms are the accuracy of the learned hypothesis and the computational
resources consumed in inducing that hypothesis. One
of the most promising ways to improve performance
along both dimensions is to make use of additional
knowledge. Multi-strategy learning algorithms tackle
this problem by employing several strategies for handling different kinds of knowledge in different ways.
However, integrating knowledge into an induction algorithm can be difficult when the new knowledge differs significantly from the knowledge the algorithm
already uses. In many cases the algorithm must be
rewritten.
This paper presents KII, a Knowledge Integration
framework for Induction, that provides a uniform
mechanism for integrating knowledge into induction.
In theory, arbitrary knowledge can be integrated with
this mechanism, but in practice the knowledge representation language determines both the knowledge
that can be integrated, and the costs of integration
and induction. By instantiating KII with various set
representations, algorithms can be generated at different trade-off points along these dimensions.
One instantiation of KII, called RS-KII, is presented
that can implement hybrid induction algorithms, depending on which knowledge it utilizes. RS-KII is
demonstrated to implement AQ-11 (Michalski 1978),
as well as a hybrid algorithm that utilizes a domain
theory and noisy examples. Other algorithms are also
possible.
Introduction

A Hybrid Intelligent Architecture for Refining Input Characterization and
Domain Knowledge
Ismail Taha and Joydeep Ghosh
Department of Electrical and Computer Engineering,
University of Texas,   Austin, TX 78712-1084
E-mail: fIsmail,Ghoshg@pine.ece.utexas.edu
Abstract: A Hybrid Intelligent Architecture that aims to exploit the complementary features of
expert systems and connectionist architecture, is proposed to revise input characterization and initial
domain knowledge. HIA has two building blocks, a Rule-Based module and a Connectionist Architecture
module. A specific format for the rule-based description of the initial theory acquired from the application
domain enables its mapping into a uniform, three layer network. Continuous inputs are discretized into
input vectors using a new coarse coding scheme. An extension of the Backpropagation Algorithm allows
refinement of the discretization functions. A successful application to the control of dams on the Colorado
river near Austin, is described.
1 Introduction

Generality and Difficulty in Genetic Programming:
Evolving a Sort
Kenneth E. Kinnear, Jr.
Adaptive Computing Technology
62 Picnic Rd.
Boxboro, MA 01719 USA
kim.kinnear@adapt.com
Abstract
Genetic Programming is applied to the task of
evolving general iterative sorting algorithms. A
connection between size and generality was discovered. Adding inverse size to the fitness measure along with correctness not only decreases
the size of the resulting evolved algorithms, but
also dramatically increases their generality and
thus the effectiveness of the evolution process. In
addition, a variety of differing problem formulations are investigated and the relative probability
of success for each is reported. An example of an
evolved sort from each problem formulation is
presented, and an initial attempt is made to
understand the variations in difficulty resulting
from these differing problem formulations.
1 Introduction

Representation Requirements for Supporting
Decision Model Formulation
Tze-Yun Leong
MIT Laboratory for Computer Science
545 Technology Square, room 420
Cambridge, MA 02139
(leong@lcs.mit.edu)
Abstract
This paper outlines a methodology for
analyzing the representational support
for knowledge-based decision-modeling
in a broad domain. A relevant set of inference
patterns and knowledge types are identified.
By comparing the analysis results to existing representations, some insights are gained
into a design approach for integrating categorical and uncertain knowledge in a context
sensitive manner.
1 Introduction

ACM Sigplan Notices 27,3 (March 1992),66-70.
Copyright 1991 by Nimble Computer Corporation 1
The Treadmill:
Real-Time Garbage Collection Without Motion Sickness
Henry G. Baker
Nimble Computer Corporation,   16231 Meadow Ridge Way, Encino, CA 91436
(818) 501-4956 (818) 986-1360 FAX
A simple real-time garbage collection algorithm is presented which does not copy, thereby avoiding
some of the problems caused by the asynchronous motion of objects. This in-place "treadmill"
garbage collection scheme has approximately the same complexity as other nonmoving garbage
collectors, thus making it usable in a high-level language implementation where some pointers
cannot be traced. The treadmill is currently being used in a Lisp system built in Ada.
INTRODUCTION

Multilayer perceptrons may learn simple rules quickly
R. Urbanczik
Institut fur theoretische Physik
Universitat Wurzburg
Am Hubland
D-97074 Wurzburg
Germany
November 27, 1997
Abstract
Zero temperature Gibbs learning is considered for a connected committee machine
with K hidden units. For large K, the scale of the learning curve strongly depends
on the target rule. When learning a perceptron, the sample size P needed for optimal
generalization scales so that N t P t KN, where N is the dimension of the input.
This even holds for a noisy perceptron rule if a new input is classified by the majority
vote of all students in the version space. When learning a committee machine with M
hidden units, 1 t M t K, optimal generalization requires
p
+PAGE+

Appears in the Proceedings of the ACM SIGMOD International Conference on Management of Data, San Jose, CA, May 1995
Efficient Optimistic Concurrency Control
Using Loosely Synchronized Clocks
Atul Adya Robert Gruber Barbara Liskov Umesh Maheshwari
Laboratory for Computer Science,
Massachusetts Institute of Technology,
545 Technology Square, Cambridge, MA 02139
fadya,gruber,liskov,umeshg@lcs.mit.edu
Abstract
This paper describes an efficient optimistic concurrency control
scheme for use in distributed database systems in which objects are
cached and manipulated at client machines while persistent storage
and transactional support are provided by servers. The scheme
provides both serializability and external consistency for committed
transactions; it uses loosely synchronized clocks to achieve global
serialization. It stores only a single version of each object, and
avoids maintaining any concurrency control information on a per-
object basis; instead, it tracks recent invalidations on a per-client
basis, an approach that has low in-memory space overhead and no
per-object disk overhead. In addition to its low space overheads,
the scheme also performs well. The paper presents a simulation
study that compares the scheme to adaptive callback locking, the
best concurrency control scheme for client-server object-oriented
database systems studied to date. The study shows that our
scheme outperforms adaptive callback locking for low to moderate
contention workloads, and scales better with the number of clients.
For high contention workloads, optimism can result in a high abort
rate; the scheme presented here is a first step toward a hybrid scheme
that we expect to perform well across the full range of workloads.
1 Introduction

Decision-Theoretic Troubleshooting: A Framework for
Repair and Experiment
John S. Breese
David Heckerman
&lt;breese|heckerma@microsoft.com&gt;
March, 1996
(revised May 1996)
Technical Report
MSR-TR-96-06
Microsoft Research
Advanced Technology Division
Microsoft Corporation
One Microsoft Way
Redmond, WA 98052
Also appears in the Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence,
August, 1996
+PAGE+

Context-Insensitive Alias Analysis
Reconsidered
Erik Ruf
erikruf@microsoft.com
May 16, 1995
Technical Report
MSR-TR-95-20
Microsoft Research
Advanced Technology Division
Microsoft Corporation
One Microsoft Way
Redmond, WA 98052
This report is a preprint of the paper "Context-Insensitive Alias Analysis Reconsidered," to appear in ACM SIGPLAN
'95 Conference on Programming Language Design and Implementation (PLDI'95), La Jolla, California, June 1995.
Copyright c 1995 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of all or
part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To
copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from Publications Dept, ACM Inc., fax + 1 (212) 869-0481, or permissions@acm.org.
+PAGE+

Global Tree Optimization:
A Non-greedy Decision Tree Algorithm
Kristin P. Bennett
Email bennek@rpi.edu
Department of Mathematical Sciences
Rensselaer Polytechnic Institute
Troy, NY 12180
Abstract
A non-greedy approach for constructing globally optimal
multivariate decision trees with fixed structure is proposed. Previous greedy tree construction algorithms are
locally optimal in that they optimize some splitting criterion at each decision node, typically one node at a time.
In contrast, global tree optimization explicitly considers
all decisions in the tree concurrently. An iterative linear
programming algorithm is used to minimize the classification error of the entire tree. Global tree optimization
can be used both to construct decision trees initially and
to update existing decision trees. Encouraging computational experience is reported.
1 Introduction

DEPARTMENT OF STATISTICS
University of Wisconsin
1210 West Dayton St.
Madison, WI 53706
TECHNICAL REPORT NO. 910
December 21, 1993
Behavior Near Zero of the Distribution of GCV Smoothing
Parameter Estimates 1
by
Grace Wahba and Yuedong Wang
1 Supported by the National Science Foundation under Grant DMS-9121003 and the National Eye Institute under
Grant R01 EY09946.   e-mail wahba@stat.wisc.edu, wang@stat.wisc.edu
+PAGE+

Selecting Tense, Aspect, and Connecting Words
In Language Generation
Bonnie J. Dorr
Department of Computer Science
University of Maryland
College Park, MD 20742
bonnie@cs.umd.edu
Terry Gaasterland
Mathematics and Computer Science Division
Argonne National Laboratory
Argonne, IL 60432
gaasterland@mcs.anl.gov
Abstract
Generating language that reflects the temporal organization of represented knowledge requires a language generation model that integrates contemporary theories of tense and aspect, temporal representations, and methods
to plan text. This paper presents a model
that produces complex sentences that reflect
temporal relations present in underlying temporal concepts. The main result of this work
is the successful application of constrained linguistic theories of tense and aspect to a generator which produces meaningful event combinations and selects appropriate connecting words
that relate them.
1 Introduction

Machine Translation, 10:1-2, 139-180 (1995)
c 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
Toward a Lexicalized Grammar for Interlinguas
CLARE VOSS   voss@cs.umd.edu
BONNIE J. DORR   bonnie@cs.umd.edu
Department of Computer Science, University of Maryland,   College Park, MD 20742
Received September 1,1994; Revised July 15, 1995
Abstract. In this paper we present one aspect of our research on machine translation (MT):
capturing the grammatical and computational relation between (i) the interlingua (IL) as defined
declaratively in the lexicon and (ii) the IL as defined procedurally by way of algorithms that
compose and decompose pivot IL forms. We begin by examining the interlinguas in the lexicons of
a variety of current IL-based approaches to MT. This brief survey makes it clear that no consensus
exists among MT researchers on the level of representation for defining the IL. In the section that
follows, we explore the consequences of this missing formal framework for MT system builders who
develop their own lexical-IL entries. The lack of software tools to support rapid IL respecification
and testing greatly hampers their ability to modify representations to handle new data and new
domains. Our view is that IL-based MT research needs both (a) the formal framework to specify
possible IL grammars and (b) the software support tools to implement and test these grammars.
With respect to (a), we propose adopting a lexicalized grammar approach, tapping research
results from the study of tree grammars for natural language syntax. With respect to (b), we
sketch the design and functional specifications for parts of ILustrate, the set of software tools
that we need to implement and test the various IL formalisms that meet the requirements of a
lexicalized grammar. In this way, we begin to address a basic issue in MT research, how to define
and test an interlingua as a computational language | without building a full MT system for
each possible IL formalism that might be proposed.
Keywords: interlingua, machine translation, lexicon, lexicalized grammar
1. Introduction

v4.9 1
Binding as an Interface Condition:
An Investigation of Hindi Scrambling
by
Douglas Arnold Jones
Bachelor of Arts in Linguistics, Stanford University (1987)
Master of Arts in Linguistics, Stanford University (1989)
Submitted to the Department of Linguistics and Philosophy in
Partial Fulfillment of the Requirements for the Degree of
Doctor of Philosophy
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
Douglas Arnold Jones
The author hereby grants to M.I.T permission to reproduce and
to distribute copies of this thesis document in whole or in
part.
Signature of Author
Department of Linguistics and Philosophy
July 22, 1993
Certified by
Noam Chomsky
Institute Professor
Accepted by
Wayne O'Neil
Head, Department of Linguistics and Philosophy
+PAGE+

GROWING RADIAL BASIS FUNCTION NETWORKS
E. BLANZIERI , P. KATENKAMP flfl and A. GIORDANA flflfl
Centro di Scienza Cognitiva, Universita di Torino,   Via Lagrange 3, 10100 Torino, Italy.   e-mail:
blanzier@psych.unito.it
flfl Institute for Real Time Systems and Robotics, University of Karlsruhe,   Germany.
flflfl Dipartimento di Informatica, Universita di Torino,   C.so Svizzera 185, 10149 Torino, Italy.   email: attilio@di.unito.it
Abstract. This paper presents and evaluates two algorithms for incrementally constructing Radial
Basis Function Networks, a class of neural networks which looks more suitable for adtaptive control
applications than the more popular backpropagation networks. The first algorithm has been derived
by a previous method developed by Fritzke, while the second one has been inspired by the CART
algorithm developed by Breiman for generation regression trees. Both algorithms proved to work
well on a number of tests and exhibit comparable performances. An evaluation on the standard case
study of the Mackey-Glass temporal series is reported.
Key Words. Machine Learning, Robotics, Neural Nets
1 INTRODUCTION

MULTIMEDIA MEETS MACHINE LEARNING
STEFAN M UNCH
Institute for Real-Time Computer Systems & Robotics, University of Karlsruhe,    Kaiserstr. 12,
D-76128 Karlsruhe, Germany.
Abstract. The application of Machine Learning techniques to multimedia and multimodal systems,
resp., seems to be a promising approach in order to enhance the systems' capabilities. Especially
in multimodal systems which support human-computer interaction (HCI) via several input/output
channels in parallel, intelligent mechanisms are needed in order to process the user's inputs and to
select the best output modality.
In this paper, we will deal with a multi-agent system which introduces some kind of haptic feedback
to the user interface. The main task of the system is to predict the next user action in order to launch
the haptic feedback selectively and to adapt this capability over time to both, the user's behavior
and the application's user interface structure. Therefore, a statistical interaction model is generated
and managed based on stochastic classification methods.
Key Words. Multimedia, Multimodality, Haptic Output, Man-Machine-Systems, Human-Computer Interaction, Classification, User Modelling
1 INTRODUCTION

On Scheduling Two Classes of Real Time Traffic With Identical
Deadlines
Sridhar Pingali
Dept. of Electrical and Computer Engineering
University of Masschusetts
Amherst, MA 01003
James F. Kurose
Dept. of Computer and Information Science
Univeristy of Massachusetts
Amherst, MA 01003
Abstract
The problem of scheduling two classes of real-time traffic with correlated time constraints
is considered. Three scheduling disciplines are studied: a priority discipline which gives strict
priority to one class of traffic, a threshold-based scheme in which priority is given to one class
of traffic when the minimum laxity of its queued packets falls below some threshold, and a
"balancing" scheme which assigns priority on the basis of the differences in minimum laxities in
the two classes of traffic. Analytic results are obtained by using a discrete time model to obtain
the state occupancy probabilities for the system. Here, the state is defined using the laxities of
the queued real time packets. Parameters are defined to study the tradeoff in the performance
of the two classes of traffic. Results are obtained to demonstrate how the balancing scheme
permits us to achieve significant improvement in the performance of one class of traffic with
only minimal effect on the performance of other class. A video application is suggested for this
work.
1 Introduction

Real-Time Reliable Multicast Using Proactive Forward Error Correction
Dan Rubenstein, Jim Kurose, and Don Towsley
Computer Science Department
University of Massachusetts
Amherst, MA 01003
fdrubenst, kurose, towsleyg@cs.umass.edu
Technical Report 98-19
Department of Computer Science
March 1998
Abstract
Real-Time reliable multicast over a best-effort service network remains a challenging research problem. Most
protocols for reliable multicast use repair techniques that result in significant and variable delay, which can lead to
missed deadlines in real-time scenarios. This paper presents a repair technique that combines forward error correction
(FEC) with automatic repeat request (ARQ). The novel aspect of the technique is its ability to reduce delay in reliable
multicast delivery by sending repairs proactively (i.e., before they are required). The technique requires minimal
state at senders and receivers, and no additional active router functionality beyond what is required by the current
multicast service model. Furthermore, the technique uses only end-to-end mechanisms, where all data and repairs are
transmitted by the data-originating source, leaving receivers free from any burden of sending repairs. We simulate
a simple round-based version of a protocol embodying this technique to show its effectiveness in preventing repair
request implosion, reducing the expected time of reliable delivery of data, and keeping bandwidth usage for repairs
low. We show how a protocol using the technique can be adapted to provide delivery that is reliable before a real-time
deadline with probabilities extremely close to one. Finally, we develop several variations of the protocol that use the
technique in various fashions for high rate data streaming applications, and present results from additional simulations
that examine performance in a variety of Internet-like heterogeneous networks.
1 Introduction

To appear in Proc. IEEE INFOCOM, March 1996
The Effectiveness of Affinity-Based Scheduling in Multiprocessor Networking
James D. Salehi, James F. Kurose, and Don Towsley
Computer Science Department, University of Massachusetts,   Amherst MA 01003, USA
-salehi,kurose,towsley-@cs.umass.edu
Abstract
Techniques for avoiding the high memory overheads found on
many modern shared-memory multiprocessors are of increasing
importance in the development of high-performance multiprocessor protocol implementations. One such technique is processor-cache affinity scheduling, which can significantly lower packet
latency and substantially increase protocol processing throughput
[20]. In this paper, we evaluate several aspects of the effectiveness of affinity-based scheduling in multiprocessor network
protocol processing, under packet-level and connection-level par-allelization approaches. Specifically, we evaluate the performance
of the scheduling technique 1) when a large number of streams are
concurrently supported, 2) when processing includes copying of
uncached packet data, 3) as applied to send-side protocol processing, and 4) in the presence of stream burstiness and source locality, two well-known properties of network traffic. We find that
affinity-based scheduling performs well under these conditions,
emphasizing its robustness and general effectiveness in multiprocessor network processing. In addition, we explore a technique
which improves the caching behavior and available packet-level
concurrency under connection-level parallelism, and find performance improves dramatically.
1 Introduction

Loss Correlation for Queues with Bursty Input Streams
Henning Schulzrinne James F. Kurose and Donald F. Towsley
Dept. of ECE Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
hgschulz,kurose,towsley@cs.umass.edu
Abstract
The loss probability of a queueing system provides, in many cases, insufficient information for performance evaluation, for example, of data link layer protocols and applications with forward
error correction.
This paper evaluates and characterizes the correlation between packet losses for two queueing systems in discrete time
that are motivated by BISDN applications. The first, a two-class discrete-time queueing system, approximates the output
queue of an ATM switch. The queue serves periodic foreground
traffic and random background traffic. The background traffic
is modeled as i.i.d. batches of arbitrary distribution. It is shown
that the conditional loss probability (CLP) is independent of the
buffer size if the buffer size is at least as large as the period of
the foreground traffic. Example calculations indicate that losses
occur essentially randomly as long as the foreground traffic uses
less than 10% of the channel capacity.
The second analysis derives the CLP seen by a selected
stream in a slotted finite-buffer system with a superposition of
interrupted Poisson sources. Here, the total number of arrivals
may be correlated from slot to slot. Traffic correlation is seen
to have a strong influence on loss correlation, while buffer size
is seen to have virtually none.
1 Introduction

Optimal Smoothing of Stored Video and the Impact on
Network Resource Requirements fly
James D. Salehi, Zhi-Li Zhang, James F. Kurose, and Don Towsley
Department of Computer Science
University of Massachusetts
Amherst, MA 01003, U.S.A
(413) 545-3179 (voice), (413) 545-1249 (fax)
fsalehi,zhzhang,kurose,towsleyg@cs.umass.edu
Abstract
VBR compressed video is known to exhibit significant, multiple-time-scale bit rate variability. In this paper, we consider the transmission of stored video from a server to a client across a
high speed network, and explore how the client buffer space can be used most effectively toward
reducing the variability of the transmitted bit rate.
We present two basic results. First, we show how to achieve the greatest possible reduction in
rate variability when sending stored video to a client with given buffer size. We formally establish
the optimality of our optimal smoothing approach, and illustrate its performance over a set of
long MPEG-1 encoded video traces. Second, we evaluate the impact of optimal smoothing on the
network resources needed for video transport, under two network service models: Deterministic
Guaranteed service [1, 11] and Renegotiated CBR (RCBR) service [9, 8]. Under both models, we
find the impact of optimal smoothing to be dramatic.
An earlier version of this paper appeared at the 1996 ACM SIGMETRICS conference.
This work was supported by NSF under grant NCR-9206908 and by ARPA under ESD/AVS contract F-19628-92-C
0089.
+PAGE+

Acquiring and validating background knowledge
for machine learning using function
decomposition
Blaz Zupan and Saso Dzeroski
Department of Intelligent Systems, Jozef Stefan Institute
1000 Ljubljana, Slovenia   (E-mail: Blaz.Zupan@ijs.si, Saso.Dzeroski@ijs.si)
Abstract. Domain or background knowledge is often needed in order
to solve difficult problems of learning medical diagnostic rules. Earlier
experiments have demonstrated the utility of background knowledge
when learning rules for early diagnosis of rheumatic diseases. A particular form of background knowledge comprising typical co-occurrences
of several groups of attributes was provided by a medical expert. This
paper explores the possibility to automate the process of acquiring background knowledge of this kind. A method based on function decomposition is proposed that identifies typical co-occurrences for a given set
of attributes. The method is evaluated by comparing the typical co-occurrences it identifies, as well as their contribution to the performance
of machine learning algorithms, to the ones provided by a medical expert.
1 Introduction

Appears in: "Supercomputing '94," Nov. 1994.
Reprinted by permission of IEEE.
Paging Tradeoffs in Distributed-Shared-Memory Multiprocessors
Douglas C. Burger, Rahmat S. Hyder, Barton P. Miller, David A. Wood
Computer Sciences Department
University of Wisconsin-Madison
1210 W. Dayton Street
Madison, WI 53706 USA
wwt@cs.wisc.edu
Abstract
Massively parallel processors have begun using commodity operating systems that support demand-paged
virtual memory. To evaluate the utility of virtual
memory, we measured the behavior of seven shared-memory parallel application programs on a simulated
distributed-shared-memory machine. Our results (i)
confirm the importance of gang CPU scheduling, (ii)
show that a page-faulting processor should spin rather
than invoke a parallel context switch, (iii) show that
our parallel programs frequently touch most of their
data, and (iv) indicate that memory, not just CPUs,
must be "gang scheduled". Overall, our experiments
demonstrate that demand paging has limited value
on current parallel machines because of the applications' synchronization and memory reference patterns
and the machines' high page-fault and parallel-context-switch overheads.
1 Introduction

Runtime Support to Parallelize Adaptive Irregular
Programs
Yuan-Shin Hwang 1 Bongki Moon 1 Shamik Sharma 1 Raja Das 1
Joel Saltz 1
Abstract
This paper describes how a runtime support library can be used as compiler runtime
support in irregular applications. The CHAOS runtime support library carries out
optimizations designed to reduce communication costs by performing software caching,
communication coalescing and inspector/executor preprocessing. CHAOS also supplies
special purpose routines to support specific types of irregular reduction and runtime
support for partitioning data and work between processors. A number of adaptive
irregular codes have been parallelized using the CHAOS library and performance results
from these codes are also presented in this paper.
1 Introduction

Supporting Irregular Distributions in FORTRAN 90D/HPF
Compilers
Ravi Ponnusamy yz Yuan-Shin Hwang Raja Das
Joel Saltz Alok Choudhary Geoffrey Fox
UMIACS and Computer Science Department Northeast Parallel Architectures Center
University of Maryland Syracuse University
College Park, MD 20742 Syracuse, NY 13244
Abstract
This paper presents methods that make it possible to efficiently support irregular problems using data
parallel languages. The approach involves the use of a portable, compiler-independent, runtime support
library called CHAOS. The CHAOS runtime support library contains procedures that
* support static and dynamic distributed array partitioning,
* partition loop iterations and indirection arrays,
* remap arrays from one distribution to another, and
* carry out index translation, buffer allocation and communication schedule generation.
The CHAOS runtime procedures are used by a prototype Fortran 90D compiler as runtime support for irregular problems. This paper also presents performance results of compiler-generated and
hand-parallelized versions of two stripped down applications codes. The first code is derived from
an unstructured mesh computational fluid dynamics flow solver and the second is derived from the
molecular dynamics code CHARMM.
A method is described that makes it possible to emulate irregular distributions in HPF by reordering elements of data arrays and renumbering indirection arrays. The results suggest that an HPF
compiler could use reordering and renumbering extrinsic functions to obtain performance comparable
to that achieved by a compiler for a language (such as Fortran 90D) that directly supports irregular
distributions.
This work was sponsored in part by ARPA (NAG-1-1485), NSF (ASC 9213821), and ONR (SC292-1-22913).
+PAGE+

Semi-Analytical Techniques for Substrate Characterization in the
Design of Mixed-Signal ICs
Edoardo Charbon, Ranjit Gharpurey ,
Robert G. Meyer , and Alberto Sangiovanni-Vincentelli
Cadence Design Systems Inc.,   San Jose, CA Texas Instruments Inc., Dallas, TX
Department of EECS, University of California,   Berkeley, CA
Abstract
A number of methods are presented for highly efficient calculation
of substrate current transport. A three-dimensionalGreen's Function
based substrate representation, in combination with the use of the
Fast Fourier Transform, significantly speeds up the computation of
sensitivities with respect to all parameters associated with a given
architecture. Substrate sensitivity analysis is used in a number of
physical optimization tools, such as placement and trend analysis for
the estimation of the impact of technology migration and/or layout
re-design.
1 Introduction

High Performance Verification Algorithms
by
Jagesh V. Sanghavi
B.Tech. (Indian Institute of Technology, Bombay) 1989
M.S. (University of California at Berkeley, California) 1993
A dissertation submitted in partial satisfaction of the
requirements for the degree of
Doctor of Philosophy
in
Engineering Electrical Engineering
and Computer Sciences
in the
GRADUATE DIVISION
of the
UNIVERSITY of CALIFORNIA at BERKELEY
Committee in charge:
Professor Alberto L. Sangiovanni-Vincentelli
Professor Robert K. Brayton
Professor Phillip Colella
1996
+PAGE+

Interpretable Neural Networks with BP-SOM
Ton Weijters 1 , Antal van den Bosch 2 , and Jaap van den Herik 3
1 Information Technology, Eindhoven University of Technology,   The Netherlands
2 ILK / Computational Linguistics, Tilburg University,   The Netherlands
3 Department of Computer Science, Universiteit Maastricht,   The Netherlands
Abstract. Interpretation of models induced by artificial neural networks is often a difficult task. In this paper we focus on a relatively
novel neural network architecture and learning algorithm, bp-som, that
offers possibilities to overcome this difficulty. It is shown that networks
trained with bp-som show interesting regularities, in that hidden-unit
activations become restricted to discrete values, and that the som part
can be exploited for automatic rule extraction.
1 Introduction

THE PAPIA2 MACHINE: HARDWARE AND SOFTWARE ARCHITECTURE
A. Biancardi, V. Cantoni, M. Ferretti and M. Mosconi
Dipartimento di Informatica e Sistemistica
University of Pavia
Via Abbiategrasso 209
I-27100 PAVIA, ITALY
Tel: int +39.382.391350
ABSTRACT
This paper presents the overall structure of PAPIA2, a pyramid
system belonging to the family of massive parallel machines. It embeds
the topology of the quad-pyramid into a highly regular, fault tolerant,
eight-connected proces sor array by means of specially reconfigurable
near-neighbor interconnections. The system comes with a fully-fledged
software environment designed to optimize the use of machine resources.
The highly interactive graphic tools help in understanding the machine's
capabilities, provide a valuable testbed for the machine instruction set,
and offer a suitable context for monitoring program execution.
1. Introduction

TO APPEAR IN COMPUTATIONAL OPTIMIZATION AND APPLICATIONS JOURNAL, 1997
METAHEURISTICS FOR HIGHSCHOOL
TIMETABLING
Alberto Colorni
Centro di Teoria dei Sistemi del CNR
Dipartimento di Elettronica e Informazione
Politecnico di Milano
Piazza Leonardo da Vinci 32
20133 Milano, Italy
tel. +39-2-2399-3567
email: colorni@elet.polimi.it
Marco Dorigo
IRIDIA
Universit Libre de Bruxelles,   CP 194/6
Avenue Franklin Roosevelt 50
1050 Bruxelles, Belgium, European Union
tel. +32-2-6503169
email: mdorigo@ulb.ac.be
http://iridia.ulb.ac.be/dorigo/dorigo.html
Vittorio Maniezzo
Scienze dellInformazione
Universit di Bologna
Contrada Sacchi, 3
47023 Cesena, Italy
tel. +39-547-642830
email: maniezzo@csr.unibo.it
http://www.csr.unibo.it/~maniezzo
Subject categories:
Programming: heuristic, stochastic;
Mathematics: combinatorics.
The paper presents an application of simulated annealing, tabu search and an adapted
genetic algorithm to a real world instance of the timetable problem. The computational
results obtained are compared and discussed.
Other keywords: timetable problem, tabu search, simulated annealing, genetic algorithms
+PAGE+

Simulations with an Evolvable Fitness Formula
Henrik Hautop Lund Domenico Parisi
Institute of Psychology
National Research Council,   Viale Marx 15, 00137 Rome, Italy
tel.: (+39) 6 88 94 596
- DAIMI
University of Aarhus,   Ny Munkegade, 8000 Aarhus C., Denmark
tel.: (+45) 89 42 32 21
e-mail: hhl@daimi.aau.dk domenico@gracco.irmkant.rm.cnr.it
Abstract
The concept of a fitness formula as a property of an organism is proposed. In
artificial life simulations with organisms living in an environment, the fitness formula
can be interpreted as the ability of organisms to extract energy from potential food
sources distributed in the environment. In simulations where the goal of the genetic
algorithm is that of developing systems which exhibit a certain type of behavior in a
particular environment, the fitness formula becomes an independent variable which can
be manipulated in order to obtain the desired behavior. The fitness formula can be
viewed as an evolvable trait of organisms, and therefore not fixed and decided by the
researcher. Simulations with fixed and evolvable fitness formulae show that the fitness
formula, the sensory apparatus, and the behavior of organisms may co-evolve and be
co-adapted.
+PAGE+

Foundations for the Learning Web
Brian R. Gaines, Douglas H. Norrie and Mildred L. G. Shaw
University of Calgary
Calgary, Alberta, Canada T2N 1N4
gaines@cpsc.ucalgary.ca, norrie@enme.ucalgary.ca, mildred@cpsc.ucalgary.ca
Abstract: The learning web was presented [Norrie and Gaines, 1995] at EdMedia95
as a systemic approach to the modeling and support of knowledge processes in a
learning society. This article addresses the rationale for, and systemic foundations of,
the learning web, its implications for restructuring the higher education system, and
the role of information technology in supporting that restructuring. Two associated
articles report on the implementation of some of the technologies necessary to
support the learning web [Gaines and Shaw, 1996], and some preliminary experience
in applying them in undergraduate education [Shaw and Gaines, 1996].
1 Reengineering the Educational System

Layered, Server-based Support
for Object-Oriented Application Development
Guruduth Banavar Douglas Orr Gary Lindstrom
Department of Computer Science
University of Utah,   Salt Lake City, UT 84112 USA
fbanavar,dbo,lindstromg@cs.utah.edu
Abstract
This paper advocates the idea that the physical modularity (file structure) of application components supported by conventional OS environments can be elevated to the level of logical modularity, which in turn
can directly support application development in an
object-oriented manner. We demonstrate this idea
through a system-wide server that manages the manipulation of such components effectively. The server
is designed to be a fundamental operating system service responsible for binding and mapping component
instances into client address spaces.
We show how this model solves some longstanding
problems with the management of application components in existing application development environments. We demonstrate that this model's effectiveness derives from its support for the cornerstones of
OO programming: classes and their instances, encapsulation, and several forms of inheritance.
1 Introduction

Structured Markov Chain Monte Carlo
by Daniel J. SARGENT 1 , James S. HODGES 2 , and Bradley P. CARLIN 2
1 Section of Biostatistics, Mayo Clinic
2 Division of Biostatistics, School of Public Health, University of Minnesota
January 9, 1998
Abstract
In this paper we introduce a general method for Bayesian computing in richly-parameterized
models, Structured Markov Chain Monte Carlo (SMCMC), that is based on a blocked hybrid of the
Gibbs sampling and Metropolis-Hastings algorithms. SMCMC speeds algorithm convergence by
using the structure that is present in the problem to suggest an appropriate Metropolis-Hastings
candidate distribution. While the approach is easiest to describe for hierarchical normal linear
models, we show its extension to both non-normal and nonlinear cases to be straightforward.
After describing the method in detail we compare its performance (both in terms of runtime and
autocorrelation in the samples produced) to several other existing methods, including the traditional
single-site updating Gibbs sampler available in the popular BUGS software package. Our results
suggest significant improvements in convergence for many problems using SMCMC, as well as
broad applicability of the method, including previously intractable hierarchical nonlinear model
settings.
KEY WORDS: Blocking; Convergence acceleration; Gibbs sampling; Hierarchical model; Metropolis-Hastings algorithm.
+PAGE+

To Appear in the ACM Multimedia Journal
Dynamic Management of Guaranteed Performance Multimedia Connections
Colin Parris, Hui Zhang , and Domenico Ferrari
parris, hzhang, ferrari@tenet.Berkeley.EDU
Computer Science Division
University of California at Berkeley
Berkeley, CA 94720
Keywords: Multimedia, Network Management, Quality of Service, High Speed Networks.
Abstract
Most of the solutions proposed to support real-time (i.e. guaranteed performance) communication services in packet-switching networks adopt a connection-oriented and reservation-oriented approach. In such an approach, resource allocation
and route selection decisions are made before the start of the communication on the basis of resource availability and real-time network load at that time, and are usually kept for the duration of the communication. This rather static resource
management approach has certain limitations: it does not take into account (a) the dynamics of the communicating clients;
(b) the dynamics of the network state; and (c) the tradeoff between quality of service and network availability, thus affecting
the availability and flexibility of the real-time network services. Availability is the ability of the network to accommodate
as many real-time clients as possible, while flexibility is the ability to adapt the real-time services to changing network state
and client demands. In this paper, we present the Dynamic Connection Management (DCM) scheme, which addresses these
issues by providing the network with the capability to dynamically modify the performance parameters and the routes of
any existing real-time connection. With these capabilities, DCM can be used to increase the availability and flexibility of
the guaranteed performance service offered to the clients.
1 Introduction

Estimating the Selectivity of Spatial Queries Using
the `Correlation' Fractal Dimension
Alberto Belussi
Dipartimento di Elettronica e Informatica
Politecnico di Milano
Milano (Italy)
e-mail: belussi@elet.polimi.it
Christos Faloutsos
Institute for Systems Research (ISR) and
Dept. of Computer Science
Univ. of Maryland,   College Park
e-mail: christos@cs.umd.edu
February 24, 1995
Abstract
We examine the estimation of selectivities for range and spatial join queries in real spatial
databases. As we have shown earlier [FK94a], real point sets: (a) violate consistently the
"uniformity" and "independence" assumptions, (b) can often be described as "fractals", with
non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal
dimensions, the so called "Correlation Dimension" D 2 is the one that we need to predict the
selectivity of spatial join.
The main contribution is that, for all the real and synthetic point-sets we tried, the average
number of neighbors for a given point of the point-set follows a power law, with D 2 as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for "biased"
range queries (i.e., queries whose centers prefer areas of high point density).
We present the formulas to estimate the selectivity for the biased queries, including an integration constant (K `shape 0 ) for each query shape. Finally, we show results on real and synthetic
point sets, where our formulas achieve very low relative errors (typically about 10%, versus
40%-100% of the uniform assumption).
1 Introduction

MASSACHUSETTS INSTITUTE OF TECHNOLOGY
ARTIFICIAL INTELLIGENCE LABORATORY
and
CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING
DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES
A.I. Memo No. 1565   February 2, 1996
C.B.C.L. Memo No. 132
Probabilistic Independence Networks for Hidden
Markov Probability Models
Padhraic Smyth, David Heckerman, and Michael Jordan
A bstract
Graphical techniques for modeling the dependencies of random variables have been explored in a variety
of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image
processing, and genetics. Formalisms for manipulating these models have been developed relatively
independently in these research communities. In this paper we explore hidden Markov models (HMMs)
and related structures within the general framework of probabilistic independence networks (PINs). The
paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known
forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference
algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for
more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore
a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and
coarticulation in speech recognition are introduced and treated within the graphical model framework
to illustrate the advantages of the general approach.
Copyright c Massachusetts Institute of Technology, 1996
This report describes research done at the Department of Information and Computer Science, University of
California, Irvine, the Jet Propulsion Laboratory, California Institute of Technology, Microsoft Research, the
Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts
Institute of Technology. The authors can be contacted as pjs@aig.jpl.nasa.gov, heckerma@microsoft.com,
and jordan@psyche.mit.edu. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041).
Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects
Agency of the Dept. of Defense. MIJ gratefully acknowledges discussions with Steffen Lauritzen on the application
of the IPF algorithm to UPINs.
+PAGE+

Visualisation of Large Networks in 3-D Space:
Issues in
Implementation and Experimental Evaluation
Yan Xiao Paul Milgram
Abstract
Three dimensional visualisation has become a
widespread scheme for helping users to access
and manage large information network. In this
report, various techniques for displaying depth
information are reviewed, with an emphasis on
stereoscopic displays. Input devices used to interact with a 3-D space are also examined. Issues in 3-D network visualisation are elicited from
three viewpoints: psychological, task-related and
implementational. Consideration of these issues
leads to the design of a preliminary experimental
programme for evaluating various network visu-alisation techniques.
1 Introduction

MASSACHUSETTS INSTITUTE OF TECHNOLOGY
ARTIFICIAL INTELLIGENCE LABORATORY
and
CENTER FOR BIOLOGICAL INFORMATION PROCESSING
WHITAKER COLLEGE
A.I. Memo No. 1164   October 1989
C.B.I.P. Paper No. 45
Networks and the Best Approximation Property
Federico Girosi and Tomaso Poggio
Abstract
Networks can be considered as approximation schemes. Multilayer networks of the
backpropagation type can approximate arbitrarily well continuous functions (Cybenko,
1989; Funahashi, 1989; Stinchcombe and White, 1989). We prove that networks derived from regularization theory and including Radial Basis Functions (Poggio and
Girosi, 1989), have a similar property. From the point of view of approximation theory, however, the property of approximating continuous functions arbitrarily well is not
sufficient for characterizing good approximation schemes. More critical is the property
of best approximation. The main result of this paper is that multilayer networks, of the
type used in backpropagation, are not best approximation. For regularization networks
(in particular Radial Basis Function networks) we prove existence and uniqueness of
best approximation.
c Massachusetts Institute of Technology,1994
This paper describes research done within the Center for Biological Information Processing, in the Department of Brain and Cognitive Sciences, and at the Artificial Intelligence
Laboratory. This research is sponsored by a grant from the Office of Naval Research
(ONR), Cognitive and Neural Sciences Division; by the Artificial Intelligence Center of
Hughes Aircraft Corporation; by the Alfred P. Sloan Foundation; by the National Science Foundation. Support for the A. I. Laboratory's artificial intelligence research is provided by the Advanced Research Projects Agency of the Department of Defense under
Army contract DACA76-85-C-0010, and in part by ONR contract N00014-85-K-0124.
+PAGE+

Virtual Model Control of a Biped Walking Robot
by
Jerry E. Pratt
Submitted to the Department of Electrical Engineering and Computer Science
in partial fulfillment of the requirements for the degree of
Master of Engineering in Electrical Engineering and Computer Science
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
August 1995
c Jerry E. Pratt, MCMXCV. All rights reserved.
The author hereby grants to MIT permission to reproduce and distribute publicly
paper and electronic copies of this thesis document in whole or in part, and to grant
others the right to do so.
Author : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
Department of Electrical Engineering and Computer Science
August 25, 1995
Certified by : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
Gill A. Pratt
Assistant Professor of Electrical Engineering and Computer Science
Thesis Supervisor
Accepted by : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
F. R. Morgenthaler
Chairman, Department Committee on Graduate Theses
+PAGE+

Jonathan E. Hazan
and Richard G. Morgan
Technical Report no. 3/92
Artificial Intelligence Systems Research Group
Computer Science Division
School of Engineering and Computer Science
University of Durham,   DH1 3LE, UK
12th July 1993
Abstract
Programmers using imperative languages have a number of well-established debugging tools available to them; functional programmers have few, if any, tools available.
Many of the tools and techniques developed for debugging functional programs are
based on those for imperative programming and lack a theoretical basis relevant to
functional programming. In addition, the techniques used are typically very time-consuming. A theoretical foundation on which to base the study of errors and debugging in functional programming is presented in this report. Using this theoretical
foundation, a set of program transformation schemes has been developed which facilitate the location of the type of error which results in an evaluation-time error message
and the termination of evaluation. A brief description of the practical experience ob
tained using the tool is also presented.
The authors can be contacted by emailing J.E.Hazan@durham.ac.uk. FAX: +44 (0)91 374 3741. This
research is funded by a grant from the Science and Engineering Research Council of Great Britain.
+PAGE+

Real-time Communication in Multi-hop Networks
Dilip D. Kandlur, Kang G. Shin
Real-Time Computing Laboratory
Department of Elec. Engr. and Computer Science
The University of Michigan
Ann Arbor, Michigan 48109-2122.
email: kgshin@alps.eecs.umich.edu
Domenico Ferrari
Computer Science Division
EE & CS Department
University of California
Berkeley, CA 94720.
ABSTRACT
Communication in real-time systems has to be predictable, because unpredictable delays in
the delivery of messages can adversely affect the execution of tasks dependent on these messages.
In this paper, we develop a scheme for providing predictable inter-process communication in
real-time systems with (partially connected) point-to-point interconnection networks, which
provides guarantees on the maximum delivery time for messages. This scheme is based on the
concept of a real-time channel, a unidirectional connection between source and destination. A
real-time channel has parameters which describe the performance requirements of the source-destination communication, e.g., from a sensor station to a control site. Once such a channel
is established, the communications subsystem guarantees that these performance requirements
will be met. In this paper, we concentrate on methods to compute guarantees for the delivery time of messages belonging to real-time channels. We also address problems associated
with allocating buffers for these messages and develop a scheme which preserves delivery time
guarantees.
Index Terms | Real-time systems, communication, scheduling, guaranteed delay, point-to-point networks.
The work reported here is supported in part by an IBM Graduate Fellowship and by the Office of Naval
Research under Contract N00014-85-K-0122. Any opinions, findings, and conclusions or recommendations
expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.
+PAGE+

SEQUOIA 2000 -- A REFLECTION ON THE FIRST THREE YEARS
Michael Stonebraker
EECS Department
University of California, Berkeley
Abstract
This paper describes the SEQUOIA 2000 project
and its implementation efforts during the first three years.
Included are the objectives we had, how we chose to
address them and some of the lessons we learned from
this endeavor.
1. INTRODUCTION

The Chinook Hardware/Software Co-Synthesis System
Pai H. Chou Ross B. Ortega Gaetano Borriello
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195-2350
Abstract
Designers of embedded systems are facing ever tighter
constraints on design time, but computer aided design tools
for embedded systems have not kept pace with these trends.
The Chinook co-synthesis system addresses the automation of the most time-consuming and error-prone tasks in
embedded controller design, namely: the synthesis of interface hardware and software needed to integrate system
components; the migration of functions between processors
or custom logic; and the co-simulation of the design before,
during, and after synthesis. This paper describes the principal elements of Chinook and discuss its application to a
variety of embedded designs.
1 Introduction

Parallelization of Linearized
Applications in Fortran D
Lorie M. Liebrock
Ken Kennedy
CRPC TR93342-S
November, 1993
Center for Research on Parallel Computation
Rice University
P.O. Box 1892
Houston, TX 77251-1892
This research was supported by: Center for Research on
Parallel Computation, a National Science Foundation Science and Technology Center, through Cooperative Agreement No. CCR-9120008, NSF/NASA Agreement No.
ASC-9213821, and ONR Agreement No. N00014-93-1-0158. Use of the Intel i860 was provided under a Texas
CER Grant No. CISE 8619893.
+PAGE+

Dispersion Analysis of Numerical Wave Propagation
and its Computational Consequences
Alain Sei and William Symes
(Submitted to the Journal of Scientific Computing)
Abstract
We present in this paper a comparison of the dispersion properties for several finite-difference approximations of the acoustic wave equation. We investigate the compact and
staggered schemes of fourth order accuracy in space and of second order or fourth order accuracy in time. We derive the computational cost of the simulation implied by a precision
criterion on the numerical simulation (maximum allowed error in phase or group velocity).
We conclude that for moderate accuracy the staggered scheme of second order in time is more
efficient, whereas for very precise simulation the compact scheme of fourth order in time is
a better choice. The comparison increasingly favors the lower order staggered scheme as the
dimension increases. In three dimensional simulation, the cost of extremely precise simulation with any of the schemes is very large, whereas for simulation of moderate precision the
staggered scheme is the least expensive.
1 Introduction

Deterministic Parallel Fortran
K. Mani Chandy Ian Foster
Abstract
We describe Fortran M, message-passing extensions to Fortran 77 that provide
deterministic execution and information hiding while preserving desirable properties of
message passing.
1 Introduction

Trust-region interior-point algorithms for minimization problems
with simple bounds
J. E. Dennis Lus N. Vicente
Abstract
Two trust-region interior-point algorithms for the solution of minimization problems
with simple bounds are presented. The algorithms scale the local model in a way proposed
by Coleman and Li [1], but they are new otherwise. The first algorithm is more usual in
that the trust region and the local quadratic model are consistently scaled. The second
algorithm proposed here uses an unscaled trust region. A first-order convergence result for
these algorithms is given and dogleg and conjugate-gradient algorithms to compute trial
steps are introduced. Some numerical examples that show the advantages of the the second
algorithm are presented.
Keywords. trust-region methods, interior-point algorithms, Dikin-Karmarkar ellipsoid,
Coleman and Li scaling, simple bounds.
AMS subject classification. 49M37, 90C20, 90C30
1 Introduction

RICE UNIVERSITY
Optimizing Fortran90D/HPF for
Distributed-Memory Computers
by
Gerald H. Roth
A Thesis Submitted
in Partial Fulfillment of the
Requirements for the Degree
Doctor of Philosophy
Approved, Thesis Committee:
Ken Kennedy, Noah Harding Professor
Computer Science
John Mellor-Crummey, Faculty Fellow
Computer Science
William W. Symes, Professor
Computational and Applied Mathematics
R. Gregg Brickner, Technical Staff Member
Los Alamos National Laboratory
Houston, Texas
April, 1997
+PAGE+

Space-time domain decomposition for parabolic
problems
Eldar Giladi
Applied Mathematics 217-50
Caltech,   Pasadena CA 91125.
Herbert B. Keller
Applied Mathematics 217-50
Caltech,   Pasadena CA 91125.
March 25, 1997
Abstract
We analyze a space-time domain decomposition iteration, for a model advection
diffusion equation in one and two dimensions. The asymptotic convergence rate is
superlinear, and it is governed by the diffusion of the error across the overlap between
subdomains. Hence, it depends on both the size of this overlap and the diffusion
coefficient in the equation. However, it is independent of the number of subdomains.
The convergence rate for the heat equation in a large time window is initially linear and
it deteriorates as the number of subdomains increases. The duration of the transient
linear regime is proportional to the length of the time window. For advection dominated
problems, the convergence rate is initially linear and it improves as the the ratio of
advection to diffusion increases. Moreover, it is independent of the size of the time
window and of the number of subdomains. In two space dimensions, the iteration
possesses the smoothing property: high modes of the error are damped much faster
then low modes. This is a result of the natural smoothing property of the heat equation.
Numerical calculations illustrate our analysis.
1 Introduction

Reference: Proceedings of the IASTED International Conference on Artificial Intelligence, Expert Systems and Neural Networks, pp.
249-252, 1996.
Using Multiple Node Types to Improve the
Performance of DMP (Dynamic Multilayer Perceptron)
Tim L. Andersen and Tony R. Martinez
Computer Science Department, Brigham Young University,   Provo, Utah 84602
email: tim@axon.cs.byu.edu, martinez@cs.byu.edu
ABSTRACT
This paper discusses a method for training multilayer
perceptron networks called DMP2 (Dynamic Multilayer
Perceptron 2). The method is based upon a divide and conquer
approach which builds networks in the form of binary trees,
dynamically allocating nodes and layers as needed. The focus
of this paper is on the effects of using multiple node types
within the DMP framework. Simulation results show that
DMP2 performs favorably in comparison with other learning
algorithms, and that using multiple node types can be
beneficial to network performance.
1 Introduction

Proceedings of the International Conference on Artificial Intelligence, Expert Systems and Neural Networks
(AIE96), pp. 11-14, 1996.
Instance-Based Learning with Genetically Derived Attribute Weights
D. Randall Wilson, Tony R. Martinez
e-mail: randy@axon.cs.byu.edu, martinez@cs.byu.edu
Computer Science Department, Brigham Young University,   Provo, UT 84602, U.S.A.
Key words: instance-based learning, genetic algorithms, instance weights, generalization
Abstract. This paper presents an inductive learning system called the Genetic Instance-Based
Learning (GIBL) system. This system combines instance-based learning approaches with
evolutionary computation in order to achieve high accuracy in the presence of irrelevant or
redundant attributes. Evolutionary computation is used to find a set of attribute weights that
yields a high estimate of classification accuracy. Results of experiments on 16 data sets are
shown, and are compared with a non-weighted version of the instance-based learning system.
The results indicate that the generalization accuracy of GIBL is somewhat higher than that of the
non-weighted system on regular data, and is significantly higher on data with irrelevant or
redundant attributes.
1. Introduction

The weakest reasonable memory model
by
Matteo Frigo
Laurea, Universit a di Padova (1992)
Dottorato di Ricerca, Universit a di Padova (1996)
Submitted to the Department of Electrical Engineering and Computer
Science
in partial fulfillment of the requirements for the degree of
Master of Science
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
October 1997
c Matteo Frigo, MCMXCVII. All rights reserved.
The author hereby grants to MIT permission to reproduce and distribute
publicly paper and electronic copies of this thesis document in whole or in
part, and to grant others the right to do so.
Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Department of Electrical Engineering and Computer Science
January 28, 1998
Certified by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Charles E. Leiserson
Professor of Computer Science and Engineering
Thesis Supervisor
Accepted by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
PUT NAME HERE
Chairman, Departmental Committee on Graduate Students
+PAGE+

Incremental Cryptography and Application to Virus Protection
Mihir Bellare Oded Goldreich Shafi Goldwasser
Abstract
The goal of incremental cryptography is to design cryptographic algorithms with the property that having applied
the algorithm to a document, it is possible to quickly update
the result of the algorithm for a modified document, rather
than having to re-compute it from scratch. In settings where
cryptographic algorithms such as encryption or signatures
are frequently applied to changing documents, dramatic efficiency improvements can be achieved. One such setting is
the use of authentication tags for virus protection.
We consider documents that can be modified by powerful (and realistic) document modification operations such as
insertion and deletion of character-strings (or equivalently
cut and paste of text). We provide efficient incremental
signature and message authentication schemes supporting
the above document modification operations. They meet a
strong notion of tamper-proof security which is appropriate
for the virus protection setting. We initiate a study of incremental encryption, providing definitions as well as solutions.
Finally, we raise the novel issue of "privacy" of incremental
authentication schemes.
Abstract to appear in Proceedings of the 27th ACM Symposium on the Theory of Computing, May 1995.
1 Introduction

ON THE POWER OF TWO-POINTS BASED SAMPLING
Benny Chor Oded Goldreich flfl
MIT Laboratory for Computer Science
Cambridge, Massachusetts 02139
Abstract | The purpose of this note is to present a new sampling technique and to demonstrate
some of its properties. The new technique consists of picking two elements at random, and deterministically generating (from them) a long sequence of pairwise independent elements. The
sequence is guarantees to intersect, with high probability, any set of non-negligible density.
1. Introduction

On Yao's XOR-Lemma
Oded Goldreich Noam Nisan Avi Wigderson x
March 1995 (corrected Dec. 1995 and March 1998)
Abstract
A fundamental Lemma of Yao states that computational weak-unpredictability
of functions gets amplified if the results of several independent instances are XOR
together. We survey two known proofs of Yao's Lemma and present a third alternative proof. The third proof proceeds by first proving that a function constructed
by concatenating the values of the function on several independent instances is much
more unpredictable, with respect to specified complexity bounds, than the original
function. This statement turns out to be easier to prove than the XOR-Lemma.
Using a result of Goldreich and Levin and some elementary observation, we derive
the XOR-Lemma.
Work done in part while the authors were visiting BRICS, Basic Research in Computer Science, Center
of the Danish National Research Foundation.
Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot,
Israel. Partially supported by grant No. 92-00226 from the United States - Israel Binational Science
Foundation (BSF), Jerusalem, Israel.
Institute for Computer Science, Hebrew University,   Jerusalem, Israel.
x Institute for Computer Science, Hebrew University,   Jerusalem, Israel.
+PAGE+

Eventually-Serializable Data Services
Alan Fekete David Gupta Victor Luchangco Nancy Lynch Alex Shvartsman
Abstract
We present a new specification for distributed data services that trade-off immediate consistency guarantees
for improved system availability and efficiency, while
ensuring the long-term consistency of the data. An
eventually-serializable data service maintains the operations requested in a partial order that gravitates over
time towards a total order. It provides clear and unambiguous guarantees about the immediate and long-term
behavior of the system. To demonstrate its utility, we
present an algorithm, based on one of Ladin, Liskov,
Shrira, and Ghemawat [12], that implements this specification. Our algorithm provides the interface of the
abstract service, and generalizes their algorithm by allowing general operations and greater flexibility in specifying consistency requirements. We also describe how
to use this specification as a building block for applications such as directory services.
1 Introduction

Incoercible Multiparty Computation
(Extended Abstract)
Ran Canetti Rosario Gennaro
May 17, 1996
Abstract
Current secure multiparty protocols have the following deficiency. The public transcript of the communication can be used as an involuntary commitment of the parties to their inputs and outputs. Thus
parties can be later coerced by some authority to reveal their private data. Previous work that has
pointed this interesting problem out contained only partial treatment.
In this work we present the first general and rigorous treatment of the coercion problem in secure computation. First we present a general definition of protocols that provide resilience to coercion. Our
definition constitutes a natural extension of the general paradigm used for defining secure multiparty
protocols. Next we show that if trapdoor permutations exist then any function can be incoercibly
computed (i.e., computed by a protocol that provides resilience to coercion) in the presence of com-putationally bounded adversaries and only public communication channels. This holds as long as less
than half the parties are coerced (or corrupted). In particular, ours are the first incoercible protocols
without physical assumptions. Also, our protocols constitute an alternative solution to the recently
solved adaptive security problem.
Our techniques are quite surprising and include non-standard use of deniable encryptions.
Laboratory for Computer Science, Massachusetts Institute of Technology,    545 Technology Square, Cambridge MA 02139,
U.S.A.   canetti,rosario@theory.lcs.mit.edu
+PAGE+

Proactive RSA
Yair Frankel Peter Gemmell Philip D. MacKenzie Moti Yung x
August 4, 1996
Abstract
The notion of "proactive security" of basic primitives and cryptosystems that are distributed
amongst various servers, was introduced in order to tolerate a very strong "mobile adversary." This
adversary may corrupt all participants throughout the lifetime of the system in a non-monotonic
fashion (i.e. recoveries are possible) but the adversary is unable to corrupt too many participants
during any short time period [OstrovskyYung]. The notion assures increased security and availability
of the cryptographic primitive.
We present a proactive RSA system in which a threshold of servers applies the RSA signature
(or decryption) function in a distributed manner; RSA is perhaps the most important trapdoor
function in use. Employing new combinatorial and elementary number theoretic techniques, our
protocol enables the dynamic updating of the servers (which hold the RSA key distributively);
it is secure even when a linear number of the servers are corrupted during any time period (linear
redundancy); it efficiently "self-maintains" the security of the function and its messages (ciphertexts
or signatures); and it enables continuous availability, namely, correct function application using the
shared key is possible at any time.
We present an efficient way in which l servers can share an RSA private function so that, given
0 &lt; &lt; t &lt; 1:
* Proactive (Dynamic) Robustness: A gateway G can combine information from any set of lt
(honest) servers to deduce the RSA signature for any authorized message at any period.
* Proactive Security (against mobile adversary): Our protocol is secure against a polynomial
time adversary who controls the gateway G and time-variant sets of up to minfl(1 t ); lg
servers, and can obtain the shares of up to l servers (including those that it corrupts).
* Uniform Boundedness: The share-size is always bounded by the size of an RSA private key
(i.e., logarithmically in N ).
We also present special practical instances based on designs; some of these instances were recently
implemented as part of a highly secure application testbed at Sandia National Laboratories.
A major technical difficulty in "proactivizing" RSA was the fact that the servers have to update
the "distributed representation" of an RSA key, while not learning the order of the group from
which keys are drawn (in order not to compromise the RSA security).
Sandia National Labs,   P.O Box 5800, Albuquerque, NM 87185-1110,   yair@cs.sandia.gov
Sandia National Labs,   P.O Box 5800, Albuquerque, NM 87185-1110,   psgemme@cs.sandia.gov
Sandia National Labs,   P.O Box 5800, Albuquerque, NM 87185-1110,   philmac@cs.sandia.gov.
x IBM T. J. Watson Research Center,   Yorktown Heights, NY,   moti@watson.ibm.com, moti@cs.columbia.edu
+PAGE+

Forward and Backward Simulations
Part I: Untimed Systems
Nancy Lynch
MIT
Laboratory for Computer Science
Cambridge, MA 02139, USA
lynch@theory.lcs.mit.edu
Frits Vaandrager
CWI
P.O. Box 94079, NL-1090 GB Amsterdam
fritsv@cwi.nl
University of Amsterdam
Programming Research Group
Kruislaan 403, NL-1098 SJ Amsterdam
October 31, 1994
Abstract
A unified, comprehensive presentation of simulation techniques for verification of concurrent systems is given, in terms of a simple untimed automaton model. In particular,
(1) refinements, (2) forward and backward simulations, (3) hybrid forward-backward
and backward-forward simulations, and (4) history and prophecy relations are defined.
History and prophecy relations are abstract versions of the history and prophecy variables of Abadi and Lamport, as well as the auxiliary variables of Owicki and Gries.
Relationships between the different types of simulations, as well as soundness and
completeness results, are stated and proved. Finally, it is shown how invariants can be
incorporated into all the simulations.
Even though many results are presented here for the first time, this paper can
also be read as a survey (in a simple setting) of the research literature on simulation
techniques.
The development for untimed automata is designed to support a similar development for timed automata. In Part II of this paper, it is shown how the results of this
paper can be carried over to the setting of timed automata.
1991 Mathematics Subject Classification: 68Q60, 68Q68.
1991 CR Categories: F.1.1, F.3.1.
Keywords and Phrases: Simulations, automata, refinement mappings, forward simulations, backward simulations, forward-backward simulations, backward-forward simulations, history variables, prophecy variables, history relations, prophecy relations,
verification, invariants.
Notes: This work was supported by ONR contracts N00014-85-K-0168 and N00014-91-J-1988, by NSF grants CCR-8915206 and CCR-9225124, by DARPA contracts N00014-89-J-1988 and N00014-92-J-4033, and ONR-AFOSR contract F49620-94-1-0199.
+PAGE+

M.I.T Media Laboratory Perceptual Computing Section   Technical Report No. 307
Appears: International Conference on Computer Vision '95, Cambridge, MA, June 20-23, 1995
Facial Expression Recognition using a Dynamic Model and Motion Energy
Irfan A. Essa and Alex P. Pentland
Perceptual Computing Group, The Media Laboratory,
Massachusetts Institute of Technology
Cambridge, MA 02139, U.S.A.
Abstract
Previous efforts at facial expression recognition have
been based on the Facial Action Coding System (FACS), a
representation developed in order to allow human psychologists to code expression from static facial mugshots. In
this paper we develop new, more accurate representations
for facial expression by building a video database of facial
expressions and then probabilistically characterizing the
facial muscle activation associated with each expression
using a detailed physical model of the skin and muscles.
This produces a muscle-based representation of facial motion, which is then used to recognize facial expressions in
two different ways. The first method uses the physics-based
model directly, by recognizing expressions through comparison of estimated muscle activations. The second method
uses the physics-based model to generate spatio-temporal
motion-energy templates of the whole face for each different
expression. These simple, biologically-plausible motion
energy templates are then used for recognition. Both
methods show substantially greater accuracy at expression
recognition than has been previously achieved.
1 Introduction

M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 358
Also to appear: Springer Verlag Workshops in Computing, MIRO 95, Invited Paper, Glasgow, Sep. 95
Toward a Visual Thesaurus
Rosalind W. Picard
MIT Media Laboratory,   20 Ames St., Cambridge, MA 02139
picard@media.mit.edu,   http://www.media.mit.edu/~picard/
Abstract
A thesaurus is a book containing synonyms in a
given language; it provides similarity links when
trying to retrieve articles or stories about a particular topic. A "visual thesaurus" works with
pictures, not words. It aids in recognizing visually similar events, "visual synonyms," including
both spatial and motion similarity. This paper
describes a method for building such a tool, and
recent research results in the MIT Media Lab
which contribute toward this goal. The heart
of the method is a learning system which gathers information by interacting with a user of a
database. The learning system is also capable of
incorporating audio and other perceptual information, ultimately constructing a representation
of common sense knowledge.
1 Introduction

Multimodal Person Recognition using
Unconstrained Audio and Video
Tanzeem Choudhury, Brian Clarkson, Tony Jebara, Alex Pentland
Perceptual Computing Group
MIT Media Laboratory
Cambridge, MA 02139
ftanzeem,clarkson,jebara,sandyg@media.mit.edu
Abstract
We propose a person identification technique that
can recognize and verify people from unconstrained
video and audio. We do not expect fully frontal face
image or clean speech as our input. Our recognition algorithm can detect and compensate for pose variation
and changes in the auditory background and also select the most reliable video frame and audio clip to use
for recognition. We also use 3D depth information of
a human head to detect the presence of an actual person as opposed to an image of that person. Our system achieves 100% recognition and verification rates
on natural real-time input with 26 registered clients.
1 Introduction

Building Query Optimizers with Combinators:
Dissertation Proposal
Mitch Cherniack
Abstract
Query optimizers generate plans to retrieve data specified by queries. Query optimization
for object databases (i.e., object-oriented and object-relational databases) is an immature field,
and stands to benefit from adaptation of techniques that have proved useful for relations. One
technique uses query-to-query transformations to rewrite queries into queries that are potentially
more amenable to plan generation. For transformations to be useful, they must preserve the
semantics of the queries they rewrite (correctness) and usually result in queries that generate
better plans (effectiveness). Object databases complicate the expression of correct and effective
transformations.
Transformation correctness is problematic even for relational queries. Especially error-prone
are transformations that rewrite complex nested queries (queries containing other queries) or
queries that return duplicates. Objects make correctness more difficult because object queries
can be far more complex than relational queries.
The effectiveness of a relational transformation typically depends on the syntax of a query
rather than the semantics of of its data or functions. On the other hand, the lack of uniformity
in data functions and collections in an object query makes effectiveness more subtle. The effectiveness of a transformation for object queries may depend on the semantics of data functions,
and may vary from object to object in a collection. Therefore, optimizers may have to perform
sophisticated reasoning and apply transformations on a per object basis to ensure that they are
used only when appropriate.
This thesis considers the correctness and effectiveness of optimizer transformations. To
address correctness, we propose a formally specified query algebra and two-tiered language
(COKO-KOLA) for expressing transformations that can be verified with a theorem prover. To
address effectiveness, we propose semantic and dynamic extensions to the traditional optimizer
architecture. The high-level contribution of the thesis is the observation that the choice of
query representation impacts the quality of the optimizer. Specifically, a combinator-based
(variable-free) query representation simplifies the query manipulations that are required to make
transformations correct and effective.
1 Introduction

PARTIAL SHAPE MATCHING USING GENETIC
ALGORITHMS
Ender Ozcan and Chilukuri K. Mohan
eozcan/mohan@top.cis.syr.edu
2-120 Center for Science and Technology,
Department of Electrical Engineering and Computer Science,
Syracuse University,   Syracuse, NY 13244-4100, U.S.A.
315-443-2322/(fax) 1122
Abstract
Shape recognition is a challenging task when images contain overlapping, noisy, occluded, partial shapes.
This paper addresses the task of matching input shapes with model shapes described in terms of features
such as line segments and angles. The quality of matching is gauged using a measure derived from attributed
shape grammars. We apply genetic algorithms to the partial shape-matching task. Preliminary results, using
model shapes with 6 to 70 features each, are extremely encouraging.
Key words : Partial Shape Matching, Genetic Algorithms, Attributed Strings, Pattern Recognition.
+PAGE+

Description Logics are not just for the Flightless-Birds:
A New Look at the Utility and Foundations of Description Logics
Alex Borgida
Dept. of Computer Science
Rutgers University
New Brunswick, NJ 08904
June 1992
Abstract
This paper presents some of the underlying principles of description logics (also known
as terminological logics or kl-one-style languages), grounding them in the lattice of terms
organized by the so-called "subsumption" relationship. A survey of the increasingly varied uses
of description logics, including industrial applications, is presented by considering their role in
a number of different operations that one can apply to a knowledge base, including languages
for queries, answers, updates, rules, and constraints. Finally, we discuss some of the complexity
results related to the logics of descriptions, and survey a spectrum of responses to the many
intractability proofs.
+PAGE+

Finding pattern matchings for permutations
Louis Ibarra
Dept. of Computer Science
Hill Center, Busch Campus
Rutgers University
Piscataway, NJ 08855
ibarra@paul.rutgers.edu
January 19, 1995
Abstract
Given a permutation P of f1; : : : ; kg and T of f1; : : : ; ng, the pattern matching problem for per
mutations is to determine whether there is a length k subsequence of T whose elements are ordered
in the same way as the elements of P . We present an O(kn 4 ) time and O(kn 3 ) space algorithm
for finding a match of P into T or determining that no match exists, given that P is separable, i.e.
contains neither (2, 4, 1, 3) nor (3, 1, 4, 2) as a subpattern.
1 Introduction

On Algorithms for Simplicial Depth
Andrew Y. Cheng
Department of Industrial Engineering
Ming Ouyang
Department of Computer Science
Rutgers University
New Brunswick, New Jersey 08903
ABSTRACT
Simplicial depth is a way to measure how deep a point is among a set of points. Efficient
algorithms to compute it are important to the usefulness of its applications, such as in
multivariate analysis in statistics. A straightforward method takes O(n d+1 ) time when the
points are in d-dimensional space. We discuss an algorithm that takes O(n 2 ) time when the
points are in three-dimensional space, and we generalize it to four-dimensional space with
a time complexity of O(n 4 ). For spaces higher than four-dimensional, there are no known
algorithms faster than the straightforward method.
1 Simplicial depth

An Algorithm for Bayesian Belief Network Construction from Data
Jie Cheng, David A. Bell, Weiru Liu
School of Information and Software Engineering
University of Ulster at Jordanstown
Northern Ireland, UK, BT37 0QB
email: -j.cheng, da.bell, w.liu-@ulst.ac.uk
Abstract
This paper presents an efficient algorithm for constructing Bayesian belief networks from databases. The
algorithm takes a database and an attributes ordering (i.e., the causal attributes of an attribute should appear earlier
in the order) as input and constructs a belief network structure as output. The construction process is based on the
computation of mutual information of attribute pairs. Given a data set which is large enough and has a DAG-Isomorphic probability distribution, this algorithm guarantees that the perfect map [1] of the underlying dependency
model is generated, and at the same time, enjoys the time complexity of O N( ) 2 on conditional independence (CI)
tests. To evaluate this algorithm, we present the experimental results on three versions of the well-known ALARM
network database, which has 37 attributes and 10,000 records. The correctness proof and the analysis of
computational complexity are also presented. We also discuss the features of our work and relate it to previous
works.
1 Introduction

Planning information gathering under uncertainty
Joshua Grass and Shlomo Zilberstein
Computer Science Department
University of Massachusetts at Amherst
CMPSCI Technical Report 97-32
May 21, 1997
+PAGE+

Evaluation of Architectural Support for Global Address-Based Communication
in Large-Scale Parallel Machines
Arvind Krishnamurthy , Klaus E. Schauser , Chris J. Scheiman ,
Randolph Y. Wang , David E. Culler , and Katherine Yelick
Abstract
Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic
evaluation of a broad spectrum of current design alternatives
based on our implementations of a global address language
on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes
a range of compilation strategies that make varying use of
the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family
of interacting issues that determine the performance tradeoffs in each implementation, quantify the resulting latency,
overhead, and bandwidth of the global access operations,
and demonstrate the effects on application performance.
1 Introduction

An Improved Algorithm for Performance Optimal Technology
Mapping with Retiming in LUT-Based FPGA Design
Jason Cong and Chang Wu
Department of Computer Science
University of California,   Los Angeles, CA 90024
Abstract
A novel algorithm, named SeqMapII, of technology
mapping with retiming for optimal clock period for K-LUT based FPGAs was recently proposed by Pan and
Liu [13]. The time complexity of their algorithm, however, is O(K 3 n 4 log(Kn 2 ) log n) for sequential circuits
with n gates, which is too high for medium and large
size designs in practice. In this paper, we present
three strategies to improve the performance of the approach in [13]: 1) efficient label update with single
K-cut computation based on the monotone property
of labels that we showed for sequential circuits, 2) a
novel approach for the K-cut computation on partial
flow networks, which are much smaller in practice, 3)
SCC (strongly connected component) partition to further speedup the algorithm. In practice, our algorithm
works in O(K 2 n 3 log n) time and O(Kn) space according to our experimental results. It is 2fi10 4 times faster
than SeqMapII-opt for computing optimal solutions and
2 times faster than SeqMapII-heu which uses very small
expanded circuits as a heuristic.
1. Introduction

Extended Capabilities for
Visual Cryptography
Giuseppe Ateniese 1 , Carlo Blundo 2 , Alfredo De Santis 2 , and Douglas R. Stinson 3
1 Dipartimento di Informatica e Scienze dell'Informazione,
Universita di Genova,   via Dodecaneso 35, 16146 Genova, Italy
E-mail: ateniese@disi.unige.it
URL: http://www.disi.unige.it/phd/ateniese/ateniese.html
2 Dipartimento di Informatica ed Applicazioni,
Universita di Salerno,   84081 Baronissi (SA), Italy
E-mail: fcarblu,adsg@dia.unisa.it
URL: http://www.unisa.it/f~carblu, ~adsg
3 Department of Combinatorics and Optimization
University of Waterloo,   Waterloo Ontario, N2L 3G1, Canada
Abstract
An extended visual cryptography scheme, EVCS for short, for an access structure
( Qual ; Forb ) on a set of n participants, is a technique to encode n images in such a
way that when we stack together the transparencies associated to participants in any
set X 2 Qual we get the secret message with no trace of the original images, but any
X 2 Forb has no information on the shared image. Moreover, after the original images
are encoded they are still meaningful, that is, any user will recognize the image on his
transparency.
The main contributions of this paper are the following:
* A trade-off between the contrast of the reconstructed image and the contrast of the
image on each transparency for (k; k)-threshold EVCS (in a (k; k)-threshold EVCS
the image is visible if and only if k transparencies are stacked together). This yields
a necessary and sufficient condition for the existence of (k; k)-threshold EVCS for
the values of such contrasts. In case a scheme exists we explicitly construct it.
* A general technique to implement extended visual cryptography schemes, which
uses hypergraph colourings. This technique yields (k; k)-threshold EVCS which
are optimal with respect to the pixel expansion. Finally, we discuss some applications of this technique to various interesting classes of access structures by using
relevant results from the theory of hypergraph colourings.
Keywords: Visual Cryptography, Secret Sharing Schemes.
1 Introduction

Intelligent Sensors for Atomization Processing
of Molten Metals and Alloys
* G. Jiang
* H. Henein
and
** M.W. Siegel
(*) Department of Metallurgical Engineering and Materials Science
(**) The Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213
+PAGE+

Final Version,   94-Mar-17,   1 of 9
Automation Tools for
NonDestructive Inspection of Aircraft:
Promise of Technology Transfer from the
Civilian to the Military Sector
Chris Seher 1 , Mel Siegel 2 , and William M. Kaufman 3
1 Federal Aviation Administration, Technical Center,   Atlantic City NJ 08201
2 Carnegie Mellon University, Robotics Institute,   Pittsburgh PA 15213
3 Carnegie Mellon University, CMRI,   Pittsburgh PA 15213
Abstract
The FAA Aging Aircraft Research Program is
supporting the development of a robotic mobile
nondestructive inspection (NDI) instrument
deployment tool at Carnegie Mellon University
(CMU) with the active participation of USAir. The
program has spawned several new relationships
and entities: an alliance with an ARPA-funded
research program at CMU having the capability to
add 3D-stereoscopic enhanced visual inspection
capability, a start-up company organized to
commercialize the combined technologies, and
State of Pennsylvania funding to foster this
commercialization. As a result of these activities
and connections the civilian sector appears to be
ahead of the military sector in important aspects of
automation for deployment of aircraft inspection
equipment. A partnership between the university
researchers, the airline operator, the start-up
company, and the state government is thus
emerging as the likely agent for transfer of the
civilian-developed technology to the military sector.
1. Introduction

CARNEGIE MELLON UNIVERSITY
EFFICIENT COMPRESSION OF ARBITRARY
MULTI-VIEW VIDEO SIGNALS
A DISSERTATION
SUBMITTED TO THE GRADUATE SCHOOL
IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
for the degree
DOCTOR OF PHILOSOPHY
in
ELECTRICAL AND COMPUTER ENGINEERING
by
Jeffrey Scott McVeigh
Pittsburgh, Pennsylvania
June, 1996
+PAGE+

Feature selection for classification based on text hierarchy
Dunja Mladenic and Marko Grobelnik
Department of Intelligent Systems, J.Stefan Institute,
Jamova 39, 1111 Ljubljana, Slovenia
Phone: (+386)(61) 1773 272, Fax: (+386)(61) 1258-158
E-mail: Dunja.Mladenic@ijs.si, Marko.Grobelnik@ijs.si
Abstract
This paper describes automatic document categorization based on large text hierarchy. We
handle the large number of features and training examples by taking into account hierarchical
structure of examples and using feature selection for large text data. We experimentally evaluate
feature subset selection on real-world text data collected from the existing Web hierarchy named
Yahoo. In our learning experiments naive Bayesian classifier was used on text data using feature-vector document representation that includes word sequences (n-grams) instead of just single words
(unigrams). Experimental evaluation on real-world data collected form the Web shows that our
approach gives promising results and can potentially be used for document categorization on the
Web. Additionally the best result on our data is achieved for relatively small feature subset, while for
larger subset the performance substantially drops. The best performance among six tested feature
scoring measure was achieved by the feature scoring measure called Odds ratio that is known from
information retrieval.
1 Introduction

Unification and Polymorphism in Region Inference
Mads Tofte,   Department of Computer Science, University of Copenhagen
Lars Birkedal,   School of Computer Science, Carnegie Mellon University
Dedicated to Robin Milner on the occasion of his 60th birthday.
Abstract
Region Inference is a technique for inferring lifetimes of values in strict, higher-order programming languages such as Standard ML. The purpose of this paper is to show how ideas
from Milner's polymorphic type discipline can serve as a basis for region inference, even in the
presence of a limited form of polymorphic recursion.
1 Introduction

An Introduction to Software Architecture
David Garlan and Mary Shaw
January 1994
CMU-CS-94-166
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213-3890
Also published as An Introduction to Software Architecture, Advances in Software Engineering
and Knowledge Engineering, Volume I, edited by V.Ambriola and G.Tortora, World Scientific
Publishing Company, New Jersey, 1993.
Also appears as CMU Software Engineering Institute Technical Report
CMU/SEI-94-TR-21, ESC-TR-94-21.
1994 by David Garlan and Mary Shaw
This work was funded in part by the Department of Defense Advanced Research Project Agency under grant
MDA972-92-J-1002, by National Science Foundation Grants CCR-9109469 and CCR-9112880, and by a grant
from Siemens Corporate Research. It was also funded in part by the Carnegie Mellon University School of
Computer Science and Software Engineering Institute (which is sponsored by the U.S. Department of Defense).
The views and conclusions contained in this document are those of the authors and should not be interpreted
as representing the official policies, either expressed or implied, of the U.S. Government, the Department of
Defense, the National Science Foundation, Siemens Corporation, or Carnegie Mellon University.
Keywords: Software architecture, software design, software engineering
+PAGE+

An Agenda for Research in
Large-Scale Distributed Data Repositories
M. Satyanarayanan
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Invited Paper for
Workshop on Operating Systems of the 90s and Beyond
Dagstuhl Castle, Germany,   July 1991
Abstract
Access to shared data is provided today by distributed file systems and
databases. In this paper, we explore certain usage and technological trends that
will radically change the way shared data is used in the future. The usage trends
include the growing need to access shared data from anywhere, increasing scale,
and the increasing importance of efficient search. The technology trends include
the advent of portable machines, the availability of software and hardware for
using diverse types of data, and the growing diversity of network speeds and
capabilities. These trends induce fundamental research problems in the areas of
adaptive system behavior, secure remote execution and extensibility.
This work has been supported by the Defense Advanced Research Projects Agency (Avionics Lab, Wright Research and Development Center,
Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio, 45433-6543 under Contract F33615-90-C-1465, ARPA
Order No. 7597), the National Science Foundation (PYI Award), and the IBM Corporation (Research Initiation Grant). The views and
conclusion expressed in this paper are those of the author, and should not be interpreted as those of the funding agencies or Carnegie Mellon
University.
+PAGE+

To appear in the Proceedings of the 16th ACM Symposium on Operating System Principles
Agile Application-Aware Adaptation for Mobility
Brian D. Noble, M. Satyanarayanan, Dushyanth Narayanan, James Eric Tilton, Jason Flinn, Kevin R. Walker
School of Computer Science
Carnegie Mellon University
Abstract
In this paper we show that application-aware adaptation, a
collaborative partnership between the operating system and
applications, offers the most general and effective approach
to mobile information access. We describe the design of
Odyssey, a prototype implementing this approach, and show
how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it.
We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a
benchmark of three applications concurrently using remote
services over a network with highly variable bandwidth.
1 Introduction

Automatic Program Specialization for
Interactive Media
Scott Draves
July 23, 1997
CMU-CS-97-159
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Submitted in partial fulfillment of the requirements
for the degree of Doctor of Philosophy.
Thesis Committee:
Peter Lee, Chair
William Scherlis
Andy Witkin
Olivier Danvy
c fl1997 Scott Draves
This research was sponsored in part by the Defense Advanced Research Projects Agency
CSTO under the title The Fox Project: Advanced Languages for Systems Software, ARPA Order No. C533, issued by ESC/ENS under Contract No. F19628-95-C-0050. The views and
conclusions contained in this document are those of the authors and should not be interpreted as
representing the official policies, either expressed or implied, of the Defense Advanced Research
Projects Agency or the U.S. Government.
+PAGE+

RECENT ADVANCES IN JANUS:
A SPEECH TRANSLATION SYSTEM
M.Woszczyna, N.Coccaro, A.Eisele, A.Lavie, A.McNair, T.Polzin, I.Rogina,
C.P.Rose,T.Sloboda, M.Tomita, J.Tsutsumi, N.Aoki-Waibel, A.Waibel, W. Ward
Carnegie Mellon University
University of Karlsruhe
ABSTRACT
We present recent advances from our efforts in increasing coverage, robustness, generality and speed of JANUS, CMU's
speech-to-speech translation system. JANUS is a speaker-independent system which translates spoken utterances in
English and also in German into one of German, English or
Japanese. The system has been designed around the task
of conference registration (CR). It has initially been built
based on a speech database of 12 read dialogs, encompassing a vocabulary of around 500 words. We have since been
expanding the system along several dimensions to improve
speed, robustness and coverage and to move toward spontaneous input.
1. INTRODUCTION

AURORA: A Vision-Based Roadway
Departure Warning System
Mei Chen, Todd Jochem, Dean Pomerleau
-meichen, tjochem, pomerlea-@ri.cmu.edu
The Robotics Institute, Carnegie Mellon University,   Pittsburgh PA 15213
Abstract
AURORA is a vision-based system designed to warn a vehicle driver of possible impending roadway departure accidents. It employs a downward looking color video camera with a wide angle
lens, a digitizer, and a portable Sun Sparc workstation. Using a binormalized adjustable template
correlation algorithm, it reliably detects lane markers on structured roads at 60 Hz. A time-to-lane-crossing (TLC) measurement is calculated for each image based on the estimation of vehicles lateral position and velocity. This measurement is used to trigger an alarm when the TLC falls below
a preset threshold. Promising results have been achieved under a variety of weather and lighting
conditions, on many road types.
1. Introduction

A Combinatorial Approach to Trajectory Planning for Binary
Manipulators
David S. Lees
Gregory S. Chirikjian
Department of Mechanical Engineering, Johns Hopkins University,   Baltimore, MD 21218
Abstract
Binary manipulators are powered by actuators
which have only two stable states. Therefore, they
can reach only a discrete (but possibly large) number
of locations. Compared to a manipulator built with
continuous actuators, a binary manipulator provides
reasonable performance, and is relatively inexpensive
(up to an order of magnitude cheaper). The number
of states of a binary manipulator grows exponentially
with the number of actuators. This makes the calculation of its inverse kinematics quite difficult. This
paper presents a combinatorial method for computing
the inverse kinematics of a binary manipulator that
reduces the search space to a manageable size. It also
creates extremely smooth motions that follow a specified trajectory very accurately (in both position and
orientation), despite the discrete nature of binary actuation.
1 Introduction

An Overview of a Compiler for
Scalable Parallel Machines
Saman P. Amarasinghe, Jennifer M. Anderson,
Monica S. Lam and Amy W. Lim
Computer Systems Laboratory
Stanford University,   CA 94305
Abstract. This paper presents an overview of a parallelizing compiler
to automatically generate efficient code for large-scale parallel architectures from sequential input programs. This research focuses on loop-level
parallelism in dense matrix computations. We illustrate the basic techniques the compiler uses by describing the entire compilation process for
a simple example.
Our compiler is organized into three major phases: analyzing array references, allocating the computation and data to the processors to optimize
parallelism and locality, and generating code.
An optimizing compiler for scalable parallel machines requires more sophisticated program analysis than the traditional data dependence analysis. Our compiler uses a precise data-flow analysis technique to identify
the producer of the value read by each instance of a read access. In order to allocate the computation and data to the processors, the compiler
first transforms the program to expose loop-level parallelism in the computation. It then finds a decomposition of the computation and data
such that parallelism is exploited and the communication overhead is
minimized. The compiler will trade off extra degrees of parallelism to
reduce or eliminate communication. Finally, the compiler generates code
to manage the multiple address spaces and to communicate data across
processors.
1 Introduction

CC
++ : A Declarative Concurrent
Object Oriented Programming Notation
K. Mani Chandy Carl Kesselman
California Institute of Technology
September 18, 1992
Abstract
CC ++ is Compositional C ++ , a parallel object-oriented notation
that consists of C ++ with six extensions. The goals of the CC ++
project are to provide a theory, notation and tools for developing reliable scalable concurrent program libraries, and to provide a framework
for unifying:
1. distributed reactive systems, batch-oriented numeric and sym
bolic applications, and user-interface systems,
2. declarative programs and object-oriented imperative programs,
and
3. deterministic and nondeterministic programs.
This paper is a brief description of the motivation for CC ++ , the
extensions to C ++ , a few examples of CC ++ programs with reasoning
about their correctness, and an evaluation of CC ++ in the context of
other research on concurrent computation. A short description of
C ++ is provided.
1 Introduction

Distributed Simulation of DEVS-Based Multiformalism Models
Herbert Praehofer and Gernot Reisinger
Institute of Systems Science
Systems Theory and Information Engineering
Johannes Kepler University Linz
A-4040 Linz, Austria
Abstract
In this paper we introduce a new approach for parallel, distributed simulation of modular, hierarchical
DEVS and DEVS-based combined discrete/continuous
multiformalism models. The algorithm combines
conservative and optimistic distributed simulation
strategies and is able to optimally exploit lookahead
capabilities of the model. The object oriented implementation in C++ is intended to serve as a powerful
simulator in the STIMS modeling and simulation environment.
1 Introduction and Motivation

Building Scalable Parallel Processors Using
Networked Computers - A Tutorial For
Synergy V2.0
Yuan Shi
January 1994
@1994   Temple University
SYNERGY
+PAGE+

Tolerating Latency with Dagger
Attila Gursoy and L.V.Kale
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana IL 61801, USA
fgursoy,kaleg@cs.uiuc.edu
Abstract
The communication latency is a major issue that must be dealt with in parallel
computing. The parallel computation model therefore must provide the ability to tolerate
such latencies. Communication using blocking receives is the commonly used mechanism
in parallel programming today. Message driven execution is an alternate mechanism
which does not use receive style statements at all. The message driven execution style
promotes the overlap of computation and communication: Programs written in this style
exhibit increased latency tolerance. However, they are often difficult to develop and
debug. We present a coordination language called Dagger to alleviate this problem. The
language has a mechanism which is called expect, that replaces the receive statement.
It has been implemented in the Charm parallel programming system, and runs programs
portably on a variety of parallel machines.
1. INTRODUCTION

Projections: A Preliminary Performance Tool for Charm
Amitabh B. Sinha Laxmikant V. Kale
Department of Computer Science Department of Computer Science
University of Illinois University of Illinois
Urbana, IL 61801 Urbana, IL 61801
email: sinha@cs.uiuc.edu email: kale@cs.uiuc.edu
Abstract
The advent and acceptance of massively parallel
machines has made it increasingly important to have
tools to analyze the performance of programs running on these machines. Current day performance
tools suffer from two drawbacks: they are not scalable
and they lose specific information about the user program in their attempt for generality. In this paper,
we present Projections, a scalable performance tool,
for Charm that can provide program-specific information to help the users better understand the behavior
of their programs.
1 Introduction

Inference Networks for Document Retrieval
A Dissertation Presented
by
Howard Robert Turtle
Submitted to the Graduate School of the
University of Massachusetts   in partial fulfillment
of the requirements for the degree of
Doctor of Philosophy
February 1991
Department of Computer and Information Science
+PAGE+

Comparison of Distributed Concurrency Control Protocols
on a Distributed Database Testbed
Chia-Shiang Shih and Asit Dan
ECE Department, University of Massachusetts
Amherst, MA 01003
Walter H. Kohler
Digital Equipment Corporation
200 Forest Street, MRO1-1/A65
Marlboro, MA 01752-9101
John A. Stankovic and Don Towsley
COINS Department, University of Massachusetts
Amherst, MA 01003
This work was supported by the National Science Foundation, grant number SDB-8418216 and by a grant from
Digital Equipment Corporation.
Walter H. Kohler is manager of High Performance Transaction Processing System group at Digital Equipment
Corporation.
+PAGE+

Resolving Ambiguity for Cross-language Retrieval
Lisa Ballesteros
balleste@cs.umass.edu
Center for Intelligent Information Retrieval
Computer Science Department
University of Massachusetts
Amherst, MA 01003-4610 USA
http://ciir.cs.umass.edu/
W. Bruce Croft
croftcs.umass.edu
Center for Intelligent Information Retrieval
Computer Science Department
University of Massachusetts
Amherst, MA 01003-4610 USA
http://ciir.cs.umass.edu/
Abstract One of the main hurdles to improved CLIR effectiveness is resolving ambiguity associated with translation.
Availability of resources is also a problem. First we present a
technique based on co-occurrence statistics from unlinked corpora which can be used to reduce the ambiguity associated with
phrasal and term translation. We then combine this method
with other techniques for reducing ambiguity and achieve more
than 90% monolingual effectiveness. Finally, we compare the
co-occurrence method with parallel corpus and machine translation techniques and show that good retrieval effectiveness can
be achieved without complex resources.
1 Introduction

On computing global similarity in images
S. Ravela and R. Manmatha
Computer Science Department
University of Massachusetts,   Amherst, MA 01003
Email: fravela,manmathag@cs.umass.edu
Abstract
The retrieval of images based on their visual similarity
to an example image is an important and fascinating area of
research. Here, a method to characterize visual appearance
for determining global similarity in images is described.
Images are filtered with Gaussian derivatives and geometric features are computed from the filtered images.
The geometric features used here are curvature and phase.
Two images may be said to be similar if they have similar distributions of such features. Global similarity may,
therefore, be deduced by comparing histograms of these
features. This allows for rapid retrieval and examples from
collection of gray-level and trademark images are shown.
1 Introduction

Kinetic Binary Space Partitions for
Intersecting Segments and Disjoint Triangles
(Extended Abstract)
Pankaj K. Agarwal Jeff Erickson Leonidas J. Guibas
Abstract
We describe randomized algorithms for efficiently maintaining a binary space partition of continuously moving, possibly
intersecting, line segments in the plane, and of continuously
moving but disjoint triangles in space. Our two-dimensional
BSP has depth O(log n) and size O(n log n + k) and can be
constructed in expected O(n log 2 n + k log n) time, where k
is the number of intersecting pairs. We can detect combinatorial changes to our BSP caused by the motion of the segments, and we can update our BSP in expected O(log n) time
per change. Our three-dimensional BSP has depth O(log n),
size O(n log 2 n+k 0 ), construction time O(n log 3 n+k 0 log n),
and update time O(log 2 n) (all expected), where k 0 is the
number of intersections between pairs of edges in the xy-projection of the triangles. Under reasonable assumptions
about the motion of the segments or triangles, the expected
number of number of combinatorial changes to either BSP is
O(mn s (n)), where m is the number of moving objects and
s (n) is the maximum length of an (n; s) Davenport-Schinzel
sequence for some constant s.
1 Introduction

Raising Roofs, Crashing Cycles, and Playing Pool:
Applications of a Data Structure for Finding Pairwise Interactions
David Eppstein Jeff Erickson
Submitted to Discrete & Computational Geometry
July 1, 1998
Abstract
The straight skeleton of a polygon is a variant of the medial axis, introduced by
Aichholzer et al., defined by a shrinking process in which each edge of the polygon
moves inward at a fixed rate. We construct the straight skeleton of an n-gon with r
reflex vertices in time O(n 1+" + n 8=11+" r 9=11+" ), for any fixed " &gt; 0, improving the
previous best upper bound of O(nr log n). Our algorithm simulates the sequence of
collisions between edges and vertices during the shrinking process, using a technique of
Eppstein for maintaining extrema of binary functions to reduce the problem of finding
successive interactions to two dynamic range query problems: (1) maintain a changing
set of triangles in IR 3 and answer queries asking which triangle would be first hit by
a query ray, and (2) maintain a changing set of rays in IR 3 and answer queries asking
for the lowest intersection of any ray with a query triangle. We also exploit a novel
characterization of the straight skeleton as a lower envelope of triangles in IR 3 . The same
time bounds apply to constructing non-self-intersecting offset curves with mitered or
beveled corners, and similar methods extend to other problems of simulating collisions
and other pairwise interactions among sets of moving objects.
An extended abstract of this paper was presented at the 14th Annual ACM Symposium on Computational
Geometry [29]. See http://www:cs:duke:edu/ ~ jeffe/pubs/cycles:html for the most recent version of this paper.
Department of Information and Computer Science, University of California,   Irvine, CA 92697, USA;   epp-stein@ics.uci.edu;   http://www:ics:uci:edu/ ~ eppstein.   Research partially supported by NSF grant CCR-9258355 and
by matching funds from Xerox Corporation.
Center for Geometric Computing, Department of Computer Science, Duke University,   Box 90129, Durham,
NC 27708-0129, USA;   jeffe@cs.duke.edu;   http://www:cs:duke:edu/ ~ jeffe.   Research supported by NSF grant DMS-9627683 and by U. S. Army Research Office MURI grant DAAH04-96-1-0013.
+PAGE+

Transis: A Communication Sub-System
for
High Availability
Yair Amir, Danny Dolev, Shlomo Kramer, Dalia Malki
Computer Science department
The Hebrew University of Jerusalem
Jerusalem, Israel
Technical Report CS91-13
April 30, 1992
+PAGE+

From Ordinal to Euclidean Reconstruction with Partial Scene
Calibration
Daphna Weinshall
Inst. of Computer Sci.
Hebrew University
91904 Jerusalem, Israel
daphna@cs.huji.ac.il
P. Anandan
Microsoft Research
One Microsoft Way
Redmond, WA 98052
anandan@microsoft.com
Micahl Irani
Dept. of Appl. Math and CS
The Weizmann Inst. of Sci.
Rehovot, Israel
irani@wisdom.weizmann.ac.il
Abstract
Since uncalibrated images permit only projective reconstruction, metric information requires either
camera or scene calibration. We propose a stratified approach to projective reconstruction, in which
gradual increase in domain information for scene calibration leads to gradual increase in 3D information.
Our scheme includes the following steps: (1) Register the images with respect to a reference plane; this
can be done using limited scene information, e.g., the knowledge that two pairs of lines on the plane are
parallel. We show that this calibration is sufficient for ordinal reconstruction sorting the points by their
height over the reference plane. (2) If available, use the relative height of two additional out-of-plane
points to compute the height of the remaining points up to constant scaling. Our scheme is based on the
dual epipolar geometry in the reference frame, which we develop below. We show good results with five
sequences of real images, using mostly scene calibration that can be inferred directly from the images
themselves.
Keywords: projective reconstruction, affine reconstruction, partial calibration, qualitative depth
1 Introduction

Iterative Optimization and Simplification of
Hierarchical Clusterings
Technical Report CS-95-01
Doug Fisher
Department of Computer Science
Box 1679, Station B
Vanderbilt University
Nashville, TN 37235
dfisher@vuse.vanderbilt.edu
http://www.vuse.vanderbilt.edu/~dfisher/dfisher.html
(615) 343-4111
Abstract: Clustering is often used for discovering structure in data. Clustering systems
differ in the objective function used to evaluate clustering quality and the control strategy
used to search the space of clusterings. Ideally, the search strategy should consistently
construct clusterings of high quality, but be computationally inexpensive as well. In general,
we cannot have it both ways, but we can partition the search so that a system inexpensively
constructs a `tentative' clustering for initial examination, followed by iterative optimization,
which continues to search in background for improved clusterings. Given this motivation, we
evaluate an inexpensive strategy for creating initial clusterings, coupled with several control
strategies for iterative optimization, each of which repeatedly modifies an initial clustering
in search of a better one. One of these methods appears novel as an iterative optimization
strategy in clustering contexts. Once a clustering has been constructed it is judged by
analysts often according to task-specific criteria. Several authors have abstracted these
criteria and posited a generic performance task akin to pattern completion, where the error
rate over completed patterns is used to `externally' judge clustering utility. Given this
performance task we adapt resampling-based pruning strategies used by supervised learning
systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and
simplicity dimensions.
Keywords: clustering, iterative optimization, cluster validation, resampling, pruning, objective functions.
+PAGE+

On the Analysis of Indexing Schemes
Joseph M. Hellerstein
Division of Computer Science
UC Berkeley,   Berkeley, CA 94720
jmh@cs.berkeley.edu
Elias Koutsoupias
Department of Computer Science
UCLA,   Los Angeles, CA 90095
elias@cs.ucla.edu
Christos H. Papadimitriou
Division of Computer Science
UC Berkeley,   Berkeley, CA 94720
christos@cs.berkeley.edu
Abstract
We consider the problem of indexing general database
workloads (combinations of data sets and sets of potential queries). We define a framework for measuring the
efficiency of an indexing scheme for a workload based on
two characterizations: storage redundancy (how many
times each item in the data set is stored), and access
overhead (how many times more blocks than necessary
does a query retrieve). Using this framework we present
some initial results, showing upper and lower bounds
and trade-offs between them in the case of multi-dimensional range queries and set queries.
1 Introduction

-- IBROW3 --
An Intelligent Brokering Service for
Knowledge-Component Reuse on the
World-Wide Web
V. Richard Benjamins 3,4 , Enric Plaza 4 , Enrico Motta 2 , Dieter Fensel 1 , Rudi Studer 1 , Bob Wielinga 3 ,
Guus Schreiber 3 , Zdenek Zdrahal 2 and Stefan Decker 1
1 University of Karlsruhe,   Institute AIFB, 76128 Karlsruhe, Germany,
-dfe,studer-@aifb.uni-karlsruhe.de
2 Knowledge Media Institute, The Open University,   Walton Hall, Milton Keynes, United Kingdom,
-E.Motta,Z.Zdrahal-@open.ac.uk
3 Dept. of Social Science Informatics (SWI), University of Amsterdam,    Roetersstraat 15, 1018 WB
Amsterdam, The Netherlands,   -richard,schreiber,wielinga-@swi.psy.uva.nl
4 Artificial Intelligence Research Institute (IIIA), Spanish Council for Scientific Research (CSIC),
Campus UAB, 08193 Bellaterra, Barcelona, Spain,   enric@iiia.csic.es
The World-Wide Web is changing the nature of software development to a distributive
plug & play process. This requires a new way of managing software by so-called intelligent
software brokers. The aim of the European IBROW3 project is to develop an intelligent
brokering service that enables third party knowledge-component reuse through the
World-Wide Web. Suppliers provide libraries of knowledge components adhering to some
standard, and customers can consult these libraries -- through intelligent brokers -- to
configure a knowledge system suited to their needs by selection and adaptation. IBROW3
integrates research on heterogeneous databases, interoperability and web technology with
knowledge-system technology and ontologies. The aim is to develop a broker that can handle
web requests for classes of knowledge system (e.g. diagnostic systems) by accessing
libraries of reusable problem-solving methods on the Web, and selecting, adapting and
configuring these methods in accordance with the domain at hand.
The aim of this paper is to give a general overview of the project and to presents its main
ideas and approach. IBROW3 has started on January 1, 1998 and thus we can only present
preliminary results.
1 Introduction

DIMACS Technical Report 93-8
February 1993
Hilbert Series of Group Representations and
Grobner Bases for Generic Modules
by
Shmuel Onn 1
DIMACS
Rutgers University
Piscataway, New Jersey 08855-1179
E-mail: onn@dimacs.rutgers.edu
1 Research was supported by the Mittag-Le*er Institute, by the Mathematical Sciences Institute
at Cornell University, and by the Center for Discrete Mathematics and Theoretical Computer
Science at Rutgers University.
DIMACS is a cooperative project of Rutgers University, Princeton University, AT&T Bell
Laboratories and Bellcore.
DIMACS is an NSF Science and Technology Center, funded under contract STC-88-09648;
and also receives support from the New Jersey Commission on Science and Technology.
+PAGE+

Finding a Shortest Diagonal of a Simple Polygon in Linear
Time
John Hershberger
DEC/SRC
130 Lytton Avenue,
Palo Alto, California 94301
Subhash Suri
Bellcore
445 South Street,
Morristown, New Jersey 07960
August 16, 1993
Abstract
A diagonal of a planar, simple polygon P is an open line segment that connects
two non-adjacent vertices and lies in the relative interior of P . We present a linear
time algorithm for finding a shortest diagonal (in the L 2 norm) of a simple polygon,
improving the previous best result by a factor of log n. Our result provides an interesting
contrast to a known (n log n) lower bound for finding a closest pair of vertices in a
simple polygon|observe that a shortest diagonal is defined by a closest pair of vertices
satisfying an additional visibility constraint.
+PAGE+

A Randomized Linear-Time Algorithm for Finding Minimum Spanning
Trees
Philip N. Klein
Robert E. Tarjan
October 12, 1993
Abstract
We present a randomized linear-time algorithm for finding a minimum spanning tree in a connected
graph with edge weights. The algorithm is a modification of one proposed by Karger and uses random
sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the
only operations allowed on edge weights are binary comparisons.
1 Introduction

Markov Chain Algorithms for Planar Lattice Structures
(Extended Abstract)
Michael Luby Dana Randall Alistair Sinclair
Abstract
Consider the following Markov chain, whose states
are all domino tilings of a 2n fi 2n chessboard: starting from some arbitrary tiling, pick a 2 fi 2 window
uniformly at random. If the four squares appearing in
this window are covered by two parallel dominoes, rotate the dominoes in place. Repeat many times. This
process is used in practice to generate a random tiling,
and is a key tool in the study of the combinatorics of
tilings and the behavior of dimer systems in statistical physics. Analogous Markov chains are used to
randomly generate other structures on various two-dimensional lattices. This paper presents techniques
which prove for the first time that, in many interesting cases, a small number of random moves suffice to
obtain a uniform distribution.
1 Introduction

HIGH PERFORMANCE IMPLEMENTATION
OF SERVER DIRECTED I/O
BY
MAHESH SUBRAMANIAM
B.E, Birla Institute of Technology & Science, 1993
M.Sc, Birla Institute of Technology & Science, 1993
THESIS
Submitted in partial fulfillment of the requirements
for the degree of Master of Science in Computer Science
in the Graduate College of the
University of Illinois at Urbana-Champaign,   1996
Urbana, Illinois
+PAGE+

Constraint Based Design of ATM
Networks, an Experimental Study
Hongzhou Ma, Inderjeet Singh, Jonathan Turner
wucs-97-15
April 97
Department of Computer Science
Campus Box 1045
Washington University
One Brookings Drive
St. Louis, MO 63130-4899
Abstract
This paper describes an experimental study of constraint-based network design. We used a
novel network design tool, implemented in Java, to design representative networks joining
major U.S. cities. The cost of three topologies: Best Star, Minimum Spanning Tree (MST),
and Delaunay Triangulation, are compared, with and without localized traffic constraints.
The best star network gives near optimal result when the traffic is only constrained by source
and sink capacity of switches (flat traffic constraints). With localized traffic constraints, the
most cost effective network has a structure similar to the MST. The cheapest network has a
tree structure when there are only flat traffic constraints, but can have cycles when localized
traffic constraints are present.
+PAGE+

Efficient Scheduling of Branching Computations on Rings of
Processors: An Empirical Study
Lixin Gao Dawn E. Gregory Arnold L. Rosenberg Paul R. Cohen
Department of Computer Science
University of Massachusetts
Amherst, Mass. 01003, USA
fgao,gregory,rsnbrg,coheng@cs.umass.edu
Abstract
We empirically analyze and compare two simple, deterministic policies for scheduling dynamically evolving tree-structured computations on parallel architectures that are rings of identical
processing elements (PEs). Our computations have each task either halt or spawn two independent children; they abstract, for instance, computations generated by multigrid methods. Our
simpler policy, called koso, has each PE keep one child of a spawning task and pass the other
to its clockwise neighbor in the ring; our more sophisticated policy, called koso ? , operates similarly, but allows child-passing only from a more heavily loaded PE to a more lightly loaded one.
Both policies execute waiting tasks in increasing order of their depths in the evolving task-tree.
Based on partial (mathematical) analyses of our policies' behaviors, we conjectured that both
yield good parallel speedup on large classes of the computations we study, but that policy
koso ? outperforms policy koso in many important situations. Not having been able to verify
these conjectures analytically, we study them in this paper via a suite of carefully designed and
analyzed experiments. Our experiments largely substantiate both of our conjectures. We find
that both policies give close to optimal parallel speedup on large classes of computations, and
that koso ? outperforms koso on these computations, except on very small processor rings.
We believe that our methodology of experimental design and analysis will prove useful in other
such studies.
1 Introduction

Using Real-Valued Genetic Algorithms to Evolve Rule Sets for
Classification
Arthur L. Corcoran Sandip Sen
Abstract| In this paper, we use a genetic algorithm to evolve a set of classification rules with
real-valued attributes. We show how real-valued
attribute ranges can be encoded with real-valued
genes and present a new uniform method for representing don't cares in the rules. We view supervised classification as an optimization problem,
and evolve rule sets that maximize the number of
correct classifications of input instances. We use a
variant of the Pitt approach to genetic-based machine learning system with a novel conflict resolution mechanism between competing rules within
the same rule set. Experimental results demonstrate the effectiveness of our proposed approach
on a benchmark wine classifier system.
I. Introduction

The Interaction of
Architecture and Operating System Design
Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
Abstract
Today's high-performance RISC microprocessors have been
highly tuned for integer and floating point application performance. These architectures have paid less attention to
operating system requirements. At the same time, new operating system designs often have overlooked modern architectural trends which may unavoidably change the relative
cost of certain primitive operations. The result is that operating system performance is well below application code
performance on contemporary RISCs.
This paper examines recent directions in computer architecture and operating systems, and the implications of
changes in each domain for the other. The requirements of
three components of operating system design are discussed
in detail: interprocess communication, virtual memory, and
thread management. For each component, we relate operating system functional and performance needs to the mechanisms available on commercial RISC architectures such as
the MIPS R2000 and R3000, Sun SPARC, IBM RS6000,
Motorola 88000, and Intel i860.
Our analysis reveals a number of specific reasons why
the performance of operating system primitives on RISCs
has not scaled with integer performance. In addition,
we identify areas in which architectures could better (and
cost-effectively) accommodate operating system needs, and
areas in which operating system design could accommodate certain necessary characteristics of cost-effective high-performance microprocessors.
This work was supported in part by the National Science
Foundation under Grants No. CCR-8703049, CCR-8619663, and
CCR-8907666, by the Washington Technology Center, by the Digital Equipment Corporation Systems Research Center and External Research Program, and by IBM and AT&T Fellowships.
Bershad is now with the School of Computer Science, Carnegie
Mellon University.
1 Introduction

A Multi-Agent Referral System for Matchmaking
Leonard N. Foner
MIT Media Lab
20 Ames St, E15-305
Cambridge, MA 02139
617/253-9601
ABSTRACT
Many important and useful applications for software agents
require multiple agents on a network that communicate with
each other. Such agents must find each other and perform a
useful joint computation without having to know about every
other such agent on the network. This paper describes a
matchmaker
system, designed to find people with similar interests and introduce them to each other. The matchmaker is
designed to introduce
everyone
, unlike conventional Internet
media which only allow those who take the time to
speak
public to be known.
The paper details how the agents that make it up the match-making system can function in a decentralized fashion, yet
can group themselves into clusters which reflect their users
interests; these clusters are then used to make introductions or
allow users to send messages to others who share their inter
ests. The algorithm uses
referrals
from one agent to another
in the same fashion that word-of-mouth is used when people
are looking for an expert. A prototype of the system has been
implemented, and results of its use are presented.
KEYWORDS:
agents, collaborative filtering, CSCW, joint
computation, ecology of computation, user modeling, intelli
gent systems, information retrieval, distributed AI, Internet.
INTRODUCTION

Translucent Sums: A Foundation for
Higher-Order Module Systems
Mark Lillibridge
May, 1997
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Submitted in partial fulfillment of the requirements
for the degree of Doctor of Philosophy.
Thesis Committee:
Robert Harper, Chair
Peter Lee
John Reynolds
Luca Cardelli, DEC SRC
Copyright c fl1997 Mark Lillibridge
This research was sponsored by the Air Force Materiel Command (AFMC) and the Defense Advanced Research Projects Agency (DARPA) under contract number, F19628-95-C-0050. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding
any copyright notation thereon.
The views and conclusions contained in this document are those of the author and should not be
interpreted as representing the official policies or endorsements, either expressed or implied, of the U.S.
Government.
+PAGE+

Optimizing ML with Run-Time Code Generation
Peter Lee Mark Leone
School of Computer Science
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213-3891
petel@cs.cmu.edu mleone@cs.cmu.edu
Abstract
We describe the design and implementation of a compiler
that automatically translates ordinary programs written in
a subset of ML into code that generates native code at run
time. Run-time code generation can make use of values and
invariants that cannot be exploited at compile time, yielding
code that is often superior to statically optimal code. But
the cost of optimizing and generating code at run time can
be prohibitive. We demonstrate how compile-time specialization can reduce the cost of run-time code generation by
an order of magnitude without greatly affecting code quality.
Several benchmark programs are examined, which exhibit an
average cost of only six cycles per instruction generated at
run time.
1 Introduction

A Tutorial on Visual Servo Control
Seth Hutchinson
Department of Electrical and Computer Engineering
The Beckman Institute for Advanced Science and Technology
University of Illinois at Urbana-Champaign
405 N. Mathews Avenue
Urbana, IL 61801
Email: seth@uiuc.edu
Greg Hager
Department of Computer Science
Yale University
New Haven, CT 06520-8285
Phone: 203 432-6432
Email: hager@cs.yale.edu
Peter Corke
CSIRO Division of Manufacturing Technology
P.O. Box 883,
Kenmore. Australia, 4069.
pic@brb.dmt.csiro.au
May 14, 1996
Abstract
This paper provides a tutorial introduction to visual servo control of robotic manipulators.
Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including
a brief review of coordinate transformations, velocity representation, and a description of the
geometric aspects of the image formation process. We then present a taxonomy of visual servo
control systems. The two major classes of systems, position-based and image-based systems, are
then discussed. Since any visual servo system must be capable of tracking image features in a
sequence of images, we include an overview of feature-based and correlation-based methods for
tracking. We conclude the tutorial with a number of observations on the current directions of
the research field of visual servo control.
+PAGE+

Implementing Parallel Shortest Paths Algorithms
Marios Papaefthymiou
Department of Electrical Engineering and
Department of Computer Science
Yale University
Joe Rodrigue
Department of Computer Science
Yale University
Abstract
We have implemented a parallel version of the Bellman-Ford algorithm for the single-source shortest paths
problem. Our software has been developed on the CM-5 using C with CMMD communication primitives.
We have empirically compared the efficiency of our implementation with a sequential implementation of
the Bellman-Ford-Moore algorithm developed by Cherkassky, Goldberg and Radzik. We have performed
our experiments using fifty randomly generated graphs with vertex counts in the range between 2 10 and
2 15 and edge counts in the range between 2 11 and 2 21 . In our experiments, the parallel implementation
becomes faster than the sequential implementation when the average degree of the input graphs exceeds
2 5 or 2 6 . For the dense graphs in our test suite, we obtain speedups of up to 3.3 on 32 processors and up
to 8.3 on 128 processors.
In the implementation we discuss in this paper, several design decisions were taken in view of the limited
time we had to complete a working version of our software. For example, instead of performing any kind
of dynamic load balancing, we try to keep the computation load balanced by applying a straightforward
data distribution scheme at the beginning of the computation. Moreover, in the code that runs on
each processor, we avoid any sophisticated data structures and only use linear arrays. We are currently
experimenting with alternative implementations that may lead to improved speedups, particularly on the
denser graphs of our test suite.
1 Introduction

Belief Networks, Hidden Markov Models, and Markov
Random Fields: a Unifying View
Padhraic Smyth
Information and Computer Science Department
University of California, Irvine
CA 92697-3425.
smyth@ics.uci.edu
March 20, 1998
Abstract
The use of graphs to represent independence structure in multivariate probability
models has been pursued in a relatively independent fashion across a wide variety of
research disciplines since the beginning of this century. This paper provides a brief
overview of the current status of such research with particular attention to recent developments which have served to unify such seemingly disparate topics as probabilistic
expert systems, statistical physics, image analysis, genetics, decoding of error-correcting
codes, Kalman filters, and speech recognition with Markov models.
1 Introduction

Theory and Design of Multidimensional QMF Sub-Band Filters From
1-D Filters and Polynomials Using Transforms
I.A. Shah A.A.C. Kalker
Philips Research Laboratories,
P.O. Box 80.000, 5600 JA Eindhoven, The Netherlands
Net: kalker@prl.philips.nl, shah@prl.philips.nl
Abstract
The paper presents the general theory of designing multidimensional Quadrature Mirror Filters (QMF),
for use in sub-band coding (SBC) systems, using the McClellan transform [1]. It was recently shown that
McClellan transform could be used to generate 2-D diamond shape QMF filters [2]. In this paper we will
formalize the proofs of the diamond shape case, and generalize it to other shapes, sampling rasters and
dimensions.
Moreover we show that we do not really need the 1-D QMF filters: it is also possible and even more
convenient to design QMF filter banks by performing transformations on a class of real valued polynomials.
Examples are given of two dimensional diamond and fan-shape filters and three dimensional tetrad filters
designed using this transformation technique.
1 Introduction

Once Upon an Object...
Computationally-Augmented Toys for Storytelling
Jennifer W. Glos and Marina Umaschi
MIT Media Lab
20 Ames Street, E15-320R/N
Cambridge, MA 02139 USA
+1 617 253 6096
- jenglos, marinau-@media.mit.edu
Abstract
We are developing design principles applying tangible interfaces to storytelling. This paper describes an underlying
philosophy and three resultant designs for computer-mediated toys, exploring how the merging of physical objects
with computer technology can enhance childrens storytelling. Each prototype aims to develop a specific set of both
oral and written storytelling skills, as well as collaboration, sharing, and the notion of revision. By creating
narratives, children learn about culture and identity, and develop a sense of self. In addition, narrative can be used as
a gateway to draw girls into technology. The use of multi-sensory interfaces allows for richer interaction.
Keywords
storytelling, children, computer-mediated toys, identity, education, gender, tangible interfaces.
Motivation

3D Morphing
by Matt Blum, Krzysztof Gajos, Manolis Kamvysselis, Hooman Vassef
Abstract
This paper presents our work towards achieving a model based approach to three dimensional
morphing. It describes the initial algorithms and ideas that we envisioned, the final algorithm we
developed and implemented, the environment we worked in, our visualization techniques, and future
work planned on the subject.
Introduction: Different types of morphing

MPI-FM: High Performance MPI on Workstation
Clusters
Mario Lauria
Dipartimento di Informatica e Sistemistica
Universita di Napoli "Federico II"
via Claudio 21
80125 Napoli, Italy
lauria@nadis.dis.unina.it.
Andrew Chien
Department of Computer Science
University of Illinois at Urbana-Champaign
1304 W. Springfield Ave.
Urbana, IL 61801, USA
achien@cs.uiuc.edu
Abstract
Despite the emergence of high speed LANs, the communication performance available to applications on workstation clusters still falls short of that available on MPPs.
A new generation of efficient messaging layers is needed to take advantage of the hardware performance and to deliver it to the application level. Communication software
is the key element in bridging the communication performance gap separating MPPs
and workstation clusters.
MPI-FM is a high performance implementation of MPI for networks of workstations connected with a Myrinet network, built on top of the Fast Messages (FM) library.
Based on the FM version 1.1 released in Fall 1995, MPI-FM achieves a minimum one-way latency of 19 s and a peak bandwidth of 17.3 MByte/s with common MPI send
and receive function calls. A direct comparison using published performance figures
shows that MPI-FM running on SPARCstation 20 workstations connected with a relatively inexpensive Myrinet network outperforms the MPI implementations available
on the IBM SP2 and the Cray T3D, both in latency and in bandwidth, for messages
up to 2 KByte in size.
Visiting at time of writing
+PAGE+

Register Windows and User-Space Threads on the SPARC
David Keppel
Technical Report #91-08-01
Department of Computer Science and Engineering
University of Washington
Seattle, Washington 98195
1 August 1991
Abstract
Multiple lightweight processes or threads have multiple stacks, and a thread context switch moves
execution from one stack to another. On the SPARC 1 architecture, parts of a thread's stack can be
cached in register windows while the thread is running. The cached data must be flushed to memory
when the thread is suspended. Doing the flushing both efficiently and correctly can be tricky. This
document discusses the implementation of a non-preemptive user-space threads package under SunOS 2 .
1 Introduction

104
New-Value Logging in the Echo
Replicated File System
Andy Hisgen, Andrew Birrell, Charles Jerian,
Timothy Mann, Garret Swart
June 23, 1993
Systems Research Center
130 Lytton Avenue
Palo Alto, California 94301
+PAGE+

February 21, 1994
SRC
Research
Report 121
Extensible Syntax with Lexical Scoping
Luca Cardelli, Florian Matthes, and Martn Abadi
Systems Research Center
130 Lytton Avenue
Palo Alto, California 94301
+PAGE+

Using Global Consistency to Recognise Euclidean Objects with an
Uncalibrated Camera
D.A. Forsyth J.L. Mundy A. Zisserman C.A. Rothwell
Computer Science General Electric Robotics Research Group Robotics Research Group
University of Iowa Center for Research and Development Oxford University Oxford University
Iowa City, IA 52242 Schenectady, NY 12345 Oxford, UK Oxford, UK
Abstract
A recognition strategy consisting of a mixture of indexing on invariants and search, allows objects to be recog-nised up to a Euclidean ambiguity with an uncalibrated
camera. The approach works by using projective invariants to determine all the possible projectively equivalent
models for a particular imaged object; then a system of
global consistency constraints is used to determine which of
these projectively equivalent, but Euclidean distinct, models corresponds to the objects viewed. These constraints
follow from properties of the imaging geometry. In particular, a recognition hypothesis is equivalent to an assertion about, among other things, viewing conditions and geometric relationships between objects, and these assertions
must be consistent for hypotheses to be correct. The approach is demonstrated to work on images of real scenes
consisting of polygonal objects and polyhedra.   Keywords:
Recognition, Computer Vision, Invariant Theory, Indexing, Model-based Vision
1 Introduction

DRAFT 1   January 23, 1996
The Impact of Trends in Technology on
File System Design
Michael D. Dahlin
University of California, Berkeley
dahlin@cs.berkeley.edu
Abstract
This paper describes several key trends in technology that will inuence the design of file systems for the next decade. It first outlines the basic trends to hardware performance that underlie
file system design. These trends affect both user demands on file systems and trade-offs in their
design. It then considers how technologies will drive more demanding file system workloads that
will require scalable file systems. Next, it outlines how opportunities raised by these low-level
technology trends impact specific aspects of the xFS file systems serverless design [Dahlin
et al., 1994b, Anderson et al., 1996]. Finally, to put xFSs serverless approach in context, it discusses other technology trends that affect file systems but that are not explicitly addressed in the
serverless design.
1. Trends in Technology

Importing Pre-packaged Software into Lisp: Experience with
Arbitrary-Precision Floating-Point Numbers
Richard J. Fateman
University of California at Berkeley
Abstract
We advocate the use of Common Lisp as a powerful glue for
building scientific computing environments. Naturally one
then has to address mixing pre-existing (non Lisp) code into
this system. We provide a specific example as an elaborate
FORTRAN system written by David Bailey for arbitrary-precision floating-point numeric calculation. We discuss the
advantages and disadvantages of wholesale importing into
Lisp. A major advantage is being able to use state-of-the
art packaged software sooner, while overcoming the disadvantages caused by FORTRAN's traditional batch orientation and weak storage model. In this paper we emphasize in
particular how effective use of imported systems may require
one to address the contrast between the functional (Lisp-like) versus state-transition-based (Fortran-like) approaches
to dealing with compound objects. While our example is
high-precision floats, other highly useful packages including those for simulation, PDE solutions, signal processing,
statistical computation, etc. may also benefit by similar
consideration.
1 Introduction

Modeling and Optimization of a Multiresolution
Image Retrieval System
Antonio Ortega ,
Dept. of Electrical Eng.-Systems
University of Southern California
Los Angeles, California
Zhensheng Zhang,
Dept. of Electrical Engineering and Center for Telecom. Research
Columbia University,   New York
Martin Vetterli
Dept. of Electrical Engineering and Computer Science
University of California,
Berkeley, California
July 15, 1994
IEEE/ACM Transactions on Networking, Submitted, July 1994
Abstract
In this paper, we study the tradeoffs involved in choosing the bit allocation in a
multiresolution remote image retrieval system. Such a system uses a multiresolution
image coding scheme so that a user accessing the database will first see a coarse
version of the images and will be able to accept or discard a given image faster,
without needing to receive all the image data. We formalize the problem of choosing
the bit allocation (e.g., in the two resolution case, how many bits should be given
to the coarse image and the additional information, respectively?) so that the
overall delay in the query is minimized. We provide analytical methods to find the
optimal solution under different configurations and show how a good choice of the
bit allocation results in a significant reduction of the overall delay in the query (by
up to a factor of two in some cases).
This work was presented in part at the IS&T/SPIE Symp. on Electronic Imaging Science & Tech
nology '94, San Jose, CA, Feb. 94 and at Infocom '94, Toronto, Canada, Jun. 94.
Work supported in part by the Fulbright Commission and the Ministry of Education of Spain. This
work was done while at the Dept. of Electrical Eng. and Center for Telecom. Research, Columbia
University.
+PAGE+

Abstract
TCP is a reliable transport protocol tuned to perform well
in traditional networks made up of wired links with stationary hosts. Networks with wireless links and mobile
hosts violate many of the assumptions made by TCP, causing degraded performance. In this paper, we describe a
simple protocol that improves TCP performance by modifying network-layer software only at a basestation without
violating end-to-end TCP semantics. The main idea is to
cache packets at the basestation and perform local
retransmissions. Simulations of this protocol show that it
is significantly more robust in the presence of multiple
packet losses in a single transmission window as compared to TCP. This enables our protocol to tolerate at least
10 times as high an error rate without any performance
degradation.
1 Introduction

Reducing Branch Costs via Branch Alignment
Brad Calder and Dirk Grunwald
Department of Computer Science
Campus Box 430
University of Colorado
Boulder, CO 80309-0430 USA
fcalder,grunwaldg@cs.colorado.edu
Abstract
Several researchers have proposed algorithms for basic block reordering. We call these branch alignment algorithms. The primary
emphasis of these algorithms has been on improving instruction
cache locality, and the few studies concerned with branch prediction reported small or minimal improvements. As wide-issue architectures become increasingly popular the importance of reducing
branch costs will increase, and branch alignment is one mechanism
which can effectively reduce these costs.
In this paper, we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and
the branch prediction architecture when performing the basic block
reordering. We show that branch alignment algorithms can improve
a broad range of static and dynamic branch prediction architectures.
We also show that a programs performance can be improved by approximately 5% even when using recently proposed, highly accurate
branch prediction architectures. The programs are compiled by any
existing compiler and then transformed via binary transformations.
When implementing these algorithms on a Alpha AXP 21604 up to
a 16% reduction in total execution time is achieved.
Keywords: Branch Prediction, Profile-based Optimization,
Branch Target Buffers, Trace Scheduling.
1 Introduction

Empirical Study of a Dataflow Language on the CM-5
David E. Culler
Seth Copen Goldstein
Klaus Erik Schauser
Thorsten von Eicken
Computer Science Division
Department of Electrical Engineering and Computer Sciences
College of Engineering
University of California, Berkeley
Abstract: This paper presents empirical data on the behavior of large dataflow programs on
a distributed memory multiprocessor. The programs, written in the dataflow language Id90, are
compiled via a Threaded Abstract Machine (TAM) for the CM-5. TAM refines dataflow execution
models by addressing critical constraints that modern parallel architectures place on the compilation
of general-purpose parallel programming languages. It exposes synchronization, scheduling, and
network access so that the compiler can optimize against the cost of these operations.
The data presented in this paper evaluates the TAM approach in compiling dataflow languages
on stock hardware. We present data on the instruction mix, speedup, scheduling behavior, and
locality of large ID90 programs. It is shown that the TAM scheduling hierarchy is able to tolerate
long communication latencies, especially when some degree of I-structure locality is present. We investigate how frame allocation strategies, k-bounded loops, and I-structure caching and distribution
together affect the overall efficiency. Finally we document some scheduling anomalies.
1 Introduction

May 17, 1993
An Efficient Network
Interface for the RAID-II
File Server
Srinivasan Seshan
A masters report
1.0 Abstract
Distributed systems in use today depend heavily on network communications between clients and servers. In this report, we describe the
design and implementation of the network architecture (hardware, software and protocols) of the RAID-II system. RAID-II is a high speed network file server connected to an UltraNetwork. In order to support high
bandwidth network transfers with the RAID-II server, we partitioned the
networking software among the various processors in the system. Measurements of the system show that the RAID-II server can sustain 21MB/
s of data bandwidth to the Ultranet.
2.0 Introduction

Comparing Data Forwarding and Prefetching
for Communication-Induced Misses in Shared-Memory MPs 1
David Koufaty 2 and Josep Torrellas
Department of Computer Science
University of Illinois at Urbana-Champaign,   IL 61801
dkoufaty@ichips.intel.com torrella@cs.uiuc.edu
http://iacoma.cs.uiuc.edu
Abstract
As the difference in speed between processor and memory system continues to increase, it is becoming crucial to develop
and refine techniques that enhance the effectiveness of cache
hierarchies. Two such techniques are data prefetching and
data forwarding. With prefetching, a processor hides the latency of cache misses by requesting the data before it actually
needs it. With forwarding, a producer processor hides the
latency of communication-induced cache misses in the consumer processors by sending the data to the caches of the
latter. These two techniques are complementary approaches
to hiding the latency of communication-induced misses.
This paper compares the effectiveness of data forwarding
and data prefetching to hide communication-induced misses.
Although both techniques require comparable hardware support, forwarding usually has a lower instruction overhead. We
evaluate prefetching and forwarding algorithms in a paralleliz-ing compiler using execution-driven simulations of a shared-memory multiprocessor. Both data forwarding and prefetch-ing reduce the execution time of applications significantly (30-40% on average). Forwarding performs better on average,
while prefetching is more robust to changes in cache and memory parameters. Finally, we propose two ways of integrating
the two techniques. The integration of the two techniques reduces the execution time even more (43-48% on average) and
is very robust.
1 Introduction

Optimizing Primary Data Caches for Parallel
Scientific Applications: The Pool Buffer Approach 1
Liuxi Yang and Josep Torrellas
Center for Supercomputing Research and Development
University of Illinois at Urbana-Champaign,   IL 61801
Email: lyang,torrella@csrd.uiuc.edu
Abstract
Optimizing on-chip primary data caches for parallel scientific applications is challenging because different applications
exhibit different behavior. Indeed, while some applications
exhibit good spatial locality, others have accesses with long
strides that prevent the effective use of cache lines. Finally,
other applications cannot exploit long lines because they exhibit false sharing. To help processors execute these three
types of applications efficiently, we introduce the Pool Buffer,
a small direct-mapped cache accessed in parallel with the primary cache. The function of the pool buffer is to fetch long
sectors of relatively short cache lines from memory on a miss,
while only letting into the cache the lines that the processor
actually references. The pool buffer can also perform sequential prefetching of sectors.
An evaluation of the pool buffer based on simulations of
five 32-processor Perfect Club codes yields encouraging results. Adding a pool buffer of one-quarter the size of the
cache causes a small increase in area while usually achieving
large reductions in execution time. For example, for a range
of caches with 32-byte lines, the execution time decreases by
an average of about 20%. We also show that small 1-Kbyte
buffers are often large enough to get most of the potential
benefits. Finally, caches with pool buffers are more effective
than caches with long lines and no pool buffer.
1 Introduction

An Analysis of Message Sequence Charts
Peter B. Ladkin Stefan Leue
University of Berne
Institute for Informatics and Applied Mathematics
Langgassstrasse 51
CH-3012 Bern, Switzerland
ladkin@iam.unibe.ch, leue@iam.unibe.ch
IAM 92-013
June 1992
+PAGE+

Relating Test Purposes to Formal Specifications:
Towards a Theoretical Foundation of Practical Testing
Jens Grabowski, Dieter Hogrefe
Robert Nahm, Andreas Spichiger
IAM-93-014
June 1993
+PAGE+

SDL and MSC Based Test Case Generation -
An Overall View of the SAMSTAG Method
Jens Grabowski
IAM-94-005
+PAGE+

Automated Decomposition of Model-based Learning Problems
Brian C. Williams and Bill Millar
Recom Technologies, Caelum Research
NASA Ames Research Center,   MS 269-2
Moffett Field, CA 94305 USA
E-mail: williams, millar@ptolemy.arc.nasa.gov
Abstract
A new generation of sensor rich, massively distributed
autonomous systems is being developed that has
the potential for unprecedented performance, such
as smart buildings, reconfigurable factories, adaptive
traffic systems and remote earth ecosystem monitoring. To achieve high performance these massive systems will need to accurately model themselves and
their environment from sensor information. Accomplishing this on a grand scale requires automating the
art of large-scale modeling. This paper presents a
formalization of decompositional, model-based learning
(DML), a method developed by observing a modeler's
expertise at decomposing large scale model estimation
tasks. The method exploits a striking analogy between
learning and consistency-based diagnosis. Moriarty,
an implementation of DML, has been applied to thermal modeling of a smart building, demonstrating a
significant improvement in learning rate.
Introduction

Generation of task-specific segmentation
procedures as a model selection task
Ralf Herbrich and Tobias Scheffer
Technische Universitat Berlin,
Artificial Intelligence Research Group,   Sekr. FR 5-8,
Franklinstr. 28/29. D-10587 Berlin, Germany
email: ralfh|scheffer@cs.tu-berlin.de
December 1, 1997
Abstract
In image segmentation problems, there is usually a vast amount of filter
operations available, a subset of which has to be selected and instantiated
in order to obtain a satisfactory segmentation procedure for a particular domain. In supervised segmentation, a mapping from features, such as filter
outputs for individual pixels, to classes is induced automatically. However,
since the sample size required for supervised learning grows exponentially
in the number of features it is not feasible to learn a segmentation procedure
from a large amount of possible filters. But we argue that automatic model
selection methods are able to select a region model in terms of some filters.
We propose a wrapper algorithm that performs this task. We present results
on artificial textured images (Brodatz) and report on our experiences with
x-ray images.
Keywords: model based image segmentation, automatic model selection,
learning pixel classifier, texture segmentation
+PAGE+

A Neural Net for Determining Structural
Similarity of Recursive Programs
Kristina Schadler, Ute Schmid, Hendrik Lubben, Bernd
Machenschalk
Research Group "Methods of Artificial Intelligence"
Institute for Applied Computer Science, Technische Universitat
Berlin
email: schaedle,schmid,compuman,herold@cs.tu-berlin.de
Abstract. In this work it will be shown, how the comparison of recursive program schemata (RPS) can be reduced to an only slightly modified
type of the search for maximal isomorphic subgraphs by interpreting the
RPS as directed, cyclic, labelled graphs. The quality of the mapping of
two RPS can be used as a quantitative measure for similarity of RPS
among each other. The simultaneously calculated graph morphism can
serve as a basis for the detection of analogies between the RPS. A special
neural net, developed for the solution of general graph-matching problems, realizes the search for a sensible and as comprising as possible
mapping between two RPS. It is shown, which special properties of RPS
have to be accounted for in the modeling and how these can be incorporated into the algorithms when using a special neural approach for the
solution.
1. Structural similarities in case-based reason

ICOPS 97
ICOPS 97
Nonlinear Poisson Solve for Boltzmann Electrons
K. L. Cartwright , J. P. Verboncoeur , and C. K.Birdsall
Electronics Research Laboratory
University of California,   Berkeley, CA 94720
October 29, 1997
Abstract
Kinetic simulation of plasmas in which equilibrium occurs over ion timescales poses
a computational challenge due to the disparate timescales of the electron plasma frequency (~ 10 9 ), the ion plasma frequency (~ 10 7 ), ion transit frequency (~ 10 6 ), and
the ionization frequency (~ 10 7 ). Hybrid electrostatic PIC algorithms are presented
in which the electrons reach thermodynamic equilibrium with the ions each time step.
There are two different approximations for the electrons. First, the nonlinear Boltzmann relationship for the electrons can be applied to the bulk of a plasma. Second,
there is a truncated Maxwellian which is used in sheaths; this approximation truncates
the electron distribution at the wall potential. The error associated with neglecting
this second approximation in the sheath is small. The collision cross section, (E), can
be a tabulated or fitted function; the method is implemented with He cross sections.
These approximations neglect effects faster than ion time-scales, decreasing the computer time used by over an order of magnitude; however, they increase the complexity
of the boundary conditions and the simulation is no longer self-consistent. Theoretical
ramifications of these approximations are examined, and results are compared with full
Supported by ONR-AASERT N100014-94-1-1033
Supported by the Air Force Office of Scientific Research-MURI under grant F49620-95-1-0253
Supported by the Air Force Office of Scientific Research-under grant FDF49620-96-1-0154
+PAGE+

Refraction and Reflection of Ion Acoustic Solitons by Space Charge Sheath
K. L. Cartwright and C. K. Birdsall
Electronics Research Laboratory
University of California,   Berkeley, CA 94720-1774
Acknowledgments: This research is supported by ONR grant number N00014-90-J-1198 and ONR-AASERT 23057.
Abstract
Experiments have shown[1],[2] that ion acoustic solitons tunnel through the space charge sheath
in front of a grid without time delay. They are absorbed resonantly when the spatial width of the
wave is close to the characteristic gradient scale length of the sheath. The reflection and transmission
coefficients found in these experiments have compared well with theory in the long wavelength limit[3].
However, to achieve this comparison, two parameters were added that were not in the original theory.
These parameters allow for the absorption of wave energy by the space charge sheath. The goal of our
numerical simulations, designed to reproduce the experimental results, is to uncover the mechanism of
this energy loss and large speed of propagation through the sheath.
1 Photo Ionization-Steady State

Survey Paper
Update-in-place Analysis for Sets
Chung Yung
Computer Science Department
Courant Institute of Mathematical Sciences
New York University
yung@cs.nyu.edu
December 15, 1997
Abstract
This survey paper describes the current approaches on the update-in-place analysis
for sets. Pure functional languages do not allow mutations, destructive updates, or selective updates so that straightforward implementations of functional language compilers
may induce large amounts of copying to preserve program semantics. The unnecessary
copying of data can increase both the execution time and the memory requirements of
an application. Introducing sets to functional languages as a primitive data constructor posts a new problem of update-in-place analysis in functional languages. Moreover, most of the compiler optimization techniques depend on the side-effects and the
update-in-place analysis serves as the premise of applying such optimization techniques.
Among other compiler optimization techniques, finite differencing captures common yet
distinctive program constructions of costly repeated calculations and transforms them
into more efficient incremental program constructions. This dissertation is an attempt
to explore the update-in-place analysis for sets in functional languages in order to apply finite differencing to compiling pure functional languages. In this survey paper,
we will describe the approaches of update-in-place analysis and the finite differencing
techniques.
1 Motivation and Introduction

Economic Allocation of Computation Time
with Computation Markets
Nathaniel Rockwood Bogan
May, 1994
c Copyright 1994 by Nathaniel R. Bogan
This report is a reset version of a masters thesis submitted to the Department of Electrical
Engineering and Computer Science on May 15, 1994 in partial fulfillment of the requirements
for the degree of Master of Engineering.
+PAGE+

Improving the Performance of Radial Basis
Function Networks by Learning Center Locations
Dietrich Wettschereck and Thomas Dietterich
Department of Computer Science
Oregon State University
Corvallis, OR 97331-3202
Advances in Neural Information Processing Systems 4
Edited by J.E.Moody, S.J.Hanson, and R.P.Lippmann,
Morgan Kaufmann Publishers, San Mateo, CA, 1992
+PAGE+

EUROPEAN ORGANISATION FOR NUCLEAR RESEARCH
CERN-PPE/96-xxx
25 January 1996
Search for Chargino and Neutralino
Production Using the OPAL Detector
at
s = 130 136 GeV at LEP
The OPAL Collaboration
Abstract
A search for charginos and neutralinos, predicted by supersymmetric theories, has been
performed using a data sample of 2.6 pb 1 at a centre-of-mass energy of p s =130 GeV and
2.6 pb 1 at 136 GeV collected with the OPAL detector at LEP during November 1995. No
candidate events were observed. The 95% C.L. lower limit on the lightest chargino mass
in the Minimal Supersymmetric Standard Model is 65.4 GeV if the universal scalar mass
m 0 is greater than 1 TeV, and 58.7 GeV for the smallest m 0 compatible with slepton and
sneutrino mass limits obtained at centre-of-mass energies near the Z peak. These limits
were obtained under the conditions that the lightest chargino is heavier than the lightest
neutralino by more than 10 GeV and tan fi is larger than 1.5. The results of a model
independent search for charginos and neutralinos are also given.
Submitted to Physics Letters
+PAGE+

System Administration: Monitoring, Diagnosing, and Repairing
Eric Anderson
We first describe the general goals of system administration explaining the
relationship to monitoring, diagnosing, and repairing (MDR). Then, we describe the
functional and environmental concerns for a MDR system, and use these concerns to
explain how previous approaches have failed to fully address the problem. Next, we
describe the major pieces of our approach, and identify the research questions
associated with each piece. Finally we present a method for testing the system when
it is created.
Introduction

Path integral approach to no-Coriolis
approximation in heavy-ion collisions
K. Hagino, 1 N. Takigawa, 1 , A.B. Balantekin 2 , and J.R. Bennett 3
1 Department of Physics, Tohoku University,   980-77 Sendai, Japan
2 Physics Department, University of Wisconsin,
Madison, Wisconsin 53706, USA
3 Department of Physics and Astronomy,
University of North Carolina at Chapel Hill,   Chapel Hill, NC 27599-3255
June 26, 1995
Abstract
We use the two time influence functional method of the path integral approach in
order to reduce the dimension of the coupled-channels equations for heavy-ion reactions
based on the no-Coriolis approximation. Our method is superior to other methods in that
it easily enables us to study the cases where the initial spin of the colliding particle is not
zero. It can also be easily applied to the cases where there is a spin-orbit force, and where
the internal degrees of freedom are not necessarily collective coordinates. It also clarifies
the underlying assumption of the approximation.
+PAGE+

ACM SIGPLAN Workshop on Languages, Compilers and Tools for Real-Time Systems, La Jolla, California, June 1995.
RTsynchronizer: Language Support for
Real-Time Specifications in Distributed Systems
Shangping Ren and Gul A. Agha
Department of Computer Science
1304 W. Springfield Avenue
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
Email: f ren j agha g@cs.uiuc.edu
Abstract
We argue that the specification of an object's functional behavior and the timing constraints imposed
on it may be separated. Specifically, we describe
RTsynchronizer, a high-level programming language
construct for specifying real-time constraints between
objects in a distributed concurrent system. During program execution, RTsynchronizers affect the
scheduling of distributed objects to enforce real-time
relations between events. Objects in our system are
defined in terms of the actor model extended with
timing assumptions. Separation of the functional
behaviors of actors and the timing constraints on
patterns of actor invocation provides at least three
important advantages. First, it simplifies code development by separating design concerns. Second,
multiple timing constraints can be independently
specified and composed. And finally, a specification
of timing constraints can be reused even if the
representation of the functional behavior of actors has
changed, and conversely.
A number of examples are given to illustrate the
use of RTsynchronizers. These examples illustrate
how real-time constraints for periodic events, simultaneous events, exception handling, and producer-consumer may be specified.
The research described has been made possible by support from the Office of Naval Research (ONR contract numbers N00014-90-J-1899 and N00014-93-1-0273), by an Incentives for Excellence Award from the Digital Equipment Corporation Faculty Program, and by joint support from the Defense
Advanced Research Projects Agency and the National Science
Foundation (NSF CCR 90-07195). The authors would like to
thank Mark Astley, Brian Nielsen, Masahiko Saitoh and Daniel
Sturman for helpful discussions concerning the manuscript.
1 Introduction

A Visualization Model for Concurrent Systems
Mark Astley and Gul A. Agha
Open Systems Laboratory
Department of Computer Science
1304 W. Springfield Avenue
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
Phone: (217) 244-3087
Fax: (217) 333-3501
Email: fastley j aghag@cs.uiuc.edu
Keywords: Actors, Distributed Systems, Program Visualization
Abstract
Concurrent systems maintain a distributed state and thus require coordination and synchronization between
components to ensure consistency. To provide a coherent design approach to concurrent systems, recent work
has employed an object-based methodology which emphasizes interactions through well-defined interfaces. The
Actor model has provided formal reasoning about distributed object systems. Nonetheless, due to the complex
interactions among components and the high volume of observable information produced, understanding and
reasoning about concurrent algorithms in terms of simple interactions is a difficult task. Coordination patterns,
which abstract over simple interactions, are not biased by low-level event orderings and are the appropriate
mechanism for reasoning about concurrent algorithms. We outline a methodology for visualizing coordination
patterns in concurrent algorithms which emphasizes observable interactions and causal connections between
objects. We introduce visualizers as a linguistic mechanism for mapping coordination patterns to visualization.
Visualizers are specified separately from algorithm code and thus respect code integrity. Moreover, visualizers
may be implemented strictly in terms of object interfaces and thus preserve object encapsulation.
Author for contact.
+PAGE+

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 13, NO. 7, SEPTEMBER 1995 1
Billing Users and Pricing for TCP
Richard J. Edell, Nick McKeown and Pravin P. Varaiya
Abstract| This paper presents a system for billing users
for their TCP traffic. This is achieved by postponing the
establishment of connections while the user is contacted,
verifying in a secure way that they are prepared to pay. By
presenting the user with cost and price information, the system can be used for cost recovery and to encourage efficient
use of network resources. The system requires no changes to
existing protocols or applications and can be used to recover
costs between cooperating sites. Statistics collected from a
four day trace of traffic between the University of Califor-nia, Berkeley and the rest of the Internet demonstrate that
such a billing system is practical and introduces acceptable
latency. An implementation based on the BayBridge prototype router is described. Our study also indicates that
pricing schemes may be used to control network congestion
either by rescheduling time-insensitive traffic to a less expensive time of the day, or by smoothing packet transfers to
reduce traffic peaks.
Keywords| Internet economics, network tracing, usage-accounting
I. Introduction

Appears in Proceedings of the 4
th
International Conference on Knowledge Discovery and Data Mining,
AAAI Press, 1998, 359-363.
Learning to Predict Rare Events in Event Sequences
Gary M. Weiss
and Haym Hirsh
Department of Computer Science
Rutgers University
New Brunswick, NJ 08903
gmweiss@att.com, hirsh@cs.rutgers.edu
Abstract
Learning to predict rare events from sequences of events
with categorical features is an important, real-world,
problem that existing statistical and machine learning
methods are not well suited to solve. This paper describes
timeweaver, a genetic algorithm based machine learning
system that predicts rare events by identifying predictive
temporal and sequential patterns. Timeweaver is applied to
the task of predicting telecommunication equipment failures
from 110,000 alarm messages and is shown to outperform
existing learning methods.
Introduction

A Comparative Study of Three Paradigms for Object
Recognition Bayesian Statistics, Neural Networks and
Expert Systems.
J. K. Aggarwal, Joydeep Ghosh, Dinesh Nair and Ismail Taha
Computer and Vision Research Center
The University of Texas at Austin,   Austin, TX, USA
email: aggarwaljk@mail.utexas.edu
Abstract
Object recognition, which involves the classification of objects into one of many a priori known object types, and determining object characteristics such as pose, is a difficult
problem. A wide range of approaches have been proposed and applied to this problem with
limited success. This paper presents a brief comparative study of methods from three different paradigms for object recognition: Bayesian, Neural Network and Expert Systems.
1 Introduction

Evaluation and Ordering of Rules Extracted from Feedforward
Networks
Ismail Taha and Joydeep Ghosh
Laboratory of Artificial Neural Systems
University of Texas
Austin, TX. 78712-1084
fismail,ghoshg@pine.ece.utexas.edu
Abstract
Rules extracted from trained feedforward networks
can be used for explanation, validation, and cross-referencing of network output decisions. This paper
introduces a rule evaluation and ordering mechanism
that orders rules extracted from feedforward networks
based on three performance measures. Detailed experiments using three rule extraction techniques as applied
to the Wisconsin breast cancer database, illustrate the
power of the proposed methods. Moreover, a method
of integrating the output decisions of both the extracted
rule-based system and the corresponding trained network is proposed. The integrated system provides further improvements.
1. Introduction

DART/HYESS Users Guide
Jerome H. Friedman
Department of Statistics and
Stanford Linear Accelerator Center
Stanford University
jhf@stat.stanford.edu
December 20, 1996
Abstract
This note provides information for using the Fortran program [Friedman (1996b)] that imple
ments the recursive covering approach to local learning described in Friedman (1996a).
1. Introduction

Network of Workstations Active Messages Target for
Ptolemy C Code Generation
by Patrick Warner
Memorandum No. UCB/ERL M97/8
January 24, 1997
Submitted to the Department of Electrical Engineering and Computer Science, University of Cal-ifornia at Berkeley, in partial satisfaction of the requirements for the degree of Master of Science,
Plan II.
+PAGE+

DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE
UNIVERSITY OF CALIFORNIA
BERKELEY, CALIFORNIA 94720   August 17, 1997
A PRELIMINARY STUDY OF
HIERARCHICAL FINITE STATE MACHINES
WITH MULTIPLE CONCURRENCY MODELS
by
Alain Girault, Bilung Lee, and Edward A. Lee
Memorandum No. UCB/ERL M97/57
ELECTRONICS RESEARCH LABORATORY
College of Engineering
University of California, Berkeley
94720
+PAGE+

Digital Communication Over Rayleigh
Fading Channels
T. M. Parks
15 December 1992
Abstract
The properties of Rayleigh fading channels are derived and their
effects on various QAM signal constellations are explored. A simplified
channel model for an urban radio environment is justified in order
to simplify the analysis of error performance for the constellations.
Finally, arguments are made for extending the results to more general
channel models.
+PAGE+

Switching through Singularities
C. J. Tomlin and S. S. Sastry
Department of Electrical Engineering and Computer Sciences
University of California,   Berkeley CA 94720
fclairet, sastryg@eecs.berkeley.edu
Abstract
Asymptotic tracking is studied for systems in which the relative degree is not well defined,
meaning that the control law derived from exact input-output linearization has singularities in
the state space. We propose a tracking control law which switches between approximate tracking
[1] close to the singularities, and exact tracking away from the singularities, and we study the
applicability of this law based on the behavior of the system's zero dynamics at the switching
boundary. As in [1], the ball and beam example is used to motivate the study.
Keywords: Switching, nonlinear control, zero dynamics, exact and asymptotic tracking,
nonminimum phase.
1 Introduction

Simulation as a Tool for Hybrid System Design
John Lygeros, Datta Godbole & Shankar Sastry
Intelligent Machines and Robotics Laboratory
Department of Electrical Engineering and Computer Sciences
University of California,   Berkeley, CA 94720
lygeros,godbole,sastry@eecs.berkeley.edu
Abstract
A case study of the use of simulation as a tool
for design and validation of hybrid systems is presented. We use the Intelligent Vehicle Highway Systems (IVHS) architecture of [1], a system that involves
both continuous state and discrete event controllers as
our example of a hierarchical hybrid system. We point
out that even though analytical methods do not exist
for verification of hybrid control system, a simulation
tool can be useful to (in)validate that the the hybrid
system operates properly.
1 Introduction

Construction of Fuzzy Linguistic Model
Tak-Kuen John Koo
Robotics and Intelligent Machines Laboratory
211-85 Cory Hall,   Department of EECS
University of California at Berkeley
Berkeley, CA94720
koo@robotics.eecs.berkeley.edu
Abstract
Using linguistic variables to describe the behavior of a
hybrid system, which consists of a discrete event system and a continuous system, could make the design
of the controller and verification of the system perform
on an unified framework. In this paper, we show the
construction of a fuzzy linguistic model from a given
mathematical model of a physical system. By considering the state-space realization of the model, the system
construction problem can be transformed into a function approximation problem. We propose to use projection theorem in obtaining an optimal fuzzy system
which is the best approximation of a given nonlinear
function in L 2 (U ) space. We show that the existence
and uniqueness of the optimal solution is assured when
the Fuzzy Basis Functions(FBFs) are linearly independent. We demonstrate that the dependence of the basis
functions can be examined by checking the condition of
the Gram determinant associated to the FBFs. Finally,
a method is proposed in converting the optimal coefficients into fuzzy sets to obtain a fuzzy linguistic model.
1 Introduction

Hybrid Control in Air Traffic Management Systems
S. Sastry, G. Meyer , C. Tomlin, J. Lygeros, D. Godbole and G. Pappas
Department of Electrical Engineering & Computer Sciences,
University of California,   Berkeley, CA 94720
MS 210-3,   NASA Ames Research Center,   Moffett Field CA 94035
fsastry, gmeyer, clairet, lygeros, godbole, gpappas@eecs.berkeley.edug
ABSTRACT
In a new collaborative project involving the University of California, Berkeley, NASA Ames Research
Center, and Honeywell Systems Research Center, we
have begun the study of hierarchical, hybrid control
systems in the framework of air traffic management
systems (ATMS). The need for a new ATMS arises
from the overcrowding of large urban airports and
the need to more efficiently land and take off larger
numbers of aircraft, without building new runways.
Technological advances that make a more advanced
air traffic control system a reality include the availability of relatively inexpensive and fast real time
computers (both on board the aircraft and in the control tower) and global positioning systems. The usefulness of these technological advances is currently
limited by today's air traffic control system, which
involves the use of "freeways" in the Terminal Radar
Approach Control (TRACON) region around urban
airports. These freeways are set approach patterns
to runways which do not allow for the possibility of
so-called "free flight" by an aircraft to its destination.
Limiting the aircraft trajectories in this manner results in the addition of both planned and unplanned
delays to air travel.
Keywords: hybrid systems, air traffic management,
flight control.
1. Introduction

A Fault Tolerant Control Architecture
for Automated Highway Systems
John Lygeros, Datta N. Godbole, Mireille Broucke
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
Berkeley, CA 94720
lygeros, godbole, mire@robotics.eecs.berkeley.edu
Abstract
We propose a hierarchical control architecture for dealing with faults and adverse environmental conditions on an Automated Highway System (AHS). Our design extends
a previous control architecture that works under normal conditions of operation. The
faults that are considered in our design are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these
capabilities is used by supervisors in each of the layers of the architecture to select appropriate control strategies. We outline the extended control strategies that are needed
by these supervisors and, in certain cases, give examples of their detailed operation.
Keywords
Automated Highway System Design, Fault Tolerant Control, Safety
Research supported by the California PATH program, Institute of Transportation Studies, University of
California, Berkeley, under MOU-135
+PAGE+

Motion Recovery From Image Sequences: Discrete Viewpoint vs.
Differential Viewpoint
Yi Ma Jana Kosecka Shankar Sastry x
Electronics Research Laboratory
University of California at Berkeley
Berkeley, CA 94720-1774
fmayi, janka, sastryg@robotics.eecs.berkeley.edu
November 3, 1997
Abstract
The aim of this paper is to explore intrinsic geometric methods of recovering the three
dimensional motion of a moving camera from a sequence of images. Generic similarities between
the discrete approach and the differential approach are revealed through a parallel development
of their analogous motion estimation theories.
We begin with a brief review of the (discrete) essential matrix approach, showing how to
recover the 3D displacement from image correspondences. The space of normalized essential
matrices is characterized geometrically: the unit tangent bundle of the rotation group is a
double covering of the space of normalized essential matrices. This characterization naturally
explains the geometry of the possible number of 3D displacements which can be obtained from
the essential matrix.
Second, a differential version of the essential matrix constraint previously explored by [19, 20]
is introduced. We then present the precise characterization of the space of differential essential
matrices, which gives rise to a novel eigenvector-decomposition-based 3D velocity estimation
algorithm from the optical flow measurements. This algorithm gives a unique solution to the
motion estimation problem and serves as a differential counterpart of the SVD-based 3D displacement estimation algorithm from the discrete case.
Finally, simulation results are presented evaluating the performance of our algorithm in terms
of bias and sensitivity of the estimates with respect to the noise in optical flow measurements.
The presented unifying theory of the motion estimation using discrete and differential version of the Longuet-Higgins (essential) constraint can be extended to the case of uncalibrated
cameras.
Keywords: optical flow, epipolar constraint, motion estimation.
Research is supported by ARO under the MURI grant DAAH04-96-1-0341, "An Integrated Approach to Intelligent
Systems".
Address: 211-109 Cory Hall, EECS, UC Berkeley, Berkeley, CA 94720-1772, USA.   Tel: (USA) 510-643-2383.
Address: 211- 98 Cory Hall, EECS, UC Berkeley, Berkeley, CA 94720-1772, USA.   Tel: (USA) 510-643-5806.
x Address: 269M Cory Hall, EECS, UC Berkeley, Berkeley, CA 94720-1772, USA.   Tel: (USA) 510-642-1857.
+PAGE+

SmartATMS : A SIMULATOR FOR AIR TRAFFIC MANAGEMENT SYSTEMS
Tak-Kuen John Koo
Yi Ma
George J. Pappas
Claire Tomlin
Robotics and Intelligent Machines Laboratory
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley
Berkeley, CA 94720
koo,mayi,gpappas,clairet@eecs.berkeley.edu
ABSTRACT
Air Traffic Management Systems (ATMS) of the future will feature Free Flight, in which aircraft choose
their own routes, altitude, and speed, and automated conflict resolution methods in which aircraft
will coordinate to resolve conflicts. The resulting distributed control architecture is a hybrid system, with
mixed discrete event and continuous time dynamics.
SmartATMS is an object oriented modeling and simulation facility which accounts for these hybrid issues
and will serve as a uniform modeling framework for
the design and evaluation of various ATMS concepts.
1 INTRODUCTION

Depth Discontinuities by Pixel-to-Pixel Stereo
Stan Birchfield Carlo Tomasi
Computer Science Department
Stanford University
Stanford, California 94305
[birchfield, tomasi]@cs.stanford.edu
This research was supported by the National Science Foundation under a Graduate Research Fellowship and under contract IRI-9506064, and by the Department of Defense under
MURI contract DAAH04-96-1-0007 monitored by ARO and under a subcontract of STTR
contract F49620-95-C-0078 monitored by AFOSR.
+PAGE+

FROM KNOWLEDGE TO BELIEF
a dissertation
submitted to the department of computer science
and the committee on graduate studies
of stanford university
in partial fulfillment of the requirements
for the degree of
doctor of philosophy
By
Daphne Koller
October 1994
+PAGE+

Appears in KDD-97
MineSet: An Integrated System for Data Mining
Cliff Brunk James Kelly Ron Kohavi
Data Mining and Visualization
Silicon Graphics, Inc.
2011 N. Shoreline Blvd
Mountain View, CA 94043-1389
fbrunk,jkelly,ronnykg@engr.sgi.com
Abstract
MineSet TM , Silicon Graphics' interactive system for
data mining, integrates three powerful technologies:
database access, analytical data mining, and data visualization. It supports the knowledge discovery process from data access and preparation through iterative analysis and visualization to deployment. Mine-Set is based on a client-server architecture that scales
to large databases. The database access component
provides a rich set of operators that can be used to
preprocess and transform the stored data into forms
appropriate for visualization and analytical mining.
The 3D visualization capabilities allow direct data visualization for exploratory analysis, including tools
for displaying high-dimensional data containing geographical and hierarchical information. The analytical mining algorithms help identify potentially interesting models of the data, which can be viewed using
visualization tools specialized for the learned models.
Third party vendors can interface to the MineSet tools
for model deployment and for integration with other
packages.
Introduction

Tioga: Providing Data Management Support for
Scientific Visualization Applications
Michael Stonebraker, Jolly Chen, Nobuko Nathan, Caroline Paxson, Jiang Wu
Computer Science Division, EECS Department
University of California
Berkeley, CA 94720
Abstract
We present a user interface paradigm for
database management systems that is motivated
by scientific visualization applications. Our
graphical user interface includes a "boxes and arrows" notation for database access and a flight
simulator model of movement through information space. We also provide means to specify a
hierarchy of abstracts of data of different types
and resolutions, so that a "zoom" capability can
be supported. The underlying DBMS support for
this system is described and includes the compilation of query plans into megaplans, new algorithms for data buffering, and provisions for
a guaranteed rate of data delivery. The current state of the Tioga implementation is also described.
This research was sponsored by NSF Grant IRI-9107455, ARO Grant DAAL03-91-G-0183, and DARPA
Contract DABT63-92-C-0007. Additional support was
provided by the University of California and Digital Equipment Corporation under Research Grant #1243.
Other industrial and government partners include the
State of California Department of Water Resources, United
States Geological Survey, Construction Engineering Research Laboratory (CERL) of the U.S. Army Corps of
Engineers, the National Aeronautics and Space Administration (NASA), Epoch Systems, Inc., Hewlett-Packard
Corp., Hughes Aircraft Company, MCI, Metrum Corporation, PictureTel Corporation, Research Systems Inc., Science Applications International Corporation, Siemens Inc.,
and TRW Space and Electronics.
1 Introduction

BigSur:
A System For the Management of Earth Science Data
Paul Brown
EECS Department
University of California, Berkeley
Michael Stonebraker
EECS Department
University of California, Berkeley
ABSTRACT
In this paper we present a prototype system for
the management of earth science data which is
novel in that it takes a DBMS centric view of the
the task. Our prototype -- called "BigSur" -- is
shown in the context of its use by two geographically distributed scientific groups with demanding data storage and processing requirements.
BigSur currently stores 1 Terabyte of data, about
one thousandth of the volume EOSDIS must
store. We claim that the design principles
embodied in BigSur provide sufficient exibility
to achieve the difficult scientific and technical
objectives of Mission to Planet Earth.
1. INTRODUCTION

GEN++ | an analyzer generator for C++ programs
Prem Devanbu
Articifial Intelligence Principles Research Department
prem@research.att.com
Laura Eaves
Object Oriented and Artificial Intelligence Technologies Group,
laurae@mozart.att.com
1 Introduction

The Use of Description Logics in KBSE systems
Survey Paper
Premkumar T. Devanbu & Mark A. Jones
Artificial Intelligence Principles Research Department
AT&T Bell Laboratories
fprem,jonesg@research.att.com
Murray Hill, NJ 07063
February 9, 1994
Abstract
The increasing size and complexity of many software
systems demand a greater emphasis on capturing and
maintaining knowledge at many different levels within
the software development process. This knowledge includes descriptions of the hardware and software components and their behavior, external and internal design specifications, and support for system testing.
The knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use
formally represented knowledge, with associated inference procedures, to support the various subactivi-ties of software development. As they grow in scale,
KBSE systems must balance expressivity and inferential power with the real demands of knowledge base
construction, maintenance, performance and comprehensibility. Description Logics (DL's) possess several
features a terminological orientation, a formal semantics and efficient reasoning procedures which offer an effective tradeoff of these factors. We discuss
three KBSE systems in which DL's capture some of
the requisite knowledge needed to support design, coding and testing activities. We close with a discussion
of the benefits of DL's and ways to address some of
their limitations.
1 Introduction

The following paper was originally published in the
Proceedings of the USENIX 2nd Symposium on
Operating Systems Design and Implementation
Seattle, Washington, October 1996
For more information about USENIX Association contact:
1. Phone: 510 528-8649
2. FAX: 510 548-5738
3. Email: office@usenix.org
4. WWW URL: http://www.usenix.org
Dealing With Disaster:
Surviving Misbehaved Kernel Extensions
Margo I. Seltzer, Yasuhiro Endo, Christopher Small, Keith A. Smith
Harvard University
+PAGE+

5th Midwest Artificial Intelligence and Cognitive Science Conference
Identifying Language from Raw Speech
An Application of Recurrent Neural Networks
Weilan Wu, Stan C. Kwasny, Barry L. Kalman, E. Maynard Engebretson
Department of Computer Science
Washington Unversity
St. Louis, MO 63130
Abstract
People can differentiate spoken languages
without understanding them, and, in some
sense, this differentiation can only be done
without understanding the language. When
we consider a multi-lingual person trying to
understand an utterance spoken in one of the
languages with which they are familiar, they
will first decide which language this utterance
belongs to, before trying to interpret it.
The language identification task is one
example of high-level feature abstraction from
raw speech. Speech samples are classified
into categories according to what language
was spoken. We conjecture that this classification can be performed reliably and in real
time. To be successful, such a system should
be speaker-independent as well as context-independent. A large amount of training is
required to achieve a satisfactory level of performance.
We present a continuation of previous
work (Kwasny et al., 1992) which introduces
two important improvements to the system:
(1) replacement of the non-recurrent, feed-forward network with a recurrent one, which
is smaller, but still classifies correctly; (2)
development of a frontend processor on a
Nextfi workstation to facilitate sample
recording and data acquisition. This is important for large-scale data acquisition, training,
and testing of the network.
Introduction

Efficient Restoration of Multicolor Images with
Independent Noise
Yuri Boykov Olga Veksler Ramin Zabih
Cornell University,   USA
Keywords: Bayesian image restoration; Markov random fields; max flow-min cut
Abstract
We consider the problem of maximum a posteriori (MAP) restoration of multicolor images
where each pixel has been degraded by independent arbitrary noise. We assume that the
prior distribution is given by a Markov random field with only pairwise site interactions.
Two classes of site interactions are considered: two-valued site interactions, which form a
generalized Potts model; and linear site interactions. We give efficient algorithms based on
graph cuts for both classes. The MAP estimate for a generalized Potts model can be computed by solving a multiway minimum cut problem on a graph. While this graph problem is
computationally intractable, there are fast algorithms for computing provably good approximations. The MAP estimate with linear site interactions can be computed exactly by solving
a minimum cut problem on a graph. This can be performed in nearly linear time.
1 Introduction

Coherence of an inference is equivalent to
existence of a countably additive prior
Yuri Boykov
August 1996
1 Introduction

Client/Server Architectures for Business Information Systems Page 1
Client/Server Architectures for Business
Information Systems
A Pattern Language
Klaus Renzel
sd&m GmbH & Co. KG
Project ARCUS 1
Thomas-Dehler-Str. 27
D-81737 Mnchen, Germany
email: Klaus.Renzel@sdm.de
Phone: +49-89-63812-251
http://www.sdm.de/g/arcus
Wolfgang Keller
EA Generali
Reumannplatz 7
A-1100 Vienna, Austria
email: WofgangWKeller@compuserve.com
Phone: +43-1-53401-0
Copyright 1997, Klaus Renzel, Wolfgang Keller
Permission granted to copy for PLoP97 Conference.
All other rights reserved.
Abstract: This paper presents several patterns for distributing business information systems that are structured according to a layered architecture. 2 Each distribution pattern cuts the architecture into different client and server components. All
the patterns presented give an answer to the same question: How do I distribute a
business information system? However, the consequences of applying the patterns
are very different with regards to the forces influencing distributed systems design.
1 Introduction

07/17/97   10:13 1 of 8
The Abstract Class Pattern
Bobby Woolf
Knowledge Systems Corp.
4001 Weston Pkwy, Cary, NC 27513-2303
919-677-1119 x541,   bwoolf@ksccary.com
ABSTRACT CLASS Class Behavioral
Intent
Define the interface for a hierarchy of classes while deferring the implementation to subclasses.
Abstract Class lets subclasses redefine the implementation of an interface while preserving the
polymorphism of those classes.
Also Known As
Liskov Substitution Principle [LW93], Design by Contract [Meyer91], Base Class [Auer95] ,
Template Class [Woolf97]
Motivation
+PAGE+

Semantic Query Caching for Heterogeneous Databases
Parke Godfrey
U.S. Army Research Laboratory
2800 Powder Mill Road
Adelphi, Maryland 20783-1197
U.S.A.
godfrey@arl.mil
Jarek Gryz
Department of Computer Science
York University
Toronto, Ontario M3J 1P3
Canada
jarek@cs.yorku.ca
Abstract
Query caching can play a vital role in heterogeneous, multi-database environments. Answers to a query that are available in cache
at the local client can be returned to the user
quickly, while the rest of the query is evaluated. The use of caches can optimize query
evaluation. By caching certain sensitive data
locally, caches can be used to answer the parts
of queries that involve the sensitive data, so it
need not be shipped across the network. Most
prior cache schemes have been tuple-based or
page-based. It is unclear, however, how these
might be adapted for multi-databases. We explore a more flexible semantic query caching
(SQC) approach. In SQC, caches are the answer sets of previous queries, labeled by the
query expressions that produced them. We
promote developing the technology, based on
logic, to manipulate semantic caches, to determine when and how caches can be used to answer subsequent queries, and to optimize via
cache use.
The copyright of this paper belongs to the papers authors. Permission to copy without fee all or part of this material is granted
provided that the copies are not made or distributed for direct
commercial advantage.
Proceedings of the 4th KRDB Workshop
Athens, Greece, 30-August-1997
(F. Baader, M.A. Jeusfeld, W. Nutt, eds.)
http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-8/
1 Introduction

Revisiting the Paxos algorithm
Roberto De Prisco ? , Butler Lampson, Nancy Lynch
MIT Laboratory for Computer Science
545 Technology Square NE43, Cambridge, MA 02139, USA.
Abstract. This paper develops a new I/O automaton model called the
Clock General Timed Automaton (Clock GTA) model. The Clock GTA
is based on the General Timed Automaton (GTA) of Lynch and Vaan-
drager. The Clock GTA provides a systematic way of describing timing-
based systems in which there is a notion of "normal" timing behavior,
but that do not necessarily always exhibit this "normal" behavior. It can
be used for practical time performance analysis based on the stabilization
of the physical system.
We use the Clock GTA automaton to model, verify and analyze the
paxos algorithm. The paxos algorithm is an efficient and highly fault-
tolerant algorithm, devised by Lamport, for reaching consensus in a distributed system. Although it appears to be practical, it is not widely
known or understood. This paper contains a new presentation of the
paxos algorithm, based on a formal decomposition into several interacting components. It also contains a correctness proof and a time performance and fault-tolerance analysis.
Keywords: I/O automata models, formal verification, distributed con
sensus, partially synchronous systems, fault-tolerance
1 Introduction

Finding Maximum Flows in Undirected Graphs Seems Easier than
Bipartite Matching +L +
David R. Karger and Matthew S. Levine
Abstract
Consider an n-vertex, m-edge, undirected graph with maximum flow value v. We give a method to find augmenting
paths in such a graph in amortized sub-linear (O(n
p
v)) time
per path. This lets us improve the time bound of the classic augmenting path algorithm to O(m + nv 3=2 ) on simple
graphs. The addition of a blocking flow subroutine gives a
simple, deterministic O(nm 2=3 v 1=6 )-time algorithm. We also
use our technique to improve known randomized algorithms,
giving O(m+nv 5=4 )-time and O(m+n 11=9 v)-time algorithms
for capacitated undirected graphs. For simple graphs, in
which v n, the last bound is O(n 2:2 ), improving on the best
previous bound of O(n 2:5 ), which is also the best known time
bound for bipartite matching.
1 Introduction

Bounds on the Time to Detect Failures
Using Bounded-capacity Message Links
Stephen Ponzio
MIT Laboratory for Computer Science
Abstract
We consider a system of distributed processors that
communicate by passing messages and that have inexact information about time. Specifically, a processor knows that a single message is delayed by at most
time d and the time between any two of its consecutive steps is at least c 1 and at most c 2 ; it has no
other way of estimating elapsed time. This simple
model is very close to traditional models used in distributed computing theory, and has been studied by
Attiya and Lynch [2, 1] among others. We extend
the model by making a realistic assumption about
how the delay of messages is affected by the rate at
which they are sent. We define a model of message
links with bounded capacity, which are guaranteed to
deliver messages at only a given rate. If a processor sends messages at a greater rate, they may incur
greater delay.
We quantify the effect of this bounded capacity on
the time necessary to detect processor failures. We
consider a system of two processors connected by a
bi-directional message link of (integral) capacity .
First we give two very simple protocols that guarantee any stopping failure will be detected within
time 2Cd + d and C 2 d= + Cd + d respectively,
where C = c 2 =c 1 . The main result is an almost-matching lower bound of 2Cd+d= or C 2 d=+Cd+d,
whichever is less. If the link is uni-directional, our result specializes to give a matching upper and lower
bound of C 2 d= + Cd + d.
Supported by an NSF Graduate Fellowship
1 Introduction

Electronic Lottery Tickets as Micropayments
Ronald L. Rivest
MIT Lab for Computer Science
(RSA / Security Dynamics)
rivest@theory.lcs.mit.edu
Abstract. We present a new micropayment scheme based on the use of
"electronic lottery tickets." This scheme is exceptionally efficient since
the bank handles only winning tickets, instead of handling each micro
payment.
1 Introduction

The Design and Implementation of SOLAR,
a Portable Library for Scalable Out-of-Core Linear Algebra Computations
Sivan Toledo Fred G. Gustavson
IBM T.J. Watson Research Center
Abstract
SOLAR is a portable high-performance library for out-of-core dense
matrix computations. It combines portability with high performance
by using existing high-performance in-core subroutine libraries and
by using an optimized matrix input-output library. SOLAR works on
parallel computers, workstations, and personal computers. It supports
in-core computations on both shared-memory and distributed-memory
machines, and its matrix input-output library supports both conventional I/O interfaces and parallel I/O interfaces. This paper discusses
the overall design of SOLAR, its interfaces, and the design of several
important subroutines. Experimental results show that SOLAR can
factor on a single workstation an out-of-core positive-definite symmetric matrix at a rate exceeding 215 Mflops, and an out-of-core general
matrix at a rate exceeding 195 Mflops. Less than 16% of the running
time is spent on I/O in these computations. These results indicate
that SOLAR's portability does not compromise its performance. We
expect that the combination of portability, modularity, and the use of
a high-level I/O interface will make the library an important platform
for research on out-of-core algorithms and on parallel I/O.
1 Introduction

Compiler Technology for Portable Checkpoints
Volker Strumpen
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, MA 02139
strumpen@theory.lcs.mit.edu
Abstract
We have implemented a prototype compiler called porch that transforms C programs into C programs supporting portable checkpoints. Portable checkpoints capture the state of a computation in
a machine-independent format that allows the transfer of computations across binary incompatible machines. We introduce source-to-source compilation techniques for generating code to save and
recover from such portable checkpoints automatically. These techniques instrument a program with code that maps the state of a computation into a machine-independent representation and vice versa.
In particular, the following problems are addressed: (1) providing
stack environment portability, (2) enabling conversion of complex
data types, and (3) rendering pointers portable. Experimental results show that the overhead of checkpointing is reasonably small,
even if data representation conversion is required for portability.
1 Introduction

Regression shrinkage and selection via the lasso
Robert Tibshirani
Department of Statistics
and
Division of Biostatistics
Stanford University
Abstract
We propose a new method for estimation in linear models. The "lasso"
minimizes the residual sum of squares subject to the sum of the absolute
value of the coefficients being less than a constant. Because of the nature
of this constraint it tends to produce some coefficients that are exactly zero
and hence gives interpretable models. Our simulation studies suggest that
the lasso enjoys some of the favourable properties of both subset selection
and ridge regression. It produces interpretable models like subset selection
and exhibits the stability of ridge regression. There is also an interesting
relationship with recent work in adaptive function estimation by Donoho
and Johnstone. The lasso idea is quite general and can be applied in a
variety of statistical models: extensions to generalized regression models
and tree-based models are briefly described.
Keywords: regression, subset selection, shrinkage, quadratic programming.
1 Introduction

Task Driven Perceptual Organization for Extraction of Rooftop
Polygons
Christopher Jaynes Frank Stolle Robert Collins
Computer Science Department
University of Massachusetts
Amherst, MA 01003
Email: @cs.umass.edu
Abstract
A new method for extracting planar polygonal rooftops in monocular aerial imagery is proposed. Through bottom-up and top-down construction of perceptual groups, polygons in a
single aerial image can be robustly extracted.
Orthogonal corners and lines are extracted and
hierarchically related using perceptual grouping techniques. Top-down feature verification is
used so that features, and links between the features, are verified with local information in the
image and weighed in a graph structure according to the underlying support for each feature.
Cycles in the graph correspond to possible
building rooftop hypotheses. Virtual features
are hypothesized for the perceptual completion
of partial rooftops. Extraction of the "best"
grouping of features into a building rooftop hypothesis is posed as a graph search problem.
The maximally weighted, independent set of cycles in the graph is extracted as the final set of
roof boundaries.
1. Introduction

M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 368
Appears in: Fourth European Conference on Computer Vision, Cambridge, UK, April 1996.
Generalized Image Matching:
Statistical Learning of Physically-Based Deformations
Chahab Nastar, Baback Moghaddam and Alex Pentland
Perceptual Computing Section, The Media Laboratory,
Massachusetts Institute of Technology
20 Ames Street, Cambridge MA 02139, U.S.A.
Abstract
We describe a novel approach for image matching
based on deformable intensity surfaces. In this
approach, the intensity surface of the image
is modeled as a deformable 3D mesh in the
(x; ; I(x; )) space. Each surface point has 3
degrees of freedom, thus capturing fine surface
changes. A set of representative deformations
within a class of objects (e.g. faces) are statistically learned through a Principal Components Analysis, thus providing a priori knowledge
about object-specific deformations. We demonstrate the power of the approach by examples
such as image matching and interpolation of
missing data. Moreover this approach dramatically reduces the computational cost of solving
the governing equation for the physically based
system by approximately three orders of magni
tude.
1 Introduction

Determining 3-D Hand Motion
James Davis Mubarak Shah
Media Lab Computer Vision Lab
Massachusetts Institute of Technology University of Central Florida
Cambridge, MA 02139 Orlando, FL 32826
Abstract
This paper presents a glove-free method for tracking
hand movements using a set of 3-D models. In this
approach, the hand is represented by five cylindrical
models which are fit to the third phalangeal segments
of the fingers. Six 3-D motion parameters for each
model are calculated that correspond to the movement
of the fingertips in the image plane. Trajectories of
the moving models are then established to show the 3-D nature of hand motion.
1 Introduction

Optimal Algorithms for Substrate Testing
in Multi-Chip Modules
Andrew B. Kahng, Gabriel Robins and Elizabeth A. Walkup
Department of Computer Science, UCLA,   Los Angeles, CA 90024-1596
y Department of Computer Science,   University of Virginia, Charlottesville, VA, 22903-2442
z Department of Computer Science,   University of Washington, Seattle, WA 98195
Abstract
Multi-chip module (MCM) packaging techniques present several new technical challenges, notably substrate testing. We formulate MCM substrate testing as a problem of connectivity verification in trees via k-probes, and present a linear-time algorithm which computes a minimum
set of probes achieving complete open fault coverage. Since actual substrate testing also involves
scheduling probe operations, we formulate efficient probe scheduling as a special type of metric
traveling salesman optimization and give a provably-good heuristic. Empirical results using both
random and industry benchmarks demonstrate reductions in testing costs of up to 21% over previous methods. We conclude with generalizations to alternate probe technologies and several open
problems.
Key words: MCMs, testing, graph algorithms, fault detection, circuit probing, TSP.
1 Introduction

Efficient Gate Delay Modeling for Large Interconnect Loads
Andrew B. Kahng and Sudhakar Muddu
UCLA Computer Science Department,   Los Angeles, CA 90095-1596 USA
abk@cs.ucla.edu, sudhakar@cs.ucla.edu
Abstract
With fast switching speeds and large interconnect trees (MCMs), the
resistance and inductance of interconnect has a dominant impact on
logic gate delay. In this paper, we propose a new P model for distributed RC and RLC interconnects to estimate the driving point admittance at the output of a CMOS gate. Using this model we are able to
compute the gate delay efficiently, within 25% of SPICE-computed delays. Our parameters depend only on total interconnect tree resistance
and capacitance at the output of the gate. Previous effective load capacitance methods [7, 9], applicable only for distributed RC interconnects, are based on P model parameters obtained via a recursive admittance moment computation. Our model should be useful for iterative
optimization of performance-driven routing or for estimation of gate
delay and rise times in high-level synthesis.
Keywords: gate delay, reduced-order models, driving point admittance,
effective capacitance, interconnect modeling
1 Introduction

Gate Load Delay Computation Using Analytical Models
Andrew B. Kahng Sudhakar Muddu
UCLA Computer Science Department Silicon Graphics, Inc.
Los Angeles, CA 90095-1596 USA Mountain View, CA 94039 USA
abk@cs.ucla.edu muddu@mti.sgi.com
Abstract
With submicron technologies, gate delays are dominated by gate
load delays rather than intrinsic gate delays. While the common approach for computing gate load delay (or total gate delay) is through
delay tables (or k-factor equations), there are important methodology problems associated with the delay table approach. In this paper, we propose a gate driver model with a Thevenin equivalent circuit consisting of a ramp voltage source whose slew time is obtained
from the gate slew tables, and a driver resistance in series with the
gate load. We then develop analytical gate delay formulas using this
Thevenin driver model and modeling the load with various gate load
models under both rising and falling ramp input.
1 Introduction

Robust IP Watermarking Methodologies for Physical Design
Andrew B. Kahng, Stefanus Mantik, Igor L. Markov, Miodrag Potkonjak,
Paul Tucker , Huijuan Wang and Gregory Wolfe
UCLA Computer Science Dept.,   Los Angeles, CA 90095-1596
UCSD Computer Science & Engineering Dept.,   La Jolla, CA 92093-0114
Abstract
Increasingly popular reuse-based design paradigms create a pressing need for authorship enforcement techniques that protect the intellectual property rights of designers. We develop the first intellectual property protection protocols for embedding design watermarks at the physical design level. We demonstrate that these protocols are transparent with respect to existing industrial tools and
design flows, and that they can embed watermarks into real-world
industrial designs with very low implementation overhead (as measured by such standard metrics as wirelength, layout area, number
of vias, routing congestion and CPU time). On several industrial
test cases, we obtain extremely strong, tamper-resistant proofs of
authorship for placement and routing solutions.
1 Introduction

Concurrency and Recovery in Generalized Search Trees
Marcel Kornacker
U. C. Berkeley
http://www.cs.berkeley.edu/
marcel
marcel@cs.berkeley.edu
C. Mohan
IBM Research Division
http://www.almaden.ibm.com/
cs/people/mohan
mohan@almaden.ibm.com
Joseph M. Hellerstein
U. C. Berkeley
http://www.cs.berkeley.edu/
jmh
jmh@cs.berkeley.edu
Abstract
This paper presents general algorithms for concurrency control in
tree-based access methods as well as a recovery protocol and a
mechanism for ensuring repeatable read. The algorithms are developed in the context of the Generalized Search Tree (GiST) data
structure, an index structure supporting an extensible set of queries
and data types. Although developed in a GiST context, the algorithms are generally applicable to many tree-based access methods.
The concurrency control protocol is based on an extension of the
link technique originally developed for B-trees, and completely
avoids holding node locks during I/Os. Repeatable read isolation is
achieved with a novel combination of predicate locks and two-phase
locking of data records. To our knowledge, this is the first time that
isolation issues have been addressed outside the context of B-trees.
A discussion of the fundamental structural differences between B-trees and more general tree structures like GiSTs explains why the
algorithms developed here deviate from their B-tree counterparts.
An implementation of GiSTs emulating B-trees in DB2/Common
Server is underway.
1 Introduction

Quick Simulation of ATM Buffers with
On-off Multiclass Markov Fluid Sources 1
G. Kesidis
E & CE Dept, University of Waterloo,   Waterloo, Ontario, N2L 3G1, Canada.
J. Walrand
EECS Dept, University of California,   Berkeley, CA94720.
ACM TOMACS, Vol. 3, No. 3, pp. 269-276, July, 1993.
Abstract
The problem we address is how to quickly estimate by simulation the loss in a
buffer with multiclass on-off Markov fluid sources. We generate the Markov fluids
with the altered rate matrices given in [11], instead of the originals, to speed up
the simulation. Likelihood ratios are used to recover an estimate of the loss for the
original traffic parameters.
1 Introduction

Information Theory,
Inference,
and Learning Algorithms
David J.C. MacKay
c fl1995, 1996, 1997, 1998
Draft 1.6.1   August 31, 1998
+PAGE+

Inferring Reduced Ordered Decision Graphs of Minimal Description
Length
Arlindo L. Oliveira
Alberto Sangiovanni-Vincentelli
Dept. of EECS, UC Berkeley,   Berkeley CA 94720
May 17, 1994
1 Introduction

Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
A Method for Automatic Design Error Location and
Correction in Combinational Logic Circuits
AYMAN M. WAHBA AND DOMINIQUE BORRIONE
Modelisation et Preuves de Circuits, TIMA Laboratory,   BP 53X, 38041 Grenoble Cedex FRANCE
Ayman.Wahba@imag.fr, Dominique.Borrione@imag.fr
Received ??. Revised ??.
Abstract. We present a new diagnostic algorithm, based on backward-propagation, for localising design
errors in combinational logic circuits. Three hypotheses are considered, that cover all single gate replacement and insertion errors. Diagnosis-oriented test patterns are generated in order to rapidly reduce the
suspected area where the error lies. The originality of our method is the use of patterns which do not
detect the error, in addition to detecting patterns. A theorem shows that, in favourable cases, only two
patterns suffice to get a correction. We have implemented the test generation and diagnosis algorithms.
Results obtained on benchmarks show that the error is always found, after the application of a small
number of test patterns, with an execution time proportional to the circuit size.
Keywords: design correctness, design debugging, design error diagnosis
1. Introduction

VIS : A System for Verification and Synthesis
Robert K. Brayton Gary D. Hachtel Alberto Sangiovanni-Vincentelli
Fabio Somenzi Adnan Aziz Szu-Tsung Cheng Stephen Edwards Sunil Khatri
Yuji Kukimoto Abelardo Pardo Shaz Qadeer Rajeev K. Ranjan
Shaker Sarwary Thomas R. Shiple Gitanjali Swamy Tiziano Villa
1 Introduction

INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE
Latch Optimization in Circuits Generated from High-level
Descriptions
Ellen M. Sentovich, Horia Toma, Gerard Berry
N 2943
Juillet 1996
THE ME 1
+PAGE+

Using Don't Cares in Logic Minimization for
LUT-Based FPGAs
Philip Chong  13327872
May 25, 1997
Abstract
Don't care information has proven to be useful in logic minimization.
Here, the use of don't care information in network collapsing for mapping
to LUT-based FPGAs is explored. Results are shown which indicate that
this approach does not result in appreciable improvements in network size.
1 Introduction

An Assume-Guarantee Rule For Checking Simulation
T.A. Henzinger S. Qadeer S.K. Rajamani S. Ta~sran
EECS Department, University of California at Berkeley,   CA 94720
Abstract
The simulation preorder on state transition systems is widely accepted as a
useful notion of refinement, both in its own right and as an efficiently checkable sufficient condition for trace containment. For composite systems, due to
the exponential explosion of the state space, there is a need for decomposing a
simulation check of the form P s Q into simpler simulation checks on the components of P and Q. We present an assume-guarantee rule that enables such a
decomposition. To the best of our knowledge, this is the first assume-guarantee
rule that applies to a refinement relation different from trace containment. Our
rule is circular, and its soundness proof requires induction on trace-trees. The
proof is constructive: given simulation relations that witness the simulation
preorder between components, we provide a procedure for constructing a witness relation for P s Q. We also extend our assume-guarantee rule to account
for fairness assumptions on transition systems.
1 Introduction

Submitted to ACM SIGCOMM '98
Modeling TCP Throughput: A Simple Model and its Empirical Validation
Jitendra Padhye Victor Firoiu Don Towsley Jim Kurose
jitu@cs.umass.edu vfiroiu@cs.umass.edu towsley@cs.umass.edu kurose@cs.umass.edu
1-413-545-2447 1-413-545-3179 1-413-545-0207 1-413-545-1585
Department of Computer Science
University of Massachusetts
LGRC, Box 34610
Amherst, MA 01003-4610 USA
May 29, 1998
Abstract
In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount
of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP's fast
retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling
perspective, as almost all of our TCP traces contained more timeout events than fast retransmit events.
Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is
accurate over a wider range of loss rates.
This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807
and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors
and do not necessarily reflect the views of the National Science Foundation.
Corresponding Author
http://www.cs.umass.edu/~vfiroiu/
+PAGE+

Exotica/FMQM: A Persistent
Message-Based Architecture for
Distributed Workflow Management
G. Alonso, C. Mohan, R. Gunthor
IBM Almaden Research Center
650 Harry Road, San Jose, CA 95120, USA.
E-mail: fgustavoa,rgunther,mohang@almaden.ibm.com
D. Agrawal, A. El Abbadi
Computer Science Department, UC Santa Barbara,
Santa Barbara, CA 93106, USA.
E-mail: fagrawal,amrg@cs.ucsb.edu
M. Kamath
Computer Science Department, UM at Amherst,
Amherst, MA 01003, USA.
E-mail: kamath@cs.umass.edu
Abstract
In the past few years there has been an increasing interest in workflow applications as a
way of supporting complex business processes in modern corporations. Given the nature
of the environment and the technology involved, workflow applications are inherently
distributed and pose many interesting challenges to the system designer. In most cases, a
client/server architecture is used in which knowledge about the processes being executed is
centralized in one node to facilitate monitoring, auditing, and to simplify synchronization.
In this paper, we explore a novel distributed architecture, Exotica/FMQM, for workflow
systems in which the need for such a centralized database is eliminated. Instead, we use
persistent messages as the means to store the information relevant to the execution of a
business process. Our approach is to completely distribute the execution of a process so
individual nodes are independent. The advantages of this approach are increased resilience
to failures and greater scalability and flexibility of the system configuration.
Keywords
Workflow Management Systems, Distributed Systems, reliability, scalability.
+PAGE+

Use of Architecture-Altering Operations to Dynamically
Adapt a Three-Way Analog Source Identification Circuit to
Accommodate a New Source
John R. Koza
Computer Science Dept.
Stanford University
Stanford, California 94305-9020
koza@cs.stanford.edu
http://www-cs
faculty.stanford.edu/~koza/
Forrest H Bennett III
Visiting Scholar
Computer Science Dept.
Stanford University
Stanford, California 94305
forrest@evolute.com
Jason Lohn
Visiting Scholar
Computer Science Dept.
Stanford University
Stanford, California 94305
jlohn7@leland.stanford.edu
Frank Dunlap
Dunlap Consulting
Palo Alto, California
Martin A. Keane
Martin Keane Inc.
5733 West Grover
Chicago, Illinois 60630
makeane@ix.netcom.com
David Andre
Computer Science Division
University of California
Berkeley, California
dandre@cs.berkeley.edu
ABSTRACT
The problem of source
identification involves correctly
classifying an incoming signal into
a category that identifies the
signal's source.
The problem is difficult because
information is not provided
distinguishing characteristics and
because successive signals from the
same source differ. The source
identification problem can be made
more difficult by dynamically
changing the repertoire of sources
while the problem is being solved.
We used genetic programming to
evolve both the topology and the
sizing (numerical values) for each
component of an analog electrical
circuit that can correctly classify an
incoming analog electrical signal
into three categories. Then, the
repertoire of sources was
dynamically changed by adding a
new source during the run. The
paper describe show the
enabled genetic programming to
adapt, during the run, to the
changed environment. Specifically,
a three-way source identification
circuit was evolved and then
adapted into a four-way classifier,
during the run, thereby successfully
handling the additional new source.
1. Introduction

Planar-Adaptive Routing: Low-cost Adaptive Networks for
Multiprocessors
Andrew A. Chien and Jae H. Kim
achien@cs.uiuc.edu kim@cs.uiuc.edu
Department of Computer Science
University of Illinois at Urbana-Champaign
1304 W. Springfield Avenue
Urbana, IL 61801
Abstract
Network throughput can be increased by allowing mul-tipath, adaptive routing. Adaptive routing allows more
freedom in the paths taken by messages, spreading load
over physical channels more evenly. The flexibility of
adaptive routing introduces new possibilities of deadlock. Previous deadlock avoidance schemes in k-ary n-cubes require an exponential number of virtual channels
[17]. We describe a family of deadlock-free routing algorithms, called planar-adaptive routing algorithms which
require only a constant number of virtual channels, independent of network size and dimension. Planar-adaptive
routing algorithms reduce the complexity of deadlock
prevention by reducing the number of choices at each
routing step. In the fault-free case, planar-adaptive
networks are guaranteed to be deadlock-free. In the
presence of network faults, the planar-adaptive router
can be extended with misrouting to produce a working network which remains provably deadlock free and
is provably livelock free. In addition, planar adaptive
networks can simultaneously support both in-order and
adaptive, out-of-order packet delivery.
Planar-adaptive routing is of practical significance. It
provides the simplest known support for deadlock-free
adaptive routing in k-ary n-cubes of more than two
dimensions (with k &gt; 2). Restricting adaptivity reduces the hardware complexity, improving router speed
or allowing additional performance-enhancing network
features. The structure of planar-adaptive routers is
amenable to efficient implementation.
Keywords: Multicomputers, Routing Networks, Adaptive Routing, Deadlock Avoidance, Transmission-Order
Preservation, Fault Tolerance
1 Introduction

Learning to Retrieve Information
Brian Bartell
Encylopdia Britannica and
Institute for Neural Computation
Computer Science & Engineering
University of California, San Diego
La Jolla, California 92093
Garrison W. Cottrell
Institute for Neural Computation
Computer Science & Engineering
University of California, San Diego
La Jolla, California 92093
Rik Belew
Institute for Neural Computation
Computer Science & Engineering
University of California, San Diego
La Jolla, California 92093
Abstract
Information retrieval differs significantly from function approximation in that the goal is for the system to achieve the same ranking
function of documents relative to queries as the user: the outputs of
the system relative to one another must be in the proper order. We
hypothesize that a particular rank-order statistic, Guttman's point
alienation, is the proper objective function for such a system, and
demonstrate its efficacy by using it to find the optimal combination
of retrieval experts. In application to a commercial retrieval system,
the combination performs 47% better than any single expert.
1 Introduction

Segregating Planners and Their Environments
Scott D. Anderson
Paul R. Cohen
Experimental Knowledge Systems Laboratory
Computer Science Department, LGRC
University of Massachusetts
Amherst MA 01003-4610
fanderson,coheng@cs.umass.edu
To be published in the proceedings of the
Spring Symposium on Integrated Planning Applications
Abstract
By implementing agents and environments using a domain-independent, extensible simulation substrate, described in this
paper, agents will have clean interfaces to
their environments. These makes it easier
for agents to be plugged into other environments that have been similarly defined. If
agents can interact with multiple environments, their behaviors and the associated
experimental results will be more general
and interesting.
1 Introduction

Higher Bandwidth X
(Extended Abstract)
Submitted to ACM MULTIMEDIA '94
John Danskin
Princeton University
Computer Science Department
Princeton NJ, 08540
jmd@cs.princeton.edu
Abstract
Network bandwidth has always been a key issue for multimedia protocols. Many potential users of networked multimedia protocols will continue to have low bandwidth network
connections for some time: copper wire ISDN, infra-red, cellular modems, etc.. Compression provides potential relief for users of slow networks by increasing effective bandwidth.
HBX introduces a new technique, based on arithmetic coding and statistical modeling, for
compressing structured data. Applied to the X networked graphics protocol, this technique
yields 4.5:1 compression across a representative set of traces, performing twice as well as the
popular LZW-based Xremote compression protocol. HBX's coding techniques are generally
applicable to the graphics and imaging subset of multimedia protocols. Future work will
determine whether HBX's coding techniques can be applied to audio and video streams as
well.
1 Introduction

Fast Volume Rendering Using a Shear-Warp Factorization
of the Viewing Transformation
Philippe Lacroute
Computer Systems Laboratory
Stanford University
Marc Levoy
Computer Science Department
Stanford University
Abstract
Several existing volume rendering algorithms operate by factoring the viewing transformation into a 3D shear parallel to the data
slices, a projection to form an intermediate but distorted image,
and a 2D warp to form an undistorted final image. We extend
this class of algorithms in three ways. First, we describe a new
object-order rendering algorithm based on the factorization that is
significantly faster than published algorithms with minimal loss
of image quality. Shear-warp factorizations have the property that
rows of voxels in the volume are aligned with rows of pixels in the
intermediate image. We use this fact to construct a scanline-based
algorithm that traverses the volume and the intermediate image in
synchrony, taking advantage of the spatial coherence present in
both. We use spatial data structures based on run-length encoding
for both the volume and the intermediate image. Our implementation running on an SGI Indigo workstation renders a 256 3 voxel
medical data set in one second. Our second extension is a shear-warp factorization for perspective viewing transformations, and
we show how our rendering algorithm can support this extension.
Third, we introduce a data structure for encoding spatial coherence
in unclassified volumes (i.e. scalar fields with no precomputed
opacity). When combined with our shear-warp rendering algorithm this data structure allows us to classify and render a 256 3
voxel volume in three seconds. The method extends to support
mixed volumes and geometry and is parallelizable.
CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism; I.3.3 [Computer Graphics]: Picture/Image
Generation|Display Algorithms.
Additional Keywords: Volume rendering, Coherence, Scientific
visualization, Medical imaging.
1 Introduction

David Laur and Pat Hanrahan
Princeton University
Princeton, NJ 08544, USA
Abstract
This paper presents a progressive refinement algorithm for
volume rendering which uses a pyramidal volume representation. Besides storing average values, the pyramid stores
estimated error, so an oct-tree can be fit to the pyramid
given a user-supplied precision. This oct-tree is then drawn
using a set of splats, or footprints, each scaled to match the
size of the projection of a cell. The splats themselves are approximated with RGBA Gouraud-shaded polygons, so that
they can be drawn efficiently on modern graphics workstations. The result is a real-time rendering algorithm suitable
for interactive applications.
CR Categories and Subject Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism.
Key Words: volume rendering, coherence, progressive refinement, interactive techniques.
1 Introduction

Compression Performance of the Xremote Protocol
John Danskin Pat Hanrahan
Department of Computer Science, Princeton University
Abstract
The Xremote protocol is a compressed transformation of the X Window System protocol,
designed to efficiently implement X connections across relatively slow serial lines. Using an
Xremote simulator and 11 traces of X sessions, we found that Xremote's overall compression
ratio is 2.4:1. This figure varies widely depending on the trace. A study of compression
ratio as a function of message type shows text based messages commonly achieving 3:1
compression, while geometric messages usually achieve only 1.6:1 compression. By examining bandwidth requirements and compression performance as a function of time, we see
that Xremote performs adequately for some applications which are text based or which use
small geometric datasets, except at application startup where more bandwidth is required.
Further work is required to adequately support the initialization stage of X applications and
medium to large geometric databases.
1 Introduction

Robust Meshes from Multiple Range Maps
Kari Pulli Tom Duchamp Hugues Hoppe
John McDonald Linda Shapiro Werner Stuetzle
University of Washington,   Seattle, WA
Microsoft Research,   Redmond, WA
Abstract
This paper presents a method for modeling the surface
of an object from a sequence of range maps. Our method
is based on a volumetric approach that produces a compact
surface without boundary. It provides robustness through
the use of interval analysis techniques and computational
efficiency through hierarchical processing using octrees.
1. Introduction

Constraint Satisfaction Problem as a Causal Theory
Robert Rodosek
University of Munich, Department of Computer Science
Theresienstrae 39, 80333 Munich, Germany
rodosek@informatik.uni-muenchen.de
Abstract
The aim of this paper is to identify and to characterize the features that render one class
of the Constraint Satisfaction Problem (CSP) computationally more efficient. Our approach
is to search for a causal structure not only in the topology of the subsets of variables upon
which the constraints are specified, but also in the nature of the constraints. Basically, there
should exist an ordering of variables in the system such that an assignment of the variables
can be reached without backtracking. First, an approach of a causal structure is formulated,
and second, an efficient procedure is provided (i) for deciding if such an ordering of variables
exists and, (ii) for identifying such an ordering whenever possible.
1 Introduction

Dis-equality Constraints in Linear/Integer
Programming
Mozafar T. Hajian
IC-Parc, William Penny Laboratory, Imperial College,   London, SW7 2AZ.
mh10@doc.ic.ac.uk
June 21, 1996
Abstract
We have proposed an extension to the definition of general integer linear programs (ILP) to accept dis-equality constraints explicitly. A new class of logical
variables is introduced to transform the extended ILP in general form to standard
form. Branch and Bound algorithm is modified to solve this new class of ILP.
Keywords: Mathematical Modelling, Linear/Integer Programming, Algorithms,
Branch and Bound, Dis-equality Constraints.
+PAGE+

THE AUTOMATED HIGHWAY SYSTEM: A TRANSPORTATION
TECHNOLOGY FOR THE 21ST CENTURY
M. Broucke and P. Varaiya 1
Department of Electrical Engineering and Computer Science
University of California,   Berkeley CA 94720
mire, varaiya@eclair.eecs.berkeley.edu
Abstract. The current vehicle-highway system has reached a plateau in its ability
to meet the demand for moving goods and people. We sketch an architecture for
an automated highway system or AHS. The architecture can be realized by several
designs that differ in terms of performance and sophistication. We describe one design
that could triple capacity and reduce travel time; guarantee collision-free operation
in the absence of malfunctions; limit performance degradation in the case of faults;
and reduce emissions by half. We summarize evidence suggesting that the design can
be implemented. We indicate how the design can be adapted to different urban and
rural scenarios and how a standard land use model can show the impact of AHS on
urban density. We conclude with a critique of AHS.
Keywords. Automated vehicles, hierarchical control
1. INTRODUCTION

A Parallel Implementation of an MPEG1 Encoder:
Faster Than Real-Time!
Ke Shen , Lawrence A. Rowe and Edward J. Delp
Computer Vision and Image Processing Laboratory
School of Electrical Engineering
Purdue University
West Lafayette, Indiana
Computer Science Division
Department of Electrical Engineering and Computer Science
University of California
Berkeley, California
ABSTRACT
In this paper we present an implementation of an MPEG1 encoder on the Intel Touchstone Delta and Intel
Paragon parallel computers. We describe the unique aspects of mapping the algorithm onto the parallel
machines and present several versions of the algorithms. We will show that I/O contention can be a bottleneck
relative to performance. We will also describe how the Touchstone Delta and Paragon can be used to compress
video sequences faster than real-time.
1. INTRODUCTION

Kin Recognition, Similarity, and Group Behavior
Maja J Mataric
MIT Artificial Intelligence Laboratory
545 Technology Square #721
Cambridge, MA 02139
phone: (617) 253-8839
fax: (617) 253-0039
maja@ai.mit.edu
Abstract
This paper presents an approach to describing
group behavior using simple local interactions
among individuals. We propose that for a given
domain a set of basic interactions can be defined
which describes a large variety of group behaviors.
The methodology we present allows for simplified
qualitative analysis of group behavior through the
use of shared goals, kin recognition, and minimal
communication. We also demonstrate how these
basic interactions can be simply combined into
more complex compound group behaviors.
To validate our approach we implemented an array of basic group behaviors in the domain of spatial interactions among homogeneous agents. We
describe some of the experimental results from two
distinct domains: a software environment, and a
collection of 20 mobile robots. We also describe
a compound behavior involving a combination of
the basic interactions. Finally, we compare the
performance of homogeneous groups to those of
dominance hierarchies on the same set of basic behaviors.
Introduction

Structuring Graphical Paradigms in TkGofer
Koen Claessen
OGI and Utrecht University
koen@cse.ogi.edu
Ton Vullinghs
Universitat Ulm
ton@informatik.uni-ulm.de
Erik Meijer
OGI and Utrecht University
erik@cse.ogi.edu
Abstract
In this paper we describe the implementation of several
graphical programming paradigms (Model View Controller,
Fudgets, and Functional Animations) using the GUI library
TkGofer. This library relies on a combination of monads
and multiple-parameter type classes to provide an abstract,
type safe interface to Tcl/Tk. We show how choosing the
right abstractions makes the given implementations surprisingly concise and easy to understand.
1 Introduction

REWRITING METHODS
FOR WORD PROBLEMS
NACHUM DERSHOWITZ
Department of Computer Science, University of Illinois at Urbana-Champaign,
1304 West Springfield Ave., Urbana, IL 61801-2987, U.S.A.
Abstract
This paper outlines various recent approaches to solving word problems.
Term orderings are used to define a terminating rewrite relation. When confluent, that relation defines unique normal forms that can be used to decide word
problems. Some results obtained by these methods are summarized.
1. Introduction

Disk Packings and Planar Separators
Daniel A. Spielman
U.C. Berkeley/MIT
Shang-Hua Teng
University of Minnesota
Abstract
We demonstrate that the geometric separator algorithm of Miller, Teng, Thurston, and Vavasis finds a
3=4-separator of size 1:84
p
n for every n node planar
graph. Our bound is derived from an analysis of disk
packings on the sphere.
1. Introduction

THE PRACTICAL VALUE OF N-GRAMS IN
GENERATION
Irene Langkilde and Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
ilangkil@isi.edu and knight@isi.edu
Abstract
We examine the practical synergy between symbolic and statistical language processing in a generator
called Nitrogen. The analysis provides insight into the kinds of linguistic decisions that bigram frequency
statistics can make, and how it improves scalability. We also discuss the limits of bigram statistical
knowledge. We focus on specific examples of Nitrogen's output.
1 Introduction

Building Interpreters by Composing Monads
Guy L. Steele Jr.
Thinking Machines Corporation
245 First Street
Cambridge, Massachusetts 02142
(617) 234-2860
gls@think.com
Abstract: We exhibit a set of functions coded in
Haskell that can be used as building blocks to construct
a variety of interpreters for Lisp-like languages. The
building blocks are joined merely through functional
composition. Each building block contributes code to
support a specific feature, such as numbers, continuations, functions calls, or nondeterminism. The result of
composing some number of building blocks is a parser,
an interpreter, and a printer that support exactly the
expression forms and data types needed for the combined set of features, and no more.
The data structures are organized as pseudomonads,
a generalization of monads that allows composition.
Functional composition of the building blocks implies
type composition of the relevant pseudomonads.
Our intent was that the Haskell type resolution system ought to be able to deduce the approprate data
types automatically. Unfortunately there is a deficiency
in current Haskell implementations related to recursive
data types: circularity must be reflected statically in the
type definitions.
We circumvent this restriction by applying a purpose-built program simplifier that performs partial evaluation
and a certain amount of program algebra. We construct
a wide variety of interpreters in the style of Wadler by
starting with the building blocks and a page of boiler-plate code, writing three lines of code (one to specify the
building blocks and two to (redundantly) specify type
compositions), and then applying the simplifier. The
resulting code is acceptable Haskell code.
We have tested a dozen different interpreters with
various combinations of features. In this paper we discuss the overall code structuring strategy, exhibit several building blocks, briefly describe the partial evaluator, and present a number of automatically generated
interpreters.
This is a preprint of a paper that is to appear
in the Proceedings of the Twenty-first Annual ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages, January 1994.
1 Introduction

RESTORATION OF LOSSY COMPRESSED NOISY IMAGES
Osama K. Al-Shaykh 1 Russell M. Mersereau 2
School of Electrical and Computer Engineering
Georgia Institute of Technology
1 osamakl@eedsp.gatech.edu
2 rmm@eedsp.gatech.edu
ABSTRACT
A restoration algorithm for estimating lossy compressed
images that are corrupted by data-dependent Poisson noise
is presented. The algorithm is based on modeling the image
as a Markov random field (MRF) that penalizes the blocking artifact. The effectiveness of the proposed algorithm is
illustrated using synthetic and real images.
1. INTRODUCTION

Theory and Design of Multidimensional QMF Sub-Band Filters From
1-D Filters Using Transforms
I.A. Shah A.A.C. Kalker
Philips Research Laboratories,
P.O. Box 80.000, 5600 JA Eindhoven, The Netherlands
Net: kalker@prl.philips.nl, shah@prl.philips.nl
Abstract
The paper presents the general theory of designing
multidimensional Quadrature Mirror Filters (QMF),
for use in sub-band coding (SBC) systems, using the
McClellan transform [1]. It was recently shown that
McClellan transform could be used to generate 2-D
diamond shape QMF filters [2]. In this paper we will
formalize the proofs of the diamond shape case, and
generalize it to other shapes, sampling rasters and
dimensions. Examples are given of two dimensional
diamond shape filters and three dimensional tetrad
filters designed using the technique.
1 Introduction

Optimal Multiple Description Transform
Coding of Gaussian Vectors
Vivek K Goyal
Dept. of Elec. Eng. & Comp. Sci.
University of California, Berkeley
v.goyal@ieee.org
Jelena Kovacevic
Bell Laboratories
Murray Hill, NJ
jelena@bell-labs.com
Proc. IEEE Data Compression Conference 1998, pp. 388-397. c fl1998 IEEE
Includes minor corrections.
Abstract
Multiple description coding (MDC) is source coding for multiple channels
such that a decoder which receives an arbitrary subset of the channels may produce a useful reconstruction. Orchard et al. [1] proposed a transform coding
method for MDC of pairs of independent Gaussian random variables. This paper provides a general framework which extends multiple description transform
coding (MDTC) to any number of variables and expands the set of transforms
which are considered. Analysis of the general case is provided, which can be
used to numerically design optimal MDTC systems. The case of two variables
sent over two channels is analytically optimized in the most general setting
where channel failures need not have equal probability or be independent. It
is shown that when channel failures are equally probable and independent, the
transforms used in [1] are in the optimal set, but many other choices are possible. A cascade structure is presented which facilitates low-complexity design,
coding, and decoding for a system with a large number of variables.
1 Introduction

16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 1, JANUARY 1998
Quantized Overcomplete Expansions in R
N
Analysis, Synthesis, and Algorithms
Vivek K Goyal, Student Member, IEEE, Martin Vetterli, Fellow, IEEE,
and Nguyen T. Thao, Member, IEEE
Abstract|Coefficient quantization has peculiar qualitative
effects on representations of vectors in R N with respect to
overcomplete sets of vectors. These effects are investigated
in two settings: frame expansions (representations obtained
by forming inner products with each element of the set)
and matching pursuit expansions (approximations obtained
by greedily forming linear combinations). In both cases,
based on the concept of consistency, it is shown that traditional linear reconstruction methods are suboptimal, and
better consistent reconstruction algorithms are given. The
proposed consistent reconstruction algorithms were in each
case implemented, and experimental results are included.
For frame expansions, results are proven to bound distortion as a function of frame redundancy r and quantization
step size for linear, consistent, and optimal reconstruction
methods. Taken together, these suggest that optimal reconstruction methods will yield O(1=r 2 ) MSE, and that consistency is sufficient to insure this asymptotic behavior. A
result on the asymptotic tightness of random frames is also
proven.
Applicability of quantized matching pursuit to lossy vector compression is explored. Experiments demonstrate the
likelihood that a linear reconstruction is inconsistent, the
MSE reduction obtained with a nonlinear (consistent) reconstruction algorithm, and generally competitive performance
at low bit rates.
Keywords| quantization, source coding, frames, matching
pursuit, consistent reconstruction, optimal reconstruction,
overcomplete representations, MSE bounds
I. Introduction

Planning and Proof Planning
Erica Melis 1 and Alan Bundy 2
Abstract. The paper adresses proof planning as a specific AI planning. It describes some peculiarities of proof planning and discusses
some possible cross-fertilization of planning and proof planning.
1 Introduction

The Role of Learning in Autonomous Robots
Rodney A. Brooks   MIT Artificial Intelligence Laboratory   545 Technology Square Cambridge, MA 02139   brooks@ai.mit.edu
Abstract
Applications of learning to autonomous
agents (simulated or real) have often been
restricted to learning a mapping from perceived state of the world to the next action
to take. Often this is couched in terms of
learning from no previous knowledge. This
general case for real autonomous robots is
very difficult. In any case, when building a
real robot there is usually a lot of a priori
knowledge (e.g., from the engineering that
went into its design) which doesn't need to
be learned. We describe the behavior-based
approach to autonomous robots, and then examine four classes of learning problems associated with such robots.
1 INTRODUCTION

Appears in M. Mozer, M. Jordan and T. Petsche, eds., Advances In Neural Information Processing Systems 9, MIT Press, 1997
Predicting Lifetimes in Dynamically
Allocated Memory
David A. Cohn
Adaptive Systems Group
Harlequin, Inc.
Menlo Park, CA 94025
cohn@harlequin.com
Satinder Singh
Department of Computer Science
University of Colorado
Boulder, CO 80309
baveja@cs.colorado.edu
Abstract
Predictions of lifetimes of dynamically allocated objects can be used
to improve time and space efficiency of dynamic memory management in computer programs. Barrett and Zorn [1993] used a simple
lifetime predictor and demonstrated this improvement on a variety
of computer programs. In this paper, we use decision trees to do
lifetime prediction on the same programs and show significantly
better prediction. Our method also has the advantage that during
training we can use a large number of features and let the decision
tree automatically choose the relevant subset.
1 INTELLIGENT MEMORY ALLOCATION

A Perturbation Scheme for Spherical Arrangements
with Application to Molecular Modeling
Dan Halperin
Tel Aviv University
Christian R. Shelton
Massachusetts Institute of Technology
July 31, 1997
Abstract
We describe a software package for computing and manipulating the subdivision of a sphere
by a collection of (not necessarily great) circles and for computing the boundary surface of the
union of spheres. We present problems that arise in the implementation of the software and the
solutions that we have found for them. At the core of the paper is a novel perturbation scheme to
overcome degeneracies and precision problems in computing spherical arrangements while using
floating point arithmetic. The scheme is relatively simple, it balances between the efficiency
of computation and the magnitude of the perturbation, and it performs well in practice. We
report and discuss experimental results. Our package is a major component in a larger package
aimed to support geometric queries on molecular models; it is currently employed by chemists
working in `rational drug design.' The spherical subdivisions are used to construct a geometric
model of a molecule where each sphere represents an atom. We also give an overview of the
molecular modeling package and detail additional features and implementation issues.
This work has been supported in part by a grant from Pfizer Central Research. Dan Halperin has also been
supported by an Alon Fellowship, by ESPRIT IV LTR Project No. 21957 (CGAL), and by the Hermann Minkowski
- Minerva Center for Geometry at Tel Aviv University.
Department of Computer Science, Tel Aviv University,   Tel Aviv 69978, Israel.   E-mail:
halperin@math.tau.ac.il.
Department of Computer Science, MIT,   Cambridge, MA 02139.   E-mail: cshelton@ai.mit.edu.   Part of the
work on this paper was carried out while Christian Shelton was at the Department of Computer Science, Stanford
University
+PAGE+

Observations on Cognitive Judgments
David McAllester
dam@ai.mit.edu
Abstract
It is obvious to anyone familiar with the rules of the game of chess
that a king on an empty board can reach every square. It is true, but
not obvious, that a knight can reach every square. Why is the first
fact obvious but the second fact not? This paper presents an analytic
theory of a class of obviousness judgments of this type. Whether or
not the specifics of this analysis are correct, it seems that the study of
obviousness judgments can be used to construct integrated theories of
linguistics, knowledge representation, and inference.
This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts
Institute of Technology. Support for the work described in this paper was provided in part by
Misubishi Electric Research Laboratories, Inc. Support for the laboratory's artificial intelligence
research is provided in part by the Advanced Research Projects Agency of the Department of
Defense under Office of Naval Research contract N00014-85-K-0124.
This paper appeared in AAAI-91. A postscript electronic source for this paper can be found in
ftp.ai.mit.edu:/pub/dam/aaai91a.ps. A bibtex reference can be found in ftp.ai.mit.edu:/pub/dam/dam.bib.
+PAGE+

Taxonomic Syntax for First Order Inference
DAVID MCALLESTER and ROBERT GIVAN
Massachusetts Institute of Technology, Cambridge Massachusetts
Abstract: We identify a new polynomial time decidable fragment of first order
logic and present a general method for using polynomial time inference procedures
in knowledge representation systems. Our results indicate that a non-standard
"taxonomic" syntax is essential in constructing natural and powerful polynomial
time inference procedures. The central role of taxonomic syntax in our polynomial time inference procedures provides technical support for the often expressed
intuition that knowledge is better represented in terms of taxonomic relationships
than classical first order formulas. To use our procedures in a knowledge representation system we define a "Socratic proof system" which is complete for first
order inference and which can be used as a semi-automated interface to a first
order knowledge base.
Categories and Subject Descriptors: F.4.1 [Mathematical Logic and Formal Languages]: Mathematical logic | computational logic, mechanical theorem
proving; I.2.3 [Artificial Intelligence]: Deduction and Theorem Proving | deduction
General Terms: Deduction, Algorithms
Additional Keywords and Phrases: Proof Theory, Machine Inference, Theorem Proving, Automated Reasoning, Polynomial Time Algorithms, Inference
Rules, Proof Systems, Mechanical Verification.
This research was supported in part by National Science Foundation Grant IRI-8819624 and in part by the Advanced Research Projects Agency of the Department
of Defense under Office of Naval Research contract N00014-85-K-0124 and N00014-89-J-3202.
Author's Address:   MIT Artificial Intelligence Laboratory,   545 Technology Square,
Cambridge Mass, 02139,   DAM@ai.mit.edu
This paper appeared in JACM, vol. 40, no. 2, April 1993. A postscript electronic source
for this paper can be found in ftp.ai.mit.edu:/pub/dam/jacm1.ps. A bibtex reference can
be found in internet file ftp.ai.mit.edu:/pub/dam/dam.bib.
+PAGE+

MASSACHUSETTS INSTITUTE OF TECHNOLOGY
ARTIFICIAL INTELLIGENCE LABORATORY
A.I. Memo No. 1591   November, 1996
Complex Feature Recognition: A
Bayesian Approach for Learning to
Recognize Objects
Paul A. Viola
This publication can be retrieved by anonymous ftp to publications.ai.mit.edu.
Abstract
We have developed a new Bayesian framework for visual object
recognition which is based on the insight that images of objects can be
modeled as a conjunction of local features. This framework can be used
to both derive an object recognition algorithm and an algorithm for
learning the features themselves. The overall approach, called complex
feature recognition or CFR, is unique for several reasons: it is broadly
applicable to a wide range of object types, it makes constructing object
models easy, it is capable of identifying either the class or the identity
of an object, and it is computationally efficient requiring time proportional to the size of the image.
Instead of a single simple feature such as an edge, CFR uses a large
set of complex features that are learned from experience with model
objects. The response of a single complex feature contains much more
class information than does a single edge. This significantly reduces the
number of possible correspondences between the model and the image.
In addition, CFR takes advantage of a type of image processing called
oriented energy. Oriented energy is used to efficiently pre-process the
image to eliminate some of the difficulties associated with changes in
lighting and pose.
Copyright c Massachusetts Institute of Technology, 1996
This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts
Institute of Technology. Support for this research was provided in part by the Advanced Research
Projects Agency of the Department of Defense under Office of Naval Research contract N00014-96-1-0311.
+PAGE+

Anatomical origin and computational role of diversity
in the response properties of cortical neurons
Kalanit Grill Spectory Shimon Edelmany Rafael Malachz
Departments of Applied Mathematics and Computer Science and Neurobiology
The Weizmann Institute of Science
Rehovot 76100, Israel
fkalanit,edelmang@wisdom.weizmann.ac.il bnmalach@weizmann.weizmann.ac.il
Abstract
The maximization of diversity of neuronal response properties has been recently suggested
as an organizing principle for the formation of such prominent features of the functional
architecture of the brain as the cortical columns and the associated patchy projection patterns
(Malach, 1994). We report a computational study of two aspects of this hypothesis. First, we
show that maximal diversity is attained when the ratio of dendritic and axonal arbor sizes is
equal to one, as it has been found in many cortical areas and across species (Lund et al., 1993;
Malach, 1994). Second, we show that maximization of diversity leads to better performance in
two case studies: in systems of receptive fields implementing steerable/shiftable filters, and in
matching spatially distributed signals, a problem that arises in visual tasks such as stereopsis,
motion processing, and recognition.
1 Introduction

Toward an Analysis of Forward Pruning
Stephen J. J. Smith
Computer Science Department
University of Maryland
College Park, MD 20740
sjsmith@cs.umd.edu
Dana S. Nau
Institute for Advanced Computer Studies,
Computer Science Department,
and Institute for Systems Research
University of Maryland
College Park, MD 20740
nau@cs.umd.edu
June 30, 1993
Abstract
Several early game-playing computer programs used forward pruning (i.e., the practice of
deliberately ignoring nodes that are believed unlikely to affect a game tree's minimax value),
but this technique did not seem to result in good decision-making. The poor performance of
forward pruning presents a major puzzle for AI research on game playing, because some version
of forward pruning seems to be "what people do," and the best chess-playing programs still do
not play as well as the best humans.
As a step toward deeper understanding of how forward pruning affects quality of play, in
this paper we set up a model of forward pruning on two abstract classes of binary game trees,
and we use this model to investigate how forward pruning affects the accuracy of the minimax
values returned. The primary result of our study is that forward pruning does better when there
is a high correlation among the minimax values of sibling nodes in a game tree.
This result suggests that forward pruning may possibly be a useful decision-making technique
in certain kinds of games. In particular, we believe that bridge may be such a game.
Address correspondence to   Dana S. Nau,   Computer Science Dept., University of Maryland,
College Park, MD 20742.
This work supported in part by an AT&T Ph.D. scholarship to Stephen J. J. Smith, Maryland Industrial Part
nerships (MIPS) grant 501.15, Great Game Products, and NSF grants IRI-8907890 and NSFD CDR-88003012.
+PAGE+

Parametric Models are Versatile:
The Case of Model Based Optimization
P. Fua
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
Abstract
Model-Based Optimization (MBO) is a paradigm in which an objective function is used to express
both geometric and photometric constraints on features of interest. A parametric model of a feature
(such as a road, a building, or coastline) is extracted from one or more images by adjusting the model's
state variables until a minimum value of the objective function is obtained. The optimization procedure
yields a description that simultaneously satisfies (or nearly satisfies) all constraints, and, as a result, is
likely to be a good model of the feature.
1 Introduction

A Data-Flow Graphical User Interface for Querying a Scientific
Database
Bosco S. Tjan, Leonard Breslow, Sait Dogru, Vijay Rajan,
Keith Rieck, James R. Slagle, and Marius O. Poliac
Computer Science Department
University of Minnesota,   Minneapolis MN 55455
Abstract
We describe the design principles and functionality
of a visual query language called SeeQL that represents data retrieval and analysis operations as a data-flow graph. A query is viewed as a sequence of relational algebra and other data transformation operations applied to database tables. The language is well-suited for large-scale scientific database applications,
where data analysis is a major component and the typical queries or data retrieval patterns are unrestricted.
The language provides a flexible yet easy-to-use environment for database access and data analysis for
non-programmer research scientists. We have implemented this language in a system being used in a long-term data-intensive highway pavement research project
(MnRoad) conducted by the Minnesota Department of
Transportation.
1 Introduction

Cooperative Bayesian and Case-Based Reasoning
for Solving Multiagent Planning Tasks
David W. Aha & Li Wu Chang
Navy Center for Applied Research in AI
Naval Research Laboratory,   Code 5510
Washington, DC 20375
faha; liwug@aic.nrl.navy.mil
(202) 767-2884 / FAX: 767-3172
January 25, 1996
Abstract
We describe an integrated problem solving architecture named INBANCA in
which Bayesian networks and case-based reasoning (CBR) work cooperatively on
multiagent planning tasks. This includes two-team dynamic tasks, and this paper
concentrates on simulated soccer as an example. Bayesian networks are used to characterize action selection whereas a case-based approach is used to determine how to
implement actions. This paper has two contributions. First, we survey integrations
of case-based and Bayesian approaches from the perspective of a popular CBR task
decomposition framework, thus explaining what types of integrations have been attempted. This allows us to explain the unique aspects of our proposed integration.
Second, we demonstrate how Bayesian nets can be used to provide environmental
context, and thus feature selection information, for the case-based reasoner.
1 Introduction

Cloud Classification Using Error-Correcting Output Codes
David W. Aha
Navy Center for Applied Research in Artificial Intelligence
Naval Research Laboratory
Washington, DC 20375
aha@aic.nrl.navy.mil
Richard L. Bankert
Marine Meteorology Division
Naval Research Laboratory
Monterey, CA 93943
bankert@nrlmry.navy.mil
October 30, 1996
Submission to AI Applications: Natural Resources, Agriculture, and Environmental Science.
Corresponding author: The first author is the corresponding author for this submission. David's
phone number is (202) 767-9006 and FAX number is (202) 767-3172.
Suggested running head: "Cloud Classification Using Error-Correcting Output Codes"
Available as NCARAI Technical Note AIC-96-024
+PAGE+

THE LOAD, CAPACITY AND AVAILABILITY OF QUORUM
SYSTEMS
MONI NAOR AND AVISHAI WOOL
Abstract.
A quorum system is a collection of sets (quorums) every two of which intersect. Quorum systems
have been used for many applications in the area of distributed systems, including mutual exclusion,
data replication and dissemination of information
Given a strategy to pick quorums, the load L(S) is the minimal access probability of the busiest
element, minimizing over the strategies. The capacity Cap(S) is the highest quorum accesses rate
that S can handle, so Cap(S) = 1=L(S).
The availability of a quorum system S is the probability that at least one quorum survives,
assuming that each element fails independently with probability p. A tradeoff between L(S) and the
availability of S is shown.
We present four novel constructions of quorum system, all featuring optimal or near optimal
load, and high availability. The best construction, based on paths in a grid, has a load of O(1=
p
and a failure probability of exp((
p
n)) when the elements fail with probability p &lt; 1
2 . Moreover,
even in the presence of faults, with exponentially high probability the load of this system is still
O(1=
n). The analysis of this scheme is based on Percolation Theory.
Key words. quorum systems, load, fault tolerance, distributed computing, percolation theory,
linear programming.
AMS subject classifications. 60K35, 62N05, 68M10, 68Q22, 68R05, 90A28, 90C05.
1. Introduction.

Practical Data Breakpoints: Design and Implementation
Robert Wahbe Steven Lucco Susan L. Graham
Computer Science Division,   571 Evans Hall
UC Berkeley,   Berkeley CA, 94720
Abstract
A data breakpoint associates debugging actions with
programmer-specified conditions on the memory state
of an executing program. Data breakpoints provide
a means for discovering program bugs that are tedious or impossible to isolate using control breakpoints
alone. In practice, programmers rarely use data break-points, because they are either unimplemented or prohibitively slow in available debugging software. In this
paper, we present the design and implementation of a
practical data breakpoint facility.
A data breakpoint facility must monitor all memory
updates performed by the program being debugged.
We implemented and evaluated two complementary
techniques for reducing the overhead of monitoring
memory updates. First, we checked write instructions
by inserting checking code directly into the program
being debugged. The checks use a segmented bitmap
data structure that minimizes address lookup complexity. Second, we developed data flow algorithms
that eliminate checks on some classes of write instructions but may increase the complexity of the remaining
checks.
We evaluated these techniques on the Sparc using
the spec benchmarks. Checking each write instruc
This research was sponsored in part by the Defense Advanced Research Projects Agency under grant MDA972-92-J-1028 and contract DABT63-92-C-0026. The content of the paper does not necessarily reflect the position or the policy of the
Government and no official endorsement should be inferred.
Email: frwahbe, lucco, grahamg@cs.berkeley.edu
To appear in Proceedings of the ACM SIGPLAN'93 Symposium on Programming Language Design and Implementation, Albuquerque, NM, June 23-25 1993.
tion using a segmented bitmap achieved an average
overhead of 42%. This overhead is independent of the
number of breakpoints in use. Data flow analysis eliminated an average of 79% of the dynamic write checks.
For scientific programs such the nas kernels, analysis
reduced write checks by a factor of ten or more. On the
Sparc these optimizations reduced the average overhead to 25%.
1 Introduction

URL: http://www.cam.sri.com/tr/crc033/paper.ps.Z   Eurospeech, Berlin, 1993
A SPEECH-BASED ROUTE ENQUIRY SYSTEM BUILT FROM
GENERAL-PURPOSE COMPONENTS 1
Ian Lewin , Martin Russell , David Carter , Sue Browning , Keith Ponting and Stephen Pulman
SRI International,   23 Millers Yard, Cambridge, CB2 1RQ, UK
Speech Research Unit, DRA Malvern,   St Andrews Road, Malvern, Worcs, WR14 3PS, UK
ABSTRACT
The adaptation of existing general-purpose speech recognition and language understanding systems can greatly
reduce the cost of developing applications. However, the
components must have appropriate characteristics for this
to be possible.
Work is in progress to adapt two task-independent
components, the AURIX speech recognizer and the CLARE
language processor to create a system allowing spoken
queries of the PC-based Autoroute route planning package.
Keywords: adaptability, general purpose, speech recognition, language understanding, AURIX, CLARE
1. INTRODUCTION

URL: http://www.cam.sri.com/tr/crc044/paper.ps.ZARPA   (HLT) Proceedings, Princeton, 1994
Combining Knowledge Sources
to Reorder N-Best Speech Hypothesis Lists
Manny Rayner 1 , David Carter 1 , Vassilios Digalakis 2 , Patti Price 2
(1) SRI International,   Suite 23, Millers Yard, Cambridge CB2 1RQ, UK
(2) SRI International,   333 Ravenswood Ave., Menlo Park, CA 94025, USA
November 29, 1994
Abstract
A simple and general method is described that can combine different
knowledge sources to reorder N-best lists of hypotheses produced by a
speech recognizer. The method is automatically trainable, acquiring information from both positive and negative examples. Experiments are
described in which it was tested on a 1000-utterance sample of unseen
ATIS data.
1 Introduction

AAAI-98, Madison, WI, to appear
Needles in a Haystack : Plan Recognition in Large Spatial
Domains Involving Multiple Agents
Mark Devaney and Ashwin Ram
College of Computing
Georgia Institute of Technology
Atlanta, GA 30332-0280
markd@cc.gatech.edu
ashwin@cc.gatech.edu
Abstract
While plan recognition research has been applied to a
wide variety of problems, it has largely made identical assumptions about the number of agents participating in the plan, the observability of the plan execution process, and the scale of the domain. We describe a method for plan recognition in a real-world
domain involving large numbers of agents performing
spatial maneuvers in concert under conditions of limited observability. These assumptions differ radically
from those traditionally made in plan recognition and
produce a problem which combines aspects of the fields
of plan recognition, pattern recognition, and object
tracking. We describe our initial solution which borrows and builds upon research from each of these areas,
employing a pattern-directed approach to recognize individual movements and generalizing these to produce
inferences of large-scale behavior.
Introduction

Multistrategy Learning of Adaptive Reactive Controllers
Juan Carlos Santamara and Ashwin Ram
College of Computing
Georgia Institute of Technology
Atlanta, Georgia 30332-0280
E-mail: fcarlos,ashwing@cc.gatech.edu
Phone: (404) 894-4995
Fax: (404) 894-9846
Technical Report GIT-CC-97-05
January 1997
Abstract
Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time. However, the configuration of a reactive controller depends
highly on the operating conditions of the robot and the environment; thus, a reactive controller
configured for one class of environments may not perform adequately in another. This paper
presents a formulation of learning adaptive reactive controllers. Adaptive reactive controllers
inherit all the advantages of traditional reactive controllers, but in addition they are able to adjust themselves to the current operating conditions of the robot and the environment in order to
improve task performance. Furthermore, learning adaptive reactive controllers can learn when
and how to adapt the reactive controller so as to achieve effective performance under different
conditions. The paper presents an algorithm for a learning adaptive reactive controller that
combines ideas from case-based reasoning and reinforcement learning to construct a mapping
between the operating conditions of a controller and the appropriate controller configuration;
this mapping is in turn used to adapt the controller configuration dynamically. As a case
study, the algorithm is implemented in a robotic navigation system that controls a Denning
MRV-III mobile robot. The system is extensively evaluated using statistical methods to verify
its learning performance and to understand the relevance of different design parameters on the
performance of the system.
Keywords: Reactive control, multistrategy learning, case-based reasoning, reinforcement learning,
robotic navigation.
+PAGE+

The State of the Art in Ontology Design:
A Survey and Comparative Review
Natalya Fridman Noy
Carole D. Hafner
College of Computer Science
Northeastern University
Boston, MA 02115
-natasha, hafner-@ccs.neu.edu
Abstract
In this paper we develop a framework for comparing
ontologies, and place a number of the more
prominent ontologies into it. We have selected 10
specific projects for this study, including general
ontologies, domain specific ones, and one knowledge
representation system. The comparison framework
includes general characteristics such as the purpose of
an ontology, its coverage (general or domain-specific), its size, and the formalism used. It also
includes the design process used in creating an
ontology and the methods used to evaluate it.
Characteristics that describe the content of an
ontology include taxonomic organization, types of
concepts covered, top-level divisions, internal
structure of concepts, representation of part-whole
relations, and the presence and nature of additional
axioms. Finally we consider what experiments or
applications have used the ontologies. Knowledge
sharing and reuse will require a common framework
to support interoperability of independently created
ontologies. Our study shows there is great diversity
in the way ontologies are designed and the way they
represent the world. By identifying the similarities
and differences among existing ontologies, we clarify
the range of alternatives in creating a standard
framework for ontology design.
1 Introduction

Adaptive Markov Chain Monte Carlo through
Regeneration
Walter R. Gilks
Medical Research Council
Biostatistics Unit
Cambridge, CB2 2SR, UK.
Gareth O. Roberts
Statistical Laboratory
University of Cambridge
Cambridge, CB2 1SB, UK.
Sujit K. Sahu
School of Mathematics
University of Wales, Cardiff
Cardiff, CF2 4YH, UK.
January 26, 1998
Summary
Markov chain Monte Carlo (MCMC) is used for evaluating expectations of functions of interest
under a target distribution . This is done by calculating averages over the sample path of a
Markov chain having as its stationary distribution. For computational efficiency, the Markov
chain should be rapidly mixing. This can sometimes be achieved only by careful design of the
transition kernel of the chain, on the basis of a detailed preliminary exploratory analysis of . An
alternative approach might be to allow the transition kernel to adapt whenever new features of
are encountered during the MCMC run. However, if such adaptation occurs infinitely often, the
stationary distribution of the chain may be disturbed. We describe a framework, based on the
concept of Markov chain regeneration, which allows adaptation to occur infinitely often, but which
does not disturb the stationary distribution of the chain or the consistency of sample-path averages.
Key Words: Adaptive method; Bayesian inference; Gibbs sampling; Markov chain Monte Carlo;
+PAGE+

RESONANCE AND THE PERCEPTION OF
MUSICAL METER
Edward W. Large
John F. Kolen
The Ohio State University
Abstract
Many connectionist approaches to musical expectancy and music composition let the
question of What next? overshadow the equally important question of When next?. One cannot
escape the latter question, one of temporal structure, when considering the perception of musical
meter. We view the perception of metrical structure as a dynamic process where the temporal
organization of external musical events synchronizes, or entrains, a listeners internal processing
mechanisms. This article introduces a novel connectionist unit, based upon a mathematical model
of entrainment, capable of phase and frequency-locking to periodic components of incoming
rhythmic patterns. Networks of these units can self-organize temporally structured responses to
rhythmic patterns. The resulting network behavior embodies the perception of metrical structure.
The article concludes with a discussion of the implications of our approach for theories of metrical
structure and musical expectancy.
Connection Science, 6 (1), 177 - 208.
+PAGE+

A Volumetric Approach to Virtual Simulation
of Functional Endoscopic Sinus Surgery
Gregory J. Wiet 1,2 , Roni Yagel 3 , Don Stredney 2 , Petra Schmalbrock 4 ,
Dennis J. Sessanna 2 , Yair Kurzion 3 , Louis Rosenberg 5 , Michael Levin 5 , Kenneth Martin 5
1 Department of Otolaryngology, The Ohio State University Hospitals,   Columbus, OH
2 The Ohio Supercomputer Center,   Columbus, OH
3 Department of Computer and Information Science, The Ohio State University,   Columbus, OH
4 Department of Radiology, The Ohio State University Hospitals,   Columbus, OH
5 Immersion Corporation,   San Jose, CA
Abstract
Advanced display technologies have made the virtual exploration of relatively complex models feasible
in many applications. Unfortunately, only a few human interfaces allow natural interaction with the
environment. Moreover, in surgical applications, such realistic interaction requires real-time rendering
of volumetric data - placing an overwhelming performance burden on the system. We report on a
collaboration of an interdisciplinary group developing a virtual reality system that provides intuitive
interaction with volume data by employing real-time volume rendering and force feedback (haptic)
sensations. We describe our rendering methods and the haptic devices and explain its utility of this
system in the real-world application of Endoscopic Sinus Surgery (ESS) simulation.
1. Introduction

Refining Interactions in a Distributed System
Neelam Soundarajan
Computer and Information Science
The Ohio State University
2015 Neil Avenue
Columbus, OH 43210
USA
e-mail: neelam@cis.ohio-state.edu
+PAGE+

Clustering Methods for Collaborative Filtering
Lyle H. Ungar and Dean P. Foster
CIS Dept. and Dept. of Statistics
University of Pennsylvania
Philadelphia, PA 19104
Abstract
Grouping people into clusters based on the items they have purchased allows accurate recommendations of new items for purchase:
if you and I have liked many of the same movies, then I will probably enjoy other movies that you like. Recommending items based
on similarity of interest (a.k.a. collaborative filtering) is attractive
for many domains: books, CDs, movies, etc., but does not always
work well. Because data are always sparse any given person has
seen only a small fraction of all movies much more accurate predictions can be made by grouping people into clusters with similar
movies and grouping movies into clusters which tend to be liked by
the same people. Finding optimal clusters is tricky because the movie
groups should be used to help determine the people groups and visa
versa. We present a formal statistical model of collaborative filtering,
and compare different algorithms for estimating the model parameters
including variations of K-means clustering and Gibbs Sampling. This
formal model is easily extended to handle clustering of objects with
multiple attributes.
Keywords: collaborative filtering, clustering, EM, Gibbs sampling
Email address of contact author:   ungar@cis.upenn.edu
Phone number of contact author:   215 898-7449
+PAGE+

AUCTION-DRIVEN COORDINATION FOR
PLANTWIDE OPTIMIZATION
Rinaldo A. Jose and Lyle H. Ungar
University of Pennsylvania
Philadelphia, PA 19104
Abstract
Model predictive control strategies generally focus on controlling plant outputs to setpoints; in
industry, however, a more desirable goal is maximizing a plants profitability. In principle, this can be
done by creating a plant model and maximizing profit with respect to the market prices of the plants
inputs and outputs, but in practice, such centralized approaches often cannot effectively be applied at
the operations time scale due to the size and complexity of the problem. One solution is to use
decentralized optimization at the unit operations level by tearing process streams and coordinating the
resulting pieces. Such optimization, however, requires that unit inputs and outputs be priced. We show
that a traditional Lagrangean-based approach to this pricing fails for simple systems. Instead, we define
slack resources over the torn process streams and price them using auctions. Unlike Lagrange
multipliers, slack resource prices contain useful information and can be used to make decisions
regarding capital improvements, thus providing a strong tie between the operations and management
layers in chemical plants.
Keywords
auctions, distributed optimization, resource prices, process decomposition, optimal coordination,
penalty-based methods, Lagrangean decomposition
Introduction

Integrality and Separability of Input Devices
Robert J.K. Jacob
Linda E. Sibert
Daniel C. McFarlane
M. Preston Mullen, Jr.
Human-Computer Interaction Lab
Naval Research Laboratory
Washington, D.C.
ABSTRACT
Current input device taxonomies and other frameworks typically emphasize
the mechanical structure of input devices. We suggest that selecting an
appropriate input device for an interactive task requires looking beyond the
physical structure of devices to the deeper perceptual structure of the task, the
device, and the interrelationship between the perceptual structure of the task and
the control properties of the device. We affirm that perception is key to
understanding performance of multidimensional input devices on
multidimensional tasks. We have therefore extended the theory of processing of
perceptual structure to graphical interactive tasks and to the control structure of
input devices. This allows us to predict task and device combinations that lead to
better performance and hypothesize that performance is improved when the
perceptual structure of the task matches the control structure of the device. We
conducted an experiment in which subjects performed two tasks with different
perceptual structures, using two input devices with correspondingly different
control structures, a three-dimensional tracker and a mouse. We analyzed both
speed and accuracy, as well as the trajectories generated by subjects as they used
the unconstrained three-dimensional tracker to perform each task. The results
support our hypothesis and confirm the importance of matching the perceptual
structure of the task and the control structure of the input device.
Keywords: Input devices, interaction techniques, gesture input, Polhemus
tracker, perceptual space, integrality, separability.
INTRODUCTION

To appear in Proceedings of Virtual Reality Vienna '93
Constructing Cyberspace:
Virtual Reality and Hypermedia
Keith Andrews
Institute for Information Processing and Computer Supported New Media (IICM)
Graz University of Technology,
A-8010 Graz, Austria.
Abstract
Large-scale, distributed hypermedia information systems allow fast, structured access to very large, dynamic information bases. The highly perceptual nature of a virtual reality interface has the power to take users both inside information and inside its
structure. Combining the two takes us a step towards cyberspace, William Gibson's
vision of a virtual model of all the world's interconnected data. This paper reviews
current work on the boundary of virtual reality and hypermedia.
1 Introduction

Proc. UIST '93 (ACM Symp. on User Interface Software and Technology), Atlanta GA, November 3-5, 1993, 145-155
Windows on the World:
2D Windows for 3D Augmented Reality
Steven Feiner
Blair MacIntyre
Marcus Haupt
Eliot Solomon
Department of Computer Science
Columbia University
New York, NY 10027
212-939-7000
-feiner, bm, haupt, esolomon-@cs.columbia.edu
ABSTRACT INTRODUCTION
We describe the design and implementation of a prototype When we think of the use of head-mounted displays and 3D
heads-up window system intended for use in a 3D environ- interaction devices to present virtual worlds, it is often in
ment. Our system includes a see-through head-mounted terms of environments populated solely by 3D objects.
display that runs a full X server whose image is overlaid on There are many situations, however, in which 2D text and
the user's view of the physical world. The user's head is graphics of the sort supported by current window systems
tracked so that the display indexes into a large X bitmap, can be useful components of these environments. This is
effectively placing the user inside a display space that is especially true in the case of the many applications that run
mapped onto part of a surrounding virtual sphere. By under an industry standard window system such as X [13].
tracking the user's body, and interpreting head motion rela- While we might imagine porting or enhancing a significant
tive to it, we create a portable information surround that X application to take advantage of the 3D capabilities of a
envelopes the user as they move about. virtual world, the effort and cost may not be worth the
return, especially if the application is inherently 2D.
We support three kinds of windows implemented on top of Therefore, we have been exploring how we can incorporate
the X server: windows fixed to the head-mounted display, an existing 2D window system within a 3D virtual world.
windows fixed to the information surround, and windows
fixed to locations and objects in the 3D world. Objects can We are building an experimental system that supports a full
also be tracked, allowing windows to move with them. To X11 server on a see-through head-mounted display. Our
demonstrate the utility of this model, we describe a small display overlays a selected portion of the X bitmap on the
hypermedia system that allows links to be made between user's view of the world, creating an X-based augmented
windows and windows to be attached to objects. Thus, our reality. Depending on the situation and application, the
hypermedia system can forge links between any combina- user may wish to treat a window as a stand-alone entity or
tion of physical objects and virtual windows. to take advantage of the potential relationships that can be
made between it and the visible physical world. To make
this possible, we have developed facilities that allow X KEYWORDS: augmented reality, virtual reality, virtual
windows to be situated in a variety of ways relative to the worlds, head-mounted displays, portable computers, mobile
user and the 3D world. computing, window systems, X11, hypertext/hypermedia.
In this paper we first present related work and provide an
overview of our system. Next, we describe the different
kinds of windows that we support, and show how these
windows can be used to advantage by a simple hypermedia
system. Finally, we explain the underlying system architec-
Permission to copy without fee all or part of this material is granted
ture and describe our current implementation. provided that the copies are not made or distributed for direct
commercial advantage, the ACM copyright notice and the title of
the publication and its date appear, and notice is given that copying
is by permission of the Association for Computing Machinery. To
copy otherwise, or to republish, requires a fee and/or specific
permission.
1993 ACM 0-89791-628-X/93/0011 ... $1.50
November 3-5, 1993 UIST '93 145
+PAGE+

Type classes in Haskell
Cordelia Hall, Kevin Hammond, Simon Peyton Jones
and Philip Wadler
Glasgow University
Abstract
This paper defines a set of type inference rules for resolving overloading introduced by type classes. Programs including type classes
are transformed into ones which may be typed by the Hindley-Milner inference rules. In contrast to other work on type classes, the
rules presented here relate directly to user programs. An innovative
aspect of this work is the use of second-order lambda calculus to
record type information in the program.
1. Introduction

Measuring the Difficulty of Specific Learning Problems
Chris Thornton
Cognitive and Computing Sciences
University of Sussex
Brighton BN1 9QN
Email: Chris.Thornton@cogs.susx.ac.uk
Tel: (44)273 606755 x 3239
October 21, 1994
Abstract
Existing complexity measures from contemporary learning theory cannot be conveniently applied to specific learning problems (e.g., training sets). Moreover, they are typically non-generic,
i.e., they necessitate making assumptions about the way in which the learner will operate. The lack
of a satisfactory, generic complexity measure for learning problems poses difficulties for researchers
in various areas; the present paper puts forward an idea which may help to alleviate these. It
shows that supervised learning problems fall into two, generic, complexity classes only one of which
is associated with computational tractability. By determining which class a particular problem
belongs to, we can thus effectively evaluate its degree of generic difficulty.
1 Introduction

Crooked Functions, Bent Functions, and Distance
Regular Graphs
T.D. Bending D. Fon-Der-Flaass
School of Mathematical Sciences,
Queen Mary and Westfield College,   London E1 4NS, U.K.
T.Bending@mdx.ac.uk d.g.flaass@writeme.com
Submitted:  March 25, 1998;  Accepted:  June 30, 1998.
1991 Mathematical Subject Classification: 05E30, 05B20
Abstract
Let V and W be n-dimensional vector spaces over GF (2). A mapping
Q : V ! W is called crooked if it satisfies the following three properties:
Q(0) = 0;
Q(x) + Q() + Q() + Q(x + + ) 6= 0 for any three distinct x; ; ;
Q(x) + Q() + Q() + Q(x + a) + Q( + a) + Q( + a) 6= 0 if a 6= 0 (x; ;
arbitrary).
We show that every crooked function gives rise to a distance regular graph
of diameter 3 having = 0 and = 2 which is a cover of the complete
graph. Our approach is a generalization of a recent construction found by
de Caen, Mathon, and Moorhouse. We study graph-theoretical properties of
the resulting graphs, including their automorphisms. Also we demonstrate a
connection between crooked functions and bent functions.
1 Crooked functions and bent functions

A Symbiosis of Animation and Music
ROBERT E. PRINGLE BRIAN J. ROSS
Brock University
Department of Computer Science
St. Catharines, Ontario, Canada L2S 3A1
frp94bg,brossg@sandcastle.cosc.brocku.ca
December 12, 1995
Abstract
The use of music as a means to automate the sculpting and movement of graphical objects is investigated. An interactive environment for producing musically-controlled
computer animations is presented. The graphical objects studied are based on Todd's
and Latham's work in evolutionary art. The environment permits the creation of kernel objects using an interactive toolset. In addition to a basic set of morphological
definitions, each object incorporates a script, which is an instance of programming
language code and data definitions. Scripts permit the run-time computation of object characteristics, and when done in a temporal setting, allow complex animation
control. The script language has a number of functions that can access MIDI information, as read into the system via a MIDI file. The practical consequence of this
is that animations are controllable with music data, in which music determines the
movement and morphology of animated objects. The tight integration of music and
animation in an interactive production environment such as this one has a number
of pragmatic consequences, ranging from the ability to automatically synchronize
complex activities to music, to the use of music as a creative source for graphical
sculptoring and animation.
Keywords: animation, music, MIDI, evolutionary art.
+PAGE+

On the Maximum Tolerable Noise for
Reliable Computation by Formulas
William Evans*
will@cs.arizona.edu
Department of Computer Science
The University of Arizona
Tucson, AZ 85721-0077, USA
Nicholas Pippenger**
nicholas@cs.ubc.ca
Department of Computer Science
The University of British Columbia
Vancouver, BC V6T 1Z4, Canada
Abstract: It is shown that if a formula is constructed from noisy 2-input NAND gates,
with each gate failing independently with probability ", then reliable computation can or
cannot take place according as " is less than or greater than " 0 = (3
p
* This research was supported by an NSERC Canada International Fellowship.
** This research was supported by an NSERC Research Grant.
+PAGE+

Implementing Atomic Sequences on Uniprocessors
Using Rollforward
David Mosberger, Peter Druschel , and Larry L. Peterson
Department of Computer Science
University of Arizona
Tucson, AZ 85721
fdavidm,druschel,llpg@cs.arizona.edu
Summary
This article presents a software-only solution to the synchronization problem for uniprocessors.
The idea is to execute atomic sequences without any hardware protection, and in the rare case
of pre-emption, to roll the sequence forward to the end, thereby preserving atomicity. One of
the proposed implementations protects atomic sequences without any memory-accesses. This
is significant as it enables execution at CPU-speeds, rather than memory-speeds. The benefit of
this method increases with the frequency at which atomic sequences are executed. It therefore
encourages the building of systems with fine-grained synchronization. This has the additional
advantage of reducing average latency. Experiments demonstrate that this technique has the
potential to outperform even the best hardware mechanisms. The main contribution of this article
is to discuss operating-system related issues of rollforward and to demonstrate its practicality,
both in terms of flexibility and performance.
1 Introduction

Virtual Radios
Vanu Bose, Mike Ismert, Matt Welborn, John Guttag
Software Devices and Systems Group
Laboratory for Computer Science
Massachusetts Institute of Technology
Abstract
Conventional software radios take advantage of vastly improved A/D converters and DSP hardware. Our
approach, which we refer to as virtual radios, also depends upon high performance A/D converters. However,
rather than use DSPs, we have chosen to ride the curve of rapidly improving workstation hardware. We use
wideband digitization and then perform all of the digital signal processing in user space on a general purpose
workstation. This approach allows us to experiment with new approaches to signal processing that exploit the
hardware and software resources of the workstation. Furthermore, it allows us to experiment with different
ways of structuring systems in which the radio component of communication devices are integrated with
higher-level applications.
This paper describes the design and performance of an environment we have constructed that facilitates building virtual radios and of two applications built using that environment. The environment consists of an I/O
subsystem that provides high bandwidth low latency user-level access to digitized signals and a programming
environment that provides an infrastructure for building applications. The applications, which exemplify
some of the benefits of virtual radios, are a software cellular receiver and a novel wireless network interface.
1 Introduction

Optimal Representations of Polymorphic Types with
Subtyping
Alexander Aiken Edward L. Wimmers Jens Palsberg
Report No. UCB/CSD-96-909
July 1996
Computer Science Division (EECS)
University of California
Berkeley, California 94720
+PAGE+

Draft 16   Nov 94   - 1 - To appear IEEE MICRO Feb 1995
Myrinet - A Gigabit-per-Second Local-Area Network
(Based on a keynote talk presented by Charles L. Seitz)
Nanette J. Boden, Danny Cohen, Robert E. Felderman,
Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su
Myricom, Inc.
325 N. Santa Anita Ave.
Arcadia, CA 91006
(http://www.myri.com)
Abstract. Myrinet is a new type of local-area network (LAN) based on the
technology used for packet communication and switching within "massively-parallel processors" (MPPs). Think of Myrinet as an MPP message-passing
network that can span campus dimensions, rather than as a wide-area
telecommunications network that is operating in close quarters. The
technical steps toward making Myrinet a reality included the development
of (1) robust, 25m communication channels with flow control, packet
framing, and error control; (2) self-initializing, low-latency, cut-through
switches; (3) host interfaces that can map the network, select routes, and
translate from network addresses to routes, as well as handle packet traffic;
and (4) streamlined host software that allows direct communication
between user processes and the network.
+PAGE+

Network Subsystem Design:
A Case for an Integrated Data Path
Peter Druschel
Mark B. Abbott
Michael A. Pagels
Larry L. Peterson
Department of Computer Science
University of Arizona
Tucson, AZ 85721
Abstract
This paper argues that the CPU/memory data path is a potential throughput bottleneck in
workstations connected to high-speed networks, and considers the implications for the design
of the I/O subsystem.
1 Introduction

Virtual Memory Architecture in SunOS
Robert A. Gingell
Joseph P. Moran
William A. Shannon
Sun Microsystems, Inc.
2550 Garcia Ave.
Mountain View, CA 94043
ABSTRACT
A new virtual memory architecture for the Sun implementation of the UNIX
operating system is described. Our goals included unifying and simplifying the concepts
the system used to manage memory, as well as providing an implementation that fit well
with the rest of the system. We discuss an architecture suitable for environments that
(potentially) consist of systems of heterogeneous hardware and software architectures.
The result is a page-based system in which the fundamental notion is that of mapping
process addresses to files.
1. Introduction and Motivation

A Study of the Structure and Performance
of MMU Handling Software
Yousef A. Khalidi
Vikram P. Joshi
Dock Williams
SMLI TR-94-28   June 1994
Abstract:
Modern operating systems provide a rich set of interfaces for mapping, sharing, and protecting memory. Different
memory management unit (MMU) architectures provide different mechanisms for managing memory translations.
Since the same OS usually runs on different MMU architectures, a software hardware address translation (hat)
layer that abstracts the MMU architecture is normally implemented between MMU hardware and the virtual memory system of the OS. In this paper, we study the impact of the OS and the MMU on the structure and performance
of the hat layer. In particular, we concentrate on the role of the hat layer on the scalability of system performance
on symmetric multiprocessors with 2-12 CPUs. The results show that, unlike single-user applications, multi-user
applications require very careful multi-threading of the hat layer to achieve system performance that scales with
the number of CPUs. In addition, multi-threading the hat can result in better performance in lesser amounts of
physical memory.
email addresses:
yousef.khalidi@eng.sun.com
vikram.joshi@eng.sun.com
dock.williams@eng.sun.com
M/S 29-01
2550 Garcia Avenue
Mountain View, CA 94043
+PAGE+

A Survey of Collective Communication in
Wormhole-Routed Massively Parallel Computers
Philip K. McKinley, Yih-jia Tsai, and David F. Robinson
Technical Report
MSU-CPS-94-35
June 1994
Submitted for publication, June 1994.
+PAGE+

Empirical Evaluation of Global Memory Support
on the CRAY-T3D and CRAY-T3E
Arvind Krishnamurthy, David E. Culler, and Katherine Yelick
Computer Science Division
University of California, Berkeley
1 Introduction

Feature Correspondence by Interleaving Shape and Texture
Computations
David Beymer
Artificial Intelligence Laboratory, and
Center for Biological and Computational Learning
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
email: beymer@ai.mit.edu
Abstract
The correspondence problem in computer vision is basically a matching task between two or more sets of features. In this paper, we introduce a vectorized image
representation, which is a feature-based representation
where correspondence has been established with respect
to a reference image. The representation consists of two
image measurements made at the feature points: shape
and texture. Feature geometry, or shape, is represented
using the (x; ) locations of features relative to the some
standard reference shape. Image grey levels, or texture,
are represented by mapping image grey levels onto the
standard reference shape. Computing this representation
is essentially a correspondence task, and in this paper
we explore an automatic technique for "vectorizing" face
images. Our face vectorizer alternates back and forth
between computation steps for shape and texture, and a
key idea is to structure the two computations so that each
one uses the output of the other. In addition to describing the vectorizer, an application to the problem of facial
feature detection will be presented.
1 Introduction

RUNTIME SUPPORT FOR PORTABLE DISTRIBUTED DATA STRUCTURES   Chih-Po Wen, Soumen Chakrabarti, Etienne Deprit, Arvind Krishnamurthy, Katherine Yelick   Computer Science Division, Department of EECS University of California,   Berkeley, California 94720 USA
ABSTRACT
Multipol is a library of distributed data structures designed for irregular applications, including those with asynchronous communication patterns. In this paper,
we describe the Multipol runtime layer, which provides an efficient and portable abstraction underlying the data structures. It contains a thread system to express
computations with varying degrees of parallelism and to support multiple threads
per processor for hiding communication latency. To simplify programming in a mul-tithreaded environment, Multipol threads are small, finite-length computations that
are executed atomically. Rather than enforcing a single scheduling policy on threads,
users may write their own schedulers or choose one of the schedulers provided by
Multipol. The system is designed for distributed memory architectures and performs
communication optimizations such as message aggregation to improve efficiency on
machines with high communication startup overhead. The runtime system currently
runs on the Thinking Machines CM5, Intel Paragon, and IBM SP1, and is being
ported to a network of workstations. Multipol applications include an event-driven
timing simulator [1], an eigenvalue solver [2], and a program that solves the phylogeny
problem [3].
1 INTRODUCTION

Joseph D. Darcy
CS 270 Project Report,   spring 1998
Narrowing Interval Bounds
1. Abstract
Interval arithmetic is an automated attempt to give guaranteed upper and lower bounds of a numerical
computation in the face of uncertainly in the input data and floating point roundoff during the calculation.
While a simple interval equivalent of a rational function can be readily synthesized, the bounds from this
construction may be too pessimistically large to be useful. This paper surveys a variety of techniques for
refining the interval bounds. An appendix identifies issues with realizing floating point based interval
arithmetic on current IEEE 754 compliant processors.
2. Introduction

Modeling and Rendering Architecture from Photographs:
A hybrid geometry- and image-based approach
Technical Report UCB//CSD-96-893
January 19, 1996
Paul E. Debevec Camillo J. Taylor Jitendra Malik
debevec@cs.berkeley.edu camillo@cs.berkeley.edu malik@cs.berkeley.edu
545 Soda Hall 485 Soda Hall 725 Soda Hall
(510) 642 9940 (510) 642 5029 (510) 642 7597
Computer Science Division, University of California at Berkeley
Berkeley, CA 94720-1776
(510) 642 5775 (fax)
Abstract
We present an approach for creating realistic synthetic views of existing architectural
scenes from a sparse set of still photographs. Our approach, which combines both geometry-based and image-based modeling and rendering techniques, has two components. The first
component is an easy-to-use photogrammetric modeling system which facilitates the recovery of a basic geometric model of the photographed scene. The modeling system is effective
and robust because it exploits the constraints that are characteristic of architectural scenes.
The second component is a model-based stereo algorithm, which recovers how the real scene
deviates from the basic model. By making use of the model, our stereo approach can robustly
recover accurate depth from image pairs with large baselines. Consequently, our approach
can model large architectural environments with far fewer photographs than current image-based modeling approaches. As an intermediate result, we present view-dependent texture
mapping, a method of better simulating geometric detail on basic models. Our approach
can recover models for use in either geometry-based or image-based rendering systems. We
present results that demonstrate our approach's abilty to create realistic renderings of architectural scenes from viewpoints far from the original photographs.
Keywords: Image-based modeling, image-based rendering, interactive modeling systems,
photogrammetry, reconstruction, view-dependent texture mapping, view interpolation, model-based stereo
See also:   http://www.cs.berkeley.edu/~debevec/Research/
+PAGE+

A New O(n
2 ) Algorithm for the Symmetric Tridiagonal
Eigenvalue/Eigenvector Problem
by
Inderjit Singh Dhillon
B.Tech. (Indian Institute of Technology, Bombay) 1989
A dissertation submitted in partial satisfaction of the
requirements for the degree of
Doctor of Philosophy
in
Computer Science
in the
GRADUATE DIVISION
of the
UNIVERSITY of CALIFORNIA, BERKELEY
Committee in charge:
Professor James W. Demmel, Chair
Professor Beresford N. Parlett
Professor Phil Colella
1997
+PAGE+

Query Execution Techniques for Caching Expensive Methods
Joseph M. Hellerstein Jeffrey F. Naughton
University of Wisconsin, Department of Computer Sciences
1210 W. Dayton St., Madison, WI 53706
jmh@cs.berkeley.edu, naughton@cs.wisc.edu
Abstract. Object-Relational and Object-Oriented DBMSs allow
users to invoke time-consuming (expensive) methods in their
queries. When queries containing these expensive methods are run
on data with duplicate values, time is wasted redundantly computing methods on the same value. This problem has been studied in
the context of programming languages, where memoization is the
standard solution. In the database literature, sorting has been proposed to deal with this problem. We compare these approachesalong
with a third solution, a variant of unary hybrid hashing which we call
Hybrid Cache. We demonstrate that Hybrid Cache always dominates memoization, and significantly outperforms sorting in many
instances. This provides new insights into the tradeoff between hashing and sorting for unary operations. Additionally, our Hybrid Cache
algorithm includes some new optimizations for unary hybrid hashing, which can be used for other applications such as grouping and
duplicate elimination. We conclude with a discussion of techniques
for caching multiple expensive methods in a single query, and raise
some new optimization problems in choosing caching techniques.
1 Introduction

Using Skeletons for Nonholonomic Path Planning among Obstacles
Brian Mirtich
John Canny
Computer Science Division
University of California
Berkeley, CA 94720
Abstract
This paper describes a practical path planner for
nonholonomic robots in environments with obstacles.
The planner is based on building a one-dimensional,
maximal clearance skeleton through the configuration
space of the robot. However rather than using the Eu-clidean metric to determine clearance, a special metric
which captures information about the nonholonomy of
the robot is used. The robot navigates from start to
goal states by loosely following the skeleton; the resulting paths taken by the robot are of low "complexity."
We describe how much of the computation can be done
off-line once and for all for a given robot, making for
an efficient planner. The focus is on path planning
for mobile robots, particularly the planar two-axle car,
but the underlying ideas are quite general and may be
applied to planners for other nonholonomic robots.
1 Introduction

Dynamic Procedure Placement Through Cache Windowing
Carleton Miyamoto
CS 252/265   Spring 98
University of California, Berkeley
Abstract
The relative slowdown of DRAMs with respect to
processor speeds and the widespread use of SMP
machines have bolstered the reliance on processor caches
to provide good performance. As a result, optimizing
machines and software for caches have recently received
more attention. In addition, with the popularity of
extensible computing, which includes the object oriented
programming style, shared libraries, and Java based
computing, creating effective compilers has become
more challenging, with an increased reliance on more
dynamic techniques, such as profiling and runtime code
generation. This paper proposes a dynamic optimization
method called cache windowing to reduce conflict misses
in L1 instruction caches. Using a combination of
hardware and software support, cache windowing
integrates a RollCache (a direct-mapped cache enhanced
to support dynamic cache configuration) and a software
implemented FIFO caching policy. Together, both allow
a program to reposition procedures, dynamically and
efficiently, to eliminate cache conflicts. Experiments
show that this type of caching scheme can achieve miss
rates competitive to a 2-way set associative cache for
various programs. Currently, a high software overhead
exists to support a software caching policy, though
different compiler optimizations, such as inlining, may
help to reduce this. Such a system provides a more robust
runtime architecture that, potentially, may adapt better to
a wider variety of environments.
1 Introduction

Belief Revision: A Critique
Nir Friedman
Computer Science Department
Stanford University
Gates Building 1A
Stanford, CA 94305-9010
nir@cs.stanford.edu
Joseph Y. Halpern
IBM Research Division
Almaden Research Center,   Dept. K53-B2
650 Harry Road
San Jose, CA 95120-6099
halpern@almaden.ibm.com
May 6, 1996
Abstract
The problem of belief changehow an agent should revise her beliefs upon learning new
informationhas been an active area of research in both philosophy and artificial intelligence.
Many approaches to belief change have been proposed in the literature. Our goal is not to
introduce yet another approach, but to examine carefully the rationale underlying the approaches
already taken in the literature, and to highlight what we view as methodological problems in the
literature. The main message is that to study belief change carefully, we must be quite explicit
about the ontology or scenario underlying the belief change process. This is something that
has been missing in previous work, with its focus on postulates. Our analysis shows that we
must pay particular attention to two issues which have often been taken for granted: The first
is how we model the agent's epistemic state. (Do we use a set of beliefs, or a richer structure,
such as an ordering on worlds? And if we use a set of beliefs, in what language are these
beliefs are expressed?) The second is the status of observations. (Are observations known to
be true, or just believed? In the latter case, how firm is the belief?) For example, we argue that
even postulates that have been called beyond controversy are unreasonable when the agent's
beliefs include beliefs about her own epistemic state as well as the external world. Issues of the
status of observations arise particularly when we consider iterated belief revision, and we must
confront the possibility of revising by ' and then by :'.
Keyword: Belief revision
+PAGE+

First-Order Conditional Logic Revisited
Nir Friedman
Dept. of Computer Science
Stanford University
Gates Building 1A
Stanford, CA 94305-9010
nir@cs.stanford.edu
Joseph Y. Halpern
IBM Almaden Research Center
650 Harry Road
San Jose, CA 95120-6099
halpern@almaden.ibm.com
Daphne Koller
Dept. of Computer Science
Stanford University
Gates Building 1A
Stanford, CA 94305-9010
koller@cs.stanford.edu
Abstract
Conditional logics play an important role in recent attempts
to investigate default reasoning. This paper investigates first-order conditional logic. We show that, as for first-order
probabilistic logic, it is important not to confound statistical conditionals over the domain (such as most birds fly),
and subjective conditionals over possible worlds (such as I
believe that Tweety is unlikely to fly). We then address
the issue of ascribing semantics to first-order conditional
logic. As in the propositional case, there are many possible semantics. To study the problem in a coherent way, we
use plausibility structures. These provide us with a general
framework in which many of the standard approaches can be
embedded. We show that while these standard approaches
are all the same at the propositional level, they are significantly different in the context of a first-order language. We
show that plausibilities provide the most natural extension of
conditional logic to the first-order case: We provide a sound
and complete axiomatization that contains only the KLM
properties and standard axioms of first-order modal logic.
We show that most of the other approaches have additional
properties, which result in an inappropriate treatment of an
infinitary version of the lottery paradox.
1 Introduction

A Fast Algorithm for Incremental Distance Calculation
Ming C. Lin and John F. Canny
University of California, Berkeley
Berkeley, CA 94720
Abstract
A simple and efficient algorithm for finding the closest points between two convex polyhedra is described
here. Data from numerous experiments tested on a
broad set of convex polyhedra on &lt; 3 show that the
running time is roughly constant for finding closest
points when nearest points are approximately known
and is linear in total number of vertices if no special
initialization is done. This algorithm can be used for
collision detection, computation of the distance between two polyhedra in three-dimensional space, and
other robotics problems. It forms the heart of the
motion planning algorithm of [1].
1 Introduction

Eraser: A Dynamic Data Race Detector for Multi-Threaded Programs
Stefan Savage
Department of Computer Science and Engineering
University of Washington,  Seattle
Michael Burrows Greg Nelson Patrick Sobalvarro
Digital Equipment Corporation
Systems Research Center
Thomas Anderson
Computer Science Division
University of California, Berkeley
Abstract
Multi-threaded programming is difficult and error prone. It
is easy to make a mistake in synchronization that produces a
data race, yet it can be extremely hard to locate this mistake
during debugging. This paper describes a new tool, called
Eraser, for dynamically detecting data races in lock-based
multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify
that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a
multi-threaded Web search engine, that demonstrate the effectiveness of this approach.
1 Introduction

High-Performance Sorting on Networks of Workstations
Andrea C. Arpaci-Dusseau
Computer Science Division
University of California, Berkeley
dusseau@cs.berkeley.edu
Remzi H. Arpaci-Dusseau
Computer Science Division
University of California, Berkeley
remzi@cs.berkeley.edu
David E. Culler
Computer Science Division
University of California, Berkeley
culler@cs.berkeley.edu
Joseph M. Hellerstein
Computer Science Division
University of California, Berkeley
jmh@cs.berkeley.edu
David A. Patterson
Computer Science Division
University of California, Berkeley
patterson@cs.berkeley.edu
Abstract
We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW).
We find that parallel sorting on a NOW is competitive to sorting on the large-scale SMPs that have traditionally held the
performance records. On a 64-node cluster, we sort 6.0 GB
in just under one minute, while a 32-node cluster finishes the
Datamation benchmark in 2.41 seconds.
Our implementations can be applied to a variety of disk,
memory, and processor configurations; we highlight salient
issues for tuning each component of the system. We evaluate the use of commodity operating systems and hardware for
parallel sorting. We find existing OS primitives for memory
management and file access adequate. Due to aggregate communication and disk bandwidth requirements, the bottleneck
of our system is the workstation I/O bus.
1 Introduction

An FFT algorithm on Vector Intelligent RAM
Kirby Zhang
Nathan Slingerland
CS252: Advanced Computer Architecture
Department of Electrical Engineering and Computer Science
University of California, Berkeley
May 18, 1998
Abstract
We present an implementation of a Fast Fourier Transform on Vector Intelligent RAM architecture. The algorithm computes the Discrete
Fourier Transform in O(N log N) time, is self-sorting (no bit-reversed copy
phase is required), and has a minimum vector length of
p
N=2, where N is
the number of data points in the transform. An N-element scratch space
is required. The performance of this algorithm on VIRAM is analyzed
against two variants of the Stockham [12] algorithm.
1 Introduction

The Interaction of Parallel and Sequential Workloads on a
Network of Workstations
Remzi H. Arpaci, Andrea C. Dusseau, Amin M. Vahdat,
Lok T. Liu, Thomas E. Anderson, and David A. Patterson
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
Abstract
This paper examines the plausibility of using a network of
workstations (NOW) for a mixture of parallel and sequential
jobs. Through simulations, our study examines issues that arise
when combining these two workloads on a single platform. Starting from a dedicated NOW just for parallel programs, we incrementally relax uniprogramming restrictions until we have a
multi-programmed, multi-user NOW for both interactive sequential users and parallel programs. We show that a number of issues
associated with the distributed NOW environment (e.g., daemon
activity, coscheduling skew) can have a small but noticeable effect on parallel program performance. We also find that efficient
migration to idle workstations is necessary to maintain acceptable parallel application performance. Furthermore, we present a
methodology for deriving an optimal delay time for recruiting idle
machines for use by parallel programs; this recruitment threshold
was just 3 minutes for the research cluster we measured. Finally,
we quantify the effects of the additional parallel load upon interactive users by keeping track of the potential number of user
delays in our simulations. When we limit the maximum number
of delays per user, we can still maintain acceptable parallel program performance. In summary, we find that for our workloads a
2:1 rule applies: a NOW cluster of approximately 60 machines can
sustain a 32-node parallel workload in addition to the sequential
load placed upon it by interactive users.
1 Introduction

Software-only video production switcher for the
Internet MBone
Tina H. Wong
Computer Science Division
University of California, Berkeley
twong@cs.berkeley.edu
ABSTRACT
In this paper, we describe the design and implementation of a prototype
software video production switcher, vps, that improves the quality of the content of MBone broadcasts. vps is modeled after the broadcast television industry's studio production switcher. It provides special effects processing to
incorporate audience discussions, add titles and other information, and integrate stored videos into the presentation. vps is structured to work with other
MBone conferencing tools. The ultimate goal is to automate the production of
MBone broadcasts.
1 INTRODUCTION

"Go With the Winners" Algorithms
David Aldous
Department of Statistics
University of California
Berkeley CA 94720
Umesh Vazirani
Department of Computer Science
University of California
Berkeley CA 94720
1 Introduction

Qualia Structure and the
Compositional Interpretation of Compounds
Michael Johnston x and Federica Busa
Research Lab for Linguistics and Computation,
Computer Science Department,
Volen Center for Complex Systems,
Brandeis University,
Waltham, MA 02254
johnston@cs.brandeis.edu federica@cs.brandeis.edu
Abstract
The analysis of nominal compound constructions has proven to be a recalcitrant problem
for linguistic semantics and poses serious challenges for natural language processing systems.
We argue for a compositional treatment of compound constructions which limits the need for
listing of compounds in the lexicon. We argue that the development of a practical model of
compound interpretation crucially depends on issues of lexicon design. The Generative Lexicon
(Pustejovsky 1995) provides us with a model of the lexicon which couples sufficiently expressive
lexical semantic representations with mechanisms which capture the relationship between those
representations and their syntactic expression. In our approach, the qualia structures of the
nouns in a compound provide relational structure enabling compositional interpretation of the
modification of the head noun by the modifying noun. This brings compound interpretation
under the same rubric as other forms of composition in natural language, including argument
selection, adjectival modification, and type coercion (Pustejovsky (1991,1995), Bouillon 1995).
We examine data from both English and Italian and develop analyses for both languages which use
phrase structure schemata to account for the connections between lexical semantic representation
and syntactic expression. In addition to applications in natural language understanding, machine
translation, and generation, the model of compound interpretation developed here can be applied
to multi-lingual information extraction tasks.
+PAGE+

Stochastic Interaction and Linear Logic
Patrick D. Lincoln John C. Mitchell Andre Scedrov x
Abstract
We present stochastic interactive semantics for propositional linear
logic without modalities. The framework is based on interactive
protocols considered in computational complexity theory, in which
a prover with unlimited power interacts with a verifier that can
only toss fair coins or perform simple tasks when presented with
the given formula or with subsequent messages from the prover.
The additive conjunction & is described as random choice, which
reflects the intuitive idea that the verifier can perform only "random spot checks". This stochastic interactive semantic framework
is shown to be sound and complete. Furthermore, the prover's
winning strategies are basically proofs of the given formula. In
this framework the multiplicative and additive connectives of linear logic are described by means of probabilistic operators, giving a
new basis for intuitive reasoning about linear logic and a potential
new tool in automated deduction.
A revised version appears in : "Advances in Linear Logic", ed. by J.-Y. Girard et
al., London Mathematical Society Lecture Notes Series, Volume 222, Cambridge University
Press, 1995, pp. 147-166.
lincoln@csl.sri.com   SRI International Computer Science Laboratory,   Menlo Park
CA 94025 USA.   Work supported under NSF Grant CCR-9224858.
jcm@cs.stanford.edu   http://theory.stanford.edu/people/jcm/home.html   Department of Computer Science, Stanford University,   Stanford, CA 94305.   Supported in part
by an NSF PYI Award, matching funds from Digital Equipment Corporation, the Pow-ell Foundation, and Xerox Corporation; and the Wallace F. and Lucille M. Davis Faculty
Scholarship.
andre@cis.upenn.edu   http://www.cis.upenn.edu/~andre   Department of Mathematics, University of Pennsylvania,   Philadelphia, PA 19104-6395.   Partially supported by
NSF Grants CCR-91-02753 and CCR-94-00907 and by ONR Grant N00014-92-J-1916. Sce-drov is an American Mathematical Society Centennial Research Fellow.
+PAGE+

An Experimental Comparison of
Three Graph Drawing Algorithms
(Extended Abstract)
Giuseppe Di Battista
dibattista@iasi.rm.cnr.it
Roberto Tamassia
rt@cs.brown.edu
D. I. F. A.
Univ. della Basilicata
85100 Potenza
Italy
Ashim Garg
ag@cs.brown.edu
Emanuele Tassinari x
tassinar@dis.uniroma1.it
Dept. of Computer Science
Brown University
Providence, RI 02912-1910
USA
Giuseppe Liotta
liotta@dis.uniroma1.it
Francesco Vargiu
vargiu@dis.uniroma1.it
Dip. Informatica e Sistemistica
Univ. di Roma "La Sapienza"
00198 Roma
Italy
Abstract
In this paper we present an extensive experimental study comparing three general-purpose graph
drawing algorithms. The three algorithms take as
input general graphs (with no restrictions whatsoever on the connectivity, planarity, etc.) and construct orthogonal grid drawings, which are widely
used in software and database visualization applications. The test data (available by anonymous
ftp) are 11,582 graphs, ranging from 10 to 100
vertices, which have been generated from a core
set of 112 graphs used in "real-life" software engineering and database applications. The experiments provide a detailed quantitative evaluation of
the performance of the three algorithms, and show
that they exhibit trade-offs between "aesthetic"
properties (e.g., crossings, bends, edge length) and
running time. The observed practical behavior of
the algorithms is consistent with their theoretical
properties.
Research supported in part by the US National Science Foundation, by the US Army Research Office, by the US Office of
Naval Research and the Advanced Research Projects Agency, by
the NATO Scientific Affairs Division, by the "Progetto Finalizzato
Sistemi Informatici e Calcolo Parallelo (Sottoprogetto 6, Infokit)"
and Grant 94.23.CT07 of the Italian National Research Council
(CNR), and by the ESPRIT II Basic Research Actions Program of
the European Community (project ALgorithms and Complexity).
1 Introduction

Form(ers) Over Function(s): The KOLA Reference Manual
Mitch Cherniack
June 18, 1996
0.1 Still To Do
* 6 more fold proofs, precondition proofs
* Adjust primitives for Int, Str and Char to be synchronized with the Theta operators for these types
* Add floats
* Add section describing preconditions
* Add section on semantic optimizations, nested query optimization
* Fix awk script to generate Latex version of Larch scripts without typeface glitches
1 Introduction

OBJECT-ORIENTED QUERIES: EQUIVALENCE AND OPTIMIZATION
Gail M. Shaw and Stanley B. Zdonik
Department of Computer Science
Brown University
Providence, R.I. 02912
We are interested in efficiently accessing data in an object-oriented database.
We have developed a query algebra which fully supports object identity and
abstract data types, and have identified a variety of algebraic query transformations. The equivalence of two queries is complicated by the presence of
object identity. In this paper we define a hierarchy of notions of equivalence
for queries, and present examples of equivalent query transformations for each
level of the hierarchy.
1. INTRODUCTION

To appear in Proc. 11th Intl. Conf. on Data Engg., 1995
The AQUA Approach to Querying Lists and
Trees in Object-Oriented Databases
Bharathi Subramanian Theodore W. Leung
Brown University Brown University
Scott L. Vandenberg Stanley B. Zdonik
Siena College Brown University
Abstract
Relational database systems and most object-oriented database systems provide support for queries.
Usually these queries represent retrievals over sets or
multisets. Many new applications for databases, such
as multimedia systems and digital libraries, need support for queries on complex bulk types such as lists
and trees. In this paper we describe an object-oriented
query algebra for lists and trees. The operators in the
algebra preserve the ordering between the elements of a
list or tree, even when the result list or tree contains an
arbitrary set of nodes from the original tree. We also
present predicate languages for lists and trees which
allow order-sensitive queries because they use pattern
matching to examine groups of list or tree nodes rather
than individual nodes. The ability to decompose predicate patterns enables optimizations that make use of
indices.
1 Introduction

Decomposition Techniques for Planning in Stochastic Domains
Thomas Dean Shieu-Hong Lin
Department of Computer Science
Box 1910,   Brown University
Providence, RI 02906,  USA
Email: ftld,shlg@cs.brown.edu
Abstract
This paper is concerned with modeling
planning problems involving uncertainty as
discrete-time, finite-state stochastic automata.
Solving planning problems is reduced to computing policies for Markov decision processes.
Classical methods for solving Markov decision
processes cannot cope with the size of the
state spaces for typical problems encountered
in practice. As an alternative, we investigate
methods that decompose global planning problems into a number of local problems, solve the
local problems separately, and then combine
the local solutions to generate a global solution. We present algorithms that decompose
planning problems into smaller problems given
an arbitrary partition of the state space. The
local problems are interpreted as Markov decision processes and solutions to the local problems are interpreted as policies restricted to the
subsets of the state space defined by the partition. One algorithm relies on constructing and
solving an abstract version of the original decision problem. A second algorithm iteratively
approximates parameters of the local problems
to converge to an optimal solution. We show
how properties of a specified partition affect the
time and storage required for these algorithms.
1 Introduction

Computational Intelligence, Volume 12, Number 3, 1996
LOCALIZED TEMPORAL REASONING USING SUBGOALS
AND ABSTRACT EVENTS
Shieu-Hong Lin, Thomas Dean 1
Department of Computer Science, Brown University,   Providence, RI 02912
We are concerned with temporal reasoning problems where there is uncertainty about the order
in which events occur. The task of temporal reasoning is to derive an event sequence consistent with
a given set of ordering constraints to achieve a goal. Previous research shows that the associated
decision problems are hard even for very restricted cases. In this paper, we investigate locality in
event ordering and causal dependencies. We present a localized temporal reasoning algorithm that
uses subgoals and abstract events to exploit locality. The computational efficiency of our algorithm
for a problem instance is quantified by the inherent locality in the instance. We theoretically
demonstrate the substantial improvement in performance gained by exploiting locality. This work
provides a solid evidence of the usefulness of localized reasoning in exploiting locality.
Key words: Dynamical Systems, Temporal Reasoning, Planning, Locality, Localized Reasoning, Subgoals, Abstract Events
1. INTRODUCTION

Hierarchical Solution of Markov Decision Processes using Macro-actions
Milos Hauskrecht, Nicolas Meuleau
Leslie Pack Kaelbling, Thomas Dean
Computer Science Department,   Box 1910
Brown University,   Providence, RI 02912
fmilos, nm, lpk, tldg@cs.brown.edu
Craig Boutilier
Department of Computer Science
University of British Columbia
Vancouver, BC V6T 1Z4, Canada
cebly@cs.ubc.ca
Abstract
We investigate the use of temporally abstract
actions, or macro-actions, in the solution of
Markov decision processes. Unlike current models that combine both primitive actions and
macro-actions and leave the state space unchanged, we propose a hierarchical model (using
an abstract MDP) that works with macro-actions
only, and that significantly reduces the size of the
state space. This is achieved by treating macro-actions as local policies that act in certain regions
of state space, and by restricting states in the abstract MDP to those at the boundaries of regions.
The abstract MDP approximates the original and
can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally,
we consider ways in which macro-actions can be
reused to solve multiple, related MDPs; and we
show that this can justify the computational over
head of macro-action generation.
1 Introduction

How good are genetic algorithms at finding
large cliques: an experimental study
Bob Carter
carter@cs.bu.edu
Kihong Park
park@cs.bu.edu
BU-CS-93-015
October 10, 1993
Boston University
Computer Science Department
Boston, MA 02215
Phone: 617 353-8919
Fax: 617 353-6457
Abstract
This paper investigates the power of genetic algorithms at solving the MAX-CLIQUE problem. We measure the performance of a standard genetic algorithm on an elementary set of
problem instances consisting of embedded cliques in random graphs. We indicate the need
for improvement, and introduce a new genetic algorithm, the multi-phase annealed GA, which
exhibits superior performance on the same problem set.
As we scale up the problem size and test on "hard" benchmark instances, we notice a
degraded performance in the algorithm caused by premature convergence to local minima. To
alleviate this problem, a sequence of modifications are implemented ranging from changes in
input representation to systematic local search. The most recent version, called union GA,
incorporates the features of union cross-over, greedy replacement, and diversity enhancement.
It shows a marked speed-up in the number of iterations required to find a given solution, as well
as some improvement in the clique size found.
We discuss issues related to the SIMD implementation of the genetic algorithms on a Thinking Machines CM-5, which was necessitated by the intrinsically high time complexity (O(n 3 ))
of the serial algorithm for computing one iteration.
Our preliminary conclusions are: (1) a genetic algorithm needs to be heavily customized to
work "well" for the clique problem; (2) a GA is computationally very expensive, and its use is
only recommended if it is known to find larger cliques than other algorithms; (3) although our
customization effort is bringing forth continued improvements, there is no clear evidence, at this
time, that a GA will have better success in circumventing local minima.
Part of this research was presented at the 2nd DIMACS Implementation Challenge on Combinatorial Optimiza
tion, DIMACS, October, 1993.
Supported in part by NSF grant CCR-9204284
+PAGE+

The Undecidability of
Mitchell's Subtyping Relationship
J. B. Wells
jbw@cs.bu.edu
Dept. of Computer Science
Boston University
Boston, MA 02215, U.S.A.
December 10, 1995
Abstract
Mitchell defined and axiomatized a subtyping relationship (also known
as containment, coercibility, or subsumption) over the types of System F
(with "!" and "8"). This subtyping relationship is quite simple and does
not involve bounded quantification. Tiuryn and Urzyczyn quite recently
proved this subtyping relationship to be undecidable. This paper supplies a new undecidability proof for this subtyping relationship. First, a
new syntax-directed axiomatization of the subtyping relationship is defined. Then, this axiomatization is used to prove a reduction from the
undecidable problem of semi-unification to subtyping. The undecidability of subtyping implies the undecidability of type checking for System F
extended with Mitchell's subtyping, also known as "F plus eta".
1 Introduction

In Proceedings of ICC'97: The IEEE International Conference onCommunications, Montreal, Canada, June 1997.
Implementation and Performance Evaluation of TCP Boston
A Fragmentation-tolerant TCP Protocol for ATM Networks
Azer Bestavros
best@cs.bu.edu
Gitae Kim
kgtjan@cs.bu.edu
Computer Science Department
Boston University
Boston, MA 02215
ABSTRACT:
In this paper, we overview the implementation of TCP Boston
a novel fragmentation-tolerant transport protocol, especially
suited for ATM's 53-byte cell-oriented switching architecture.
TCP Boston integrates a standard TCP/IP protocol, such as
Reno or Vegas, with a powerful redundancy control mechanism based on AIDAan adaptive version of Rabin's IDA dispersal and reconstruction algorithms. Our results show that
TCP Boston improves TCP/IP's performance over ATMs for
both network-centric metrics (e.g., effective throughput) and
application-centric metrics (e.g., response time).
1 Introduction

Transfer Equations in Global Illumination
James Arvo +L +
Program of Computer Graphics
Cornell University
Abstract
The purpose of these notes is to describe some of the physical and mathematical properties
of the equations occurring in global illumination. We first examine the physical assumptions
that make the particle model of light an appropriate paradigm for computer graphics and
then derive a balance equation for photons. In doing this we establish connections with the
field of radiative transfer and its more abstract counterpart, transport theory. The resulting
balance equation, known as the equation of transfer, accounts for large-scale interaction
of light with participating media as well as complex reflecting surfaces. Under various
simplifying assumptions the equation of transfer reduces to more conventional equations
encountered in global illumination.
1 Introduction

Generating CAD-models of teeth
Peter Johannes Neugebauer
Fraunhofer-Institute for Computer Graphics,
Wilhelminenstrasse 7, 64283 Darmstadt, Germany,
email: neugeb@igd.fhg.de
In this paper, we present an approach for the reconstruction of teeth model. Therefore,
a tooth model has to be scanned from several directions with a 3D-laser scanner. Several
views are necessary because of shadows and occluded areas in the range images. Then,
all acquired range views are combined to build a CAD-model of the tooth. The idea is
that every part of the surface should be visible in at least one view. The reconstruction
process is divided into the steps registration, volume sculpturing and generation of an
accurate polygonal representation.
1. Introduction

Sensing Polygon Poses by Inscription
Yan-Bin Jia Michael Erdmann
The Robotics Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213-3891
Abstract
Industrial assembly involves sensing the pose (orientation and position) of a part. Efficient and reliable
sensing strategies can be developed for an assembly
task if the shape of the part is known in advance. In
this paper we investigate the problem of determining
the pose of a convex n-gon from a set of m supporting
cones, i.e., cones with both sides supporting the polygon. An algorithm with running time O(nm) which
almost always reduces to O(n + m log n) is presented
to solve for all possible poses of the polygon. As a consequence, the polygon inscription problem of finding all
possible poses for a convex n-gon inscribed in another
convex m-gon, can be solved within the same asymptotic time bound. We prove that the number of possible poses cannot exceed 6n, given m 2 supporting
cones with distinct vertices. Experiments demonstrate
that two supporting cones are sufficient to determine
the real pose of the n-gon in most cases.
Our results imply that sensing in practice can be
carried out by obtaining viewing angles of a planar
part at multiple exterior sites in the plane. As a conclusion, we generalize this and other sensing methods
into a scheme named sensing by inscription.
1 Introduction

Journal of Artificial Intelligence Research 1 (1994) 159-208 Submitted 8/93; published 2/94
Bias-Driven Revision of Logical Domain Theories
Moshe Koppel   KOPPEL@BIMACS.CS.BIU.AC.IL
Ronen Feldman   FELDMAN@BIMACS.CS.BIU.AC.IL
Department of Mathematics and Computer Science, Bar-Ilan University,
Ramat-Gan, Israel
Alberto Maria Segre   SEGRE@CS.CORNELL.EDU
Department of Computer Science, Cornell University,
Ithaca, NY 14853, USA
Abstract
The theory revision problem is the problem of how best to go about revising a deficient
domain theory using information contained in examples that expose inaccuracies. In this paper we
present our approach to the theory revision problem for propositional domain theories. The
approach described here, called PTR, uses probabilities associated with domain theory elements to
numerically track the ``ow'' of proof through the theory. This allows us to measure the precise
role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given
example. This information is used to efficiently locate and repair awed elements of the theory.
PTR is proved to converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.
1. Introduction

THE CORAL USER MANUAL
A Tutorial Introduction to CORAL
Raghu Ramakrishnan Praveen Seshadri Divesh Srivastava
S. Sudarshan
Computer Sciences Department,
University of Wisconsin-Madison,   WI 53706, U.S.A.
The authors' e-mail addresses are fraghu,divesh,praveeng@cs.wisc.edu; sudarsha@research.att.com.
+PAGE+

A Middleware Service for Real-Time Push-Pull
Communications
Kanaka Juvva and Raj Rajkumar
Real-Time and Multimedia Laboratory
Carnegie Mellon University
Pittsburgh PA 15213
fkjuvva, raj+g@cs.cmu.edu
Abstract:
Current and emerging real-time and multimedia applications like multi-party collaboration, internet telephony and
distributed command control systems require the exchange of information over distributed and heterogeneous nodes.
Multiple data types including voice, video, sensor data, real-time intelligence data and text are being transported
widely across today's information, control and surveillance networks. All such applications can benefit enormously
from middleware, operating system and networking services that can support QoS guarantees, high availability,
dynamic reconfigurability and scalability.
In this paper, we propose a middleware layer called a "Real-Time Push-Pull Communications Service" to easily and quickly disseminate information across heterogeneous nodes with an underlying architecture to satisfy the
above-mentioned requirements. Push-Pull Communications is an extension of the real-time publisher/subscriber
model [4], and represents both "push" (data transfer initiated by a sender) and "pull" (data transfer initiated by a
receiver) communications. Nodes with widely differing processing power and networking bandwidth can coordinate
and co-exist by the provision of appropriate and automatic support for transformation on data and supports scaling.
Different information sources and sinks can operate at different frequencies and also can choose another (intermedi-ate) node to act as their proxy and and deliver data at the desired frequency. This service has been implemented
on RT-Mach, a resource-centric kernel using resource kernel primitives [7]. This paper presents an overview of the
design, implementation and preliminary performance evaluation of the model.
Keywords: Push communications, Pull communications, Proxy, Scaling, Middleware service, QoS
+PAGE+

Quality-of-Service Routing for
Traffic with Performance
Guarantees
Qingming Ma and Peter Steenkiste
Computer Science Department, Carnegie Mellon University
Pittsburgh, PA 15213, USA,   fqma, prsg@cs.cmu.edu
Abstract
Quality-of-Service (QoS) routing tries to select a path that satisfies a set of
QoS constraints, while also achieving overall network resource efficiency. We
present initial results on QoS path selection for traffic requiring bandwidth
and delay guarantees. For traffic with bandwidth guarantees, we found that
several routing algorithms that favor paths with fewer hops perform well. For
traffic with delay guarantees, we show that for a broad class of WFQ-like
scheduling algorithms, the problem of finding a path satisfying bandwidth,
delay, delay-jitter, and/or buffer space constraints while at the same time
deriving the bandwidth that has to be reserved to meet these constraints, is
solvable by a modified version of the Bellman-Ford shortest-path algorithm
in polynomial time.
Keywords
Routing, quality of service, integrated services networks
1 INTRODUCTION

Journal of Artificial Intelligence Research 3 (1995) 53-118 Submitted 3/95; published 7/95
Building and Refining Abstract Planning Cases
by Change of Representation Language
Ralph Bergmann   bergmann@informatik.uni-kl.de
Wolfgang Wilke   wilke@informatik.uni-kl.de
Centre for Learning Systems and Applications (LSA)
University of Kaiserslautern,   P.O.-Box 3049, D-67653 Kaiserslautern, Germany
Abstract
Abstraction is one of the most promising approaches to improve the performance of problem
solvers. In several domains abstraction by dropping sentences of a domain description as
used in most hierarchical planners has proven useful. In this paper we present examples
which illustrate significant drawbacks of abstraction by dropping sentences. To overcome
these drawbacks, we propose a more general view of abstraction involving the change of
representation language. We have developed a new abstraction methodology and a related
sound and complete learning algorithm that allows the complete change of representation
language of planning cases from concrete to abstract. However, to achieve a powerful
change of the representation language, the abstract language itself as well as rules which
describe admissible ways of abstracting states must be provided in the domain model.
This new abstraction approach is the core of Paris (Plan Abstraction and Refinement
in an Integrated System), a system in which abstract planning cases are automatically
learned from given concrete cases. An empirical study in the domain of process planning
in mechanical engineering shows significant advantages of the proposed reasoning from
abstract cases over classical hierarchical planning.
1. Introduction

Experience with Rover Navigation for Lunar-Like Terrains
Reid Simmons, Eric Krotkov, Lalitesh Katragadda, and Martial Hebert
Robotics Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
reids@cs.cmu.edu
Introduction

Final Version, 94-Mar-17,1of 9
Automation Tools for
NonDestructive Inspection of Aircraft:
Promise of Technology Transfer from the
Civilian to the Military Sector
Chris Seher 1 , Mel Siegel 2 , and William M. Kaufman 3
1 Federal Aviation Administration, Technical Center,   Atlantic City NJ 08201
2 Carnegie Mellon University, Robotics Institute,   Pittsburgh PA 15213
3 Carnegie Mellon University, CMRI,   Pittsburgh PA 15213
Abstract
The FAA Aging Aircraft Research Program is
supporting the development of a robotic mobile
nondestructive inspection (NDI) instrument
deployment tool at Carnegie Mellon University
(CMU) with the active participation of USAir. The
program has spawned several new relationships
and entities: an alliance with an ARPA-funded
research program at CMU having the capability to
add 3D-stereoscopic enhanced visual inspection
capability, a start-up company organized to
commercialize the combined technologies, and
State of Pennsylvania funding to foster this
commercialization. As a result of these activities
and connections the civilian sector appears to be
ahead of the military sector in important aspects of
automation for deployment of aircraft inspection
equipment. A partnership between the university
researchers, the airline operator, the start-up
company, and the state government is thus
emerging as the likely agent for transfer of the
civilian-developed technology to the military sector.
1. Introduction

Information Extraction from HTML:
Application of a General Machine Learning Approach
Dayne Freitag
Department of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
dayne@cs.cmu.edu
Abstract
Because the World Wide Web consists primarily of
text, information extraction is central to any effort that
would use the Web as a resource for knowledge discovery. We show how information extraction can be cast
as a standard machine learning problem, and argue for
the suitability of relational learning in solving it. The
implementation of a general-purpose relational learner
for information extraction, SRV, is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document
structure and the kinds of information available for use
in learning extraction patterns. Instead, structural and
other information is supplied as input in the form of an
extensible token-oriented feature set. We demonstrate
the effectiveness of this approach by adapting SRV for
use in learning extraction rules for a domain consisting
of university course and research project pages sampled
from the Web. Making SRV Web-ready only involves
adding several simple HTML-specific features to its basic feature set.
+PAGE+

The Candide System for Machine Translation
Adam L. Berger, Peter F. Brown , Stephen A. Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Robert L. Mercer, Harry Printz, Lubos Ures
IBM Thomas J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598
ABSTRACT
We present an overview of Candide, a system for automatic
translation of French text to English text. Candide uses
methods of information theory and statistics to develop a
probability model of the translation process. This model,
which is made to accord as closely as possible with a large
body of French and English sentence pairs, is then used to
generate English translations of previously unseen French
sentences. This paper provides a tutorial in these methods,
discussions of the training and operation of the system, and
a summary of test results.
1. Introduction

Efficient Resource Management
This paper will appear in the proceedings of the 1996 International Workshop on
Extensions of Logic Programming, Leipzig, Germany, March 28-30 1996.
for Linear Logic Proof Search
Iliano Cervesato 1 , Joshua S. Hodas 2 , and Frank Pfenning 1
1 Department of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213-3891, USA
e-mail: filianojfpg@cs.cmu.edu
2 Computer Science Department, Harvey Mudd College
Claremont, CA 91711, USA
e-mail: hodas@cs.hmc.edu
Abstract. The design of linear logic programming languages and theorem provers opens a number of new implementation challenges not
present in more traditional logic languages such as Horn clauses (Prolog)
and hereditary Harrop formulas (Prolog). Among these, the problem of
efficiently managing the linear context when solving a goal is of crucial
importance for the use of these systems in non-trivial applications. This
paper studies this problem in the case of Lolli [6] (though its results have
application to other systems). We first give a proof-theoretic presentation of the operational semantics of this language as a resolution calculus.
We then present a series of resource management systems designed to
eliminate the non-determinism in the distribution of linear formulas that
undermines the efficiency of a direct implementation of this system.
1 Introduction

A Simplified Account of Polymorphic References
Robert Harper
June, 1993
CMU-CS-93-169
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Abstract
A proof of the soundness of Tofte's imperative type discipline with respect to a structured operational
semantics is given. The presentation is based on a semantic formalism that combines the benefits of the
approaches considered by Wright and Felleisen, and by Tofte, leading to a particularly simple proof of
soundness of Tofte's type discipline.
This research was sponsored by the Defense Advanced Research Projects Agency, CSTO, under the title "The Fox
Project: Advanced Development of Systems Software", ARPA Order No. 8313, issued by ESD/AVS under Contract
No. F19628-91-C-0168.
The views and conclusions contained in this document are those of the author and should not be interpreted as
representing official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the
U.S. Government.
+PAGE+

Shumeet Baluja
baluja@cs.cmu.edu
Justsystem Pittsburgh Research Center &
Carnegie Mellon University
Scott Davies
scottd@cs.cmu.edu
School of Computer Science
Carnegie Mellon University
Fast Probabilistic Modeling for Combinatorial Optimization
Abstract
Probabilistic models have recently been utilized for the optimization of large combinatorial search problems. However,
complex probabilistic models that attempt to capture inter-parameter dependencies can have prohibitive computational
costs. The algorithm presented in this paper, termed
COMIT, provides a method for using probabilistic models in
conjunction with fast search techniques. We show how
COMIT can be used with two very different fast search algorithms: hillclimbing and Population-based incremental
learning (PBIL). The resulting algorithms maintain many of
the benefits of probabilistic modeling, with far less computational expense. Extensive empirical results are provided;
COMIT has been successfully applied to jobshop scheduling, traveling salesman, and knapsack problems. This paper
also presents a review of probabilistic modeling for combi
natorial optimization.
1 Background

New Approximation Techniques for Some Ordering Problems
Satish Rao Andrea W. Richa
Abstract
We describe logarithmic times optimal approximation
algorithms for the NP-hard graph optimization problems of minimum linear arrangement, minimum containing interval graph, and minimum storage-time
product. This improves on the best previous approximation bounds of Even, Naor, Rao, and Schieber for
these problems by an (log log n) factor.
Even, Naor, Rao, and Schieber defined "spreading
metrics" for each of the ordering problems above (and
to other problems); for each of these problems, they
provided a spreading metric of volume W , such that W
is a lower bound on the cost of a solution to the problem.
They used this spreading metric to find a solution of
cost O(W log n log log n) (for simplicity, assume that
all tasks have unit processing time in the minimum
storage-time product problem). In this paper, we show
how to find a solution within a logarithmic factor times
W for these problems.
We develop a recursion where at each level we
identify cost which, if incurred, yields subproblems
with reduced spreading metric volume. Specifically, we
present a divide-and-conquer strategy where the cost of
a solution to a problem at a recursive level is C plus the
cost of a solution to the subproblems at this level, and
where the spreading metric volume on the subproblems
is less than the original volume by (C= log n). This
ensures that the resulting solution has cost O(log n)
times the original spreading metric volume.
We note that this is an existentially tight bound on
the relationship between the spreading metric volume
W and the true optimal values for these problems.
For planar graphs, we combine a structural theorem
of Klein, Plotkin, and Rao with our new recursion technique to show that the spreading metric cost volumes
are within an O(log log n) factor of the cost of an optimal solution for the minimum linear arrangement, and
the minimum containing interval graph problems.
To appear in Proceedings of Ninth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), January 1998.
Research supported by NEC Research Institute, 4 Independence Way, Princeton, NJ 08540; satish@research.nj.nec.com.
Research supported in part by NSF NYI Award No. CCR-9457766, ARPA Contract F33615-93-1-1330, NEC Research Institute, and DIMACS. School of Computer Science, Carnegie Mellon
University, Pittsburgh, PA 15213, aricha@cs.cmu.edu.
1 Introduction.

Autonomous Robots, ??, 1-18 (??)
c ?? Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
Interleaving Planning and Robot Execution
for Asynchronous User Requests
KAREN ZITA HAIGH MANUELA M. VELOSO
khaigh@cs.cmu.edu mmv@cs.cmu.edu
http://www.cs.cmu.edu/~khaigh http://www.cs.cmu.edu/~mmv
Computer Science Department, Carnegie Mellon University,   Pittsburgh PA 15213-3891
Abstract.
Rogue is an architecture built on a real robot which provides algorithms for the integration of high-level planning, low-level robotic execution, and learning. Rogue addresses successfully several of the
challenges of a dynamic office gopher environment. This article presents the techniques for the integration
of planning and execution.
Rogue uses and extends a classical planning algorithm to create plans for multiple interacting goals
introduced by asynchronous user requests. Rogue translates the planner's actions to robot execution
actions and monitors real world execution. Rogue is currently implemented using the prodigy4.0
planner and the Xavier robot. This article describes how plans are created for multiple asynchronous goals,
and how task priority and compatibility information is used to achieve appropriate efficient execution. We
describe how Rogue communicates with the planner and the robot to interleave planning with execution
so that the planner can replan for failed actions, identify the actual outcome of an action with multiple
possible outcomes, and take opportunities from changes in the environment.
Rogue represents a successful integration of a classical artificial intelligence planner with a real mobile
robot.
1. Introduction

Implementing Distributed Server Groups for the
World Wide Web
Michael Garland, Sebastian Grassia, Robert Monroe, Siddhartha Puri
25 January 1995
CMU-CS-95-114
School of Computer Science
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213-3890
Abstract
The World Wide Web (WWW) has recently become a very popular facility for the dissemination of
information. As a result of this popularity, it is experiencing rapidly increasing traffic load. Single
machine servers cannot keep pace with the ever greater load being placed upon them. To alleviate this
problem, we have implemented a distributed Web server group. The server group can effectively balance request load amongst its members (within about 10% of optimal), and client response time is no
worse than in the single server case. Client response time was not improved because the measured client traffic consumed all available network throughput. The distributed operation of the server groups is
completely transparent to standard Web clients.
This research is sponsored in part by the Wright Laboratory, Aeronautical SystemsCenter, Air Force Materiel Command, USAF,
and the Advanced Research Projects Agency (ARPA) under grant F33615-93-1-1330. The US Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation thereon. Support also came from the
Air Force Materiel Command under contract number F19628-93-C-0171. Views and conclusions contained in this document are
those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of Wright Laboratory or the United States Government.
+PAGE+

Fast Soft Shadows
Michael Herf and Paul S. Heckbert
Abstract
Presented is a new algorithm to generate soft shadows. It
employs graphics hardware, including texture mapping and
accumulation buffering, to produce shadows resulting from
area light sources quickly.
Computer Science Dept., Carnegie Mellon University,    Pittsburgh PA 15213-3891, USA.   http://www.cs.cmu.edu/ph,   herf+@cmu.edu,
ph@cs.cmu.edu.
+PAGE+

Performance Measurements of the Multimedia
Testbed on Real-Time Mach
Roger B. Dannenberg, David B. Anderson,
Tom Neuendorffer, Dean Rubine
April 1994
CMU-CS-94-141
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213-3890
Abstract
Multimedia has generated widespread interest in real-time support within general purpose
operating systems. Multimedia also places new demands on operating systems for interprocess
communication. The Multimedia Testbed is a set of applications that stress consistent low-latency response and efficient interprocess communication for large blocks of data. The
Multimedia Testbed was ported to Real-Time Mach in the hopes of providing predictable low-latency response and, consequently, good synchronization and low jitter as required for
multimedia applications. Our work compares the performance of Real-Time Mach with that of
Mach 3.0. Although the fixed-priority scheduling of Real-Time Mach is a substantial
improvement, user threads are still preempted by device drivers, and the overall real-time
performance is not suitable for multimedia applications. We discuss areas where Real-Time
Mach needs improvement.
This research was performed by the Carnegie Mellon Information Technology Center and
supported by the IBM Corporation.
+PAGE+

Clustering Learning Tasks and the Selective Cross-Task
Transfer of Knowledge
Sebastian Thrun Joseph O'Sullivan
November 1995
CMU-CS-95-209
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
The first author is also affiliated with the Computer Science Department III of the University of
Bonn, Germany, where part of this research was carried out.
This research is sponsored in part by the National Science Foundation under award IRI-9313367,
and by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF,
and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. The
views and conclusions contained in this document are those of the author and should not be interpreted
as necessarily representing official policies or endorsements, either expressed or implied, of NSF,
Wright Laboratory or the United States Government.
+PAGE+

Moving target classification and tracking from real-time video
Alan J. Lipton Hironobu Fujiyoshi Raju S. Patil
The Robotics Institute. Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA, 15213
email: fajljhironobujrajug@cs.cmu.edu
URL: http://www.cs.cmu.edu/~ vsam
Abstract
This paper describes an end-to-end method for extracting moving targets from a real-time video stream,
classifying them into predefined categories according
to image-based properties, and then robustly tracking
them. Moving targets are detected using the pixelwise
difference between consecutive image frames. A classi-ficatoin metric is applied these targets with a temporal
consistency constraint to classify them into three categories: human, vehicle or background clutter. Once
classified, targets are tracked by a combination of temporal differencing and template matching.
The resulting system robustly identifies targets of
interest, rejects background clutter, and continually
tracks over large distances and periods of time despite
occlusions, appearance changes and cessation of target
motion.
1 Introduction

Exploiting Weak Connectivity for Mobile File Access
Lily B. Mummert, Maria R. Ebling, M. Satyanarayanan
School of Computer Science
Carnegie Mellon University
Abstract
Weak connectivity, in the form of intermittent, low-bandwidth, or expensive networks is a fact of life in mobile computing.
In this paper, we describe how the Coda File System has evolved to exploit such networks. The underlying theme of this
evolution has been the systematic introduction of adaptivity to eliminate hidden assumptions about strong connectivity.
Many aspects of the system, including communication, cache validation, update propagation and cache miss handling have
been modified. As a result, Coda is able to provide good performance even when network bandwidth varies over four orders
of magnitude from modem speeds to LAN speeds.
1. Introduction 2. Starting Point: Disconnected Operation

TIGHT ANALYSES OF TWO LOCAL LOAD BALANCING ALGORITHMS
BHASKAR GHOSH 1 F. T. LEIGHTON 2 BRUCE M. MAGGS 3;4
S. MUTHUKRISHNAN 5 C. GREG PLAXTON 6;7 R. RAJARAMAN 6;7
ANDR EA W. RICHA 3;4 ROBERT E. TARJAN 8 DAVID ZUCKERMAN 6;9
Abstract.
This paper presents an analysis of the following load balancing algorithm. At each step, each node in a network examines the number
of tokens at each of its neighbors and sends a token to each neighbor with at least 2d + 1 fewer tokens, where d is the maximum degree
of any node in the network. We show that within O(=ff) steps, the algorithm reduces the maximum difference in tokens between any
two nodes to at most O((d 2 log n)=ff), where is the global imbalance in tokens (i.e., the maximum difference between the number of
tokens at any node initially and the average number of tokens), n is the number of nodes in the network, and ff is the edge expansion of
the network. The time bound is tight in the sense that for any graph with edge expansion ff, and for any value , there exists an initial
distribution of tokens with imbalance for which the time to reduce the imbalance to even =2 is at least (=ff). The bound on the
final imbalance is tight in the sense that there exists a class of networks that can be locally balanced everywhere (i.e., the maximum
difference in tokens between any two neighbors is at most 2d), while the global imbalance remains ((d 2 log n)=ff). Furthermore, we show
that upon reaching a state with a global imbalance of O((d 2 log n)=ff), the time for this algorithm to locally balance the network can be
as large as (n 1=2 ). We extend our analysis to a variant of this algorithm for dynamic and asynchronous networks. We also present tight
bounds for a randomized algorithm in which each node sends at most one token in each step.
Keywords: load balancing, distributed network algorithms.
AMS subject classification: 68Q22
1. Introduction. A natural way to balance the workload in a distributed system is to have each work station

A PRINCIPLED REPRESENTATION OF ATTRIBUTIVE DESCRIPTIONS
FOR GENERATING INTEGRATED TEXT AND INFORMATION
GRAPHICS PRESENTATIONS
Nancy Green*, Giuseppe Carenini**, and Johanna Moore**
*Carnegie Mellon University, **University of Pittsburgh
nancy.green@cs.cmu.edu, fjmoore,careninig@cs.pitt.edu
Abstract
This paper describes a media-independent, compositional, plan-based approach to representing attributive descriptions for use in integrated text and graphics generation. An attributive
description's main function is to convey information directly contributing to the communicative
goals of a discourse, whereas a referential description's only function is to enable the audience
to identify a particular referent. This approach has been implemented as part of an architecture
for generating integrated text and information graphics. Uses of referential and attributive descriptions are represented as two distinct types of communicative acts in a media-independent
plan. It is particularly important to distinguish the two types of acts, since they have different
consequences for dialogue and text generation, and for graphic design.
1 Introduction

, , 1-8 ()
c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
A Note on Learning from Multiple-Instance
Examples
AVRIM BLUM   avrim+@cs.cmu.edu
ADAM KALAI   akalai+@cs.cmu.edu
School of Computer Science, Carnegie Mellon University,   Pittsburgh, PA 15213
Abstract.
We describe a simple reduction from the problem of PAC-learning from multiple-instance examples to that of PAC-learning with one-sided random classification noise. Thus, all concept classes
learnable with one-sided noise, which includes all concepts learnable in the usual 2-sided random
noise model plus others such as the parity function, are learnable from multiple-instance examples. We also describe a more efficient (and somewhat technically more involved) reduction to
the Statistical-Query model that results in a polynomial-time algorithm for learning axis-parallel
rectangles with sample complexity ~ O(d 2 r=* 2 ), saving roughly a factor of r over the results of Auer
et al. (1997).
Keywords: Multiple-instance examples, classification noise, statistical queries
1. Introduction and Definitions

Proceedings of the 4th International Symposium on Intelligent Robotic Systems SIRS'96, Lisbon, Portugal, July 22-26, 1996
Memory-based Stochastic Optimization for Automated Tuning
of Neural Network's High Level Parameters ?
Artur Dubrawski ??
The Robotics Institute, Carnegie Mellon University,   5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract. In this paper we describe a new method for automated tuning of high level parameters of supervised
learning systems. It uses memory-based learning principles and follows certain ideas of experimental design. The
described method allows not only for an efficient search through a decision space, but also for a concurrent validation
of the learning algorithm performance on a given data. Potential usefulness of the proposed approach is illustrated
with the Fuzzy-ARTMAP neural network application to learning a qualitative positioning of an indoor mobile robot
equipped with sonar range sensors. Automatically selected neural network setpoints reach a comparable performance
to those achieved by human experts in relatively simple 2D cases. Migration of the proposed method to higher order
optimization domains bears a big promise, but requires further research.
1 Introduction

SIGGRAPH 95, Los Angeles, August 6-11 COMPUTER GRAPHICS Proceedings, Annual Conference Series, 1995
Interactive Physically-Based Manipulation
of Discrete/Continuous Models
Mikako Harada
Department of Architecture
Andrew Witkin
Department of Computer Science
David Baraff
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213
Abstract
Physically-based modeling has been used in the past to support a variety of interactive modeling tasks including free-form surface design, mechanism design, constrained drawing, and interactive camera control. In these systems, the user interacts with the model
by exerting virtual forces, to which the system responds subject
to the active constraints. In the past, this kind of interaction has
been applicable only to models that are governed by continuous
parameters. In this paper we present an extension to mixed con-tinuous/discrete models, emphasizing constrained layout problems
that arise in architecture and other domains. When the object being
dragged is blocked from further motion by geometric constraints, a
local discrete search is triggered, during which transformations such
as swapping of adjacent objects may be performed. The result of the
search is a nearby state in which the target object has been moved
in the indicated direction and in which all constraints are satisfied.
The transition to this state is portrayed using simple but effective animated visual effects. Following the transition, continuous dragging
is resumed. The resulting seamless transitions between discrete and
continuous manipulation allow the user to easily explore the mixed
design space just by dragging objects. We demonstrate the method
in application to architectural floor plan design, circuit board layout,
art analysis, and page layout.
KeywordsInteractive techniques, physically-based modeling,
grammars.
1 Introduction

To appear in the Proceedings of the 16th ACM Symposium on Operating System Principles
Agile Application-Aware Adaptation for Mobility
Brian D. Noble, M. Satyanarayanan, Dushyanth Narayanan, James Eric Tilton, Jason Flinn, Kevin R. Walker
School of Computer Science
Carnegie Mellon University
Abstract
In this paper we show that application-aware adaptation, a
collaborative partnership between the operating system and
applications, offers the most general and effective approach
to mobile information access. We describe the design of
Odyssey, a prototype implementing this approach, and show
how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it.
We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a
benchmark of three applications concurrently using remote
services over a network with highly variable bandwidth.
1 Introduction

Mixed-Initiative Management of Integrated Process-Planning and
Production-Scheduling Solutions
David W. Hildum, Norman M. Sadeh, Thomas J. Laliberty,
Stephen F. Smith, John McA'Nulty and Dag Kjenstad
Intelligent Coordination and Logistics Laboratory
Center for Integrated Manufacturing Decision Systems
The Robotics Institute
Carnegie Mellon University
Pittsburgh PA 15213-3890
412.268.7598 fax: 412.268.5569
fHILDUM,SADEH,SFS,DAGg@ISL1.RI.CMU.EDU
Software Engineering Laboratory
Raytheon Electronic Systems
Raytheon Company
Tewksbury MA 01876-0901
508.858.5756 fax: 508.858.5976
LALIBERTY THOMAS@CAEMAC.MSD.RAY.COM
MCANULTY@CAESUN.MSD.RAY.COM
Abstract
Increased reliance on agile manufacturing techniques has created a demand for systems to solve integrated process-planning
and production-scheduling problems in large-scale dynamic environments. To be effective, these systems should provide user-oriented interactive functionality for managing the various user
tasks and objectives and reacting to unexpected events. This paper describes the mixed-initiative problem-solving features of
IP3S, an Integrated Process-Planning/Production-Scheduling
shell for agile manufacturing. IP3S is a blackboard -based system
that supports the concurrent development and dynamic revision
of integrated process-planning and production-scheduling solutions and the maintenance of multiple problem instances and
solutions, as well as other flexible user-oriented decision-making
capabilities, allowing the user to control the scope of the problem and explore alternate tradeoffs (what-if scenarios) interactively. The system is scheduled for initial deployment
and evaluation in a large and highly dynamic machine shop at
Raytheon's Andover manufacturing facility.
Introduction

Proportional-Share Scheduling: Implementation and
Evaluation in a Widely-Deployed Operating System
David Petrou and John Milford
dpetrou@cs.cmu.edu, jwm@csua.berkeley.edu
Draft 12/20/1997. Send feedback to the authors.
Abstract
This paper explores the feasibility of using lottery scheduling, a proportional-share resource management algorithm,
to schedule processes under the FreeBSD operating system.
Proportional-share scheduling enables flexible control over
relative process execution rates and processor load insulation among groups of processes. We show that a straight implementation of lottery scheduling performs worse than the
standard FreeBSD scheduler. This initial result prompted
us to extend lottery scheduling. Except for one test we
run, our resulting system performs within one percent of
the FreeBSD scheduler. We describe our design, evaluate
our implementation, and relate our experience in deploying
our lottery scheduler on production machines.
1 Introduction

Operant Conditioning in Skinnerbots
David S. Touretzky
Computer Science Department &
Center for the Neural Basis of Cognition
Carnegie Mellon University
Pittsburgh, PA 15213-3891
dst@cs.cmu.edu
Lisa M. Saksida
Robotics Institute &
Center for the Neural Basis of Cognition
Carnegie Mellon University
Pittsburgh, PA 15213-3891
saksida@ri.cmu.edu
To appear in Adaptive Behavior 5(3/4), 1997.
Copyright c 1997 The MIT Press.
Abstract
Instrumental (or operant) conditioning, a form of animal learning, is similar to reinforcement learning
(Watkins, 1989) in that it allows an agent to adapt its actions to gain maximally from the environment
while only being rewarded for correct performance. But animals learn much more complicated behaviors
through instrumental conditioning than robots presently acquire through reinforcement learning. We
describe a new computational model of the conditioning process that attempts to capture some of the
aspects that are missing from simple reinforcement learning: conditioned reinforcers, shifting reinforcement contingencies, explicit action sequencing, and state space refinement. We apply our model to a task
commonly used to study working memory in rats and monkeys: the DMTS (Delayed Match to Sample)
task. Animals learn this task in stages. In simulation, our model also acquires the task in stages, in a
similar manner. We have used the model to train an RWI B21 robot.
Keywords: operant conditioning, instrumental learning, shaping, chaining, learning robots
Running Head: Operant Conditioning in Skinnerbots
+PAGE+

May 3, 1997 1
Middleware Enabled Fault Management for
Commercial Operating Systems
April 29, 1997
for submission to: Software Reliability and Fault Tolerance Track of the Computer Science Division at the
15th Annual International Conference of the AOM/IAOM
Author Information:
Charlotte A. Rekiere
Carnegie Mellon University
5000 Forbes Ave.
2201 Hamburgh Hall
Pittsburgh, PA 15232
PH: (412) 621-9406
email: crekiere@cs.cmu.edu
Daniel P. Siewiorek
Professor CS/ECE
Carnegie Mellon University
5000 Forbes Ave.
1201 Hamburgh Hall
Pittsburgh, PA 15232
PH: (412)268-2570
email: dps@cs.cmu.edu
+PAGE+

Improving Text Classification by Shrinkage in a Hierarchy of Classes
Andrew McCallum
mccallum@justresearch.com
Ronald Rosenfeld
roni@cs.cmu.edu
Tom Mitchell
mitchell+@cs.cmu.edu
Andrew Y. Ng
ayn@ai.mit.edu
Just Research
4616 Henry Street
Pittsburgh, PA 15213
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
MIT AI Lab
545 Technology Square
Cambridge, MA 02139
Abstract
When documents are organized in a large
number of topic categories, the categories
are often arranged in a hierarchy. The U.S.
patent database and Yahoo are two examples.
This paper shows that the accuracy of a naive
Bayes text classifier can be significantly improved by taking advantage of a hierarchy of
classes. We adopt an established statistical
technique called shrinkage that smoothes parameter estimates of a data-sparse child with
its parent in order to obtain more robust parameter estimates. The approach is also employed in deleted interpolation, a technique
for smoothing n-grams in language modeling
for speech recognition.
Our method scales well to large data sets,
with numerous categories in large hierarchies.
Experimental results on three real-world data
sets from UseNet, Yahoo, and corporate web
pages show improved performance, with a reduction in error up to 29% over the tradi
tional flat classifier.
1 Introduction

Abstract Time Warping of Compound Events and Signals
Roger B. Dannenberg
Carnegie Mellon University
Pittsburgh, PA 15213 USA
dannenberg@cs.cmu.edu
ABSTRACT: Functions of time are often used to represent continuous parameters and the passage
of musical time (tempo). A new approach generalizes previous work in three ways. First, common
temporal operations of stretching and shifting are special cases of a new general time-warping
operation. Second, these operations are ``abstract.'' Instead of operating directly on signals or
events, they operate on abstract behaviors that interpret the operations at an appropriate structural
level. Third, time warping can be applied to both discrete events and continuous signals.
1. Introduction 2. Shift, Stretch, and Warp

Remote Access to Interactive Media
Roger B. Dannenberg
Carnegie Mellon University, School of Computer Science
Pittsburgh, PA 15213 USA
Email: dannenberg@cs.cmu.edu
ABSTRACT
Digital interactive media augments interactive computing with video, audio, computer graphics and text,
allowing multimedia presentations to be individually and dynamically tailored to the user. Multimedia, and
particularly continuous media pose interesting problems for system designers, including those of latency
and synchronization. These problems are especially evident when multimedia data is remote and must be
accessed via networks. Latency and synchronization issues are discussed, and an integrated system,
Tactus, is described. Tactus facilitates the implementation of interactive multimedia computer programs by
managing latency and synchronization in the framework of an object-oriented graphical user interface
toolkit.
1. Introduction

INFORMING LOADS: ENABLING SOFTWARE TO
OBSERVE AND REACT TO MEMORY BEHAVIOR
Mark Horowitz
Margaret Martonosi
Todd C. Mowry
Michael D. Smith
Technical Report No. CSL-TR-95-673
(also numbered STAN-CS-95-673)
July 1995
This research has been supported by ARPA contract DABT63-94-C-0054.
In addition, Margaret Martonosi is supported in part by a National Science Foundation
Career Award (CCR-9502516). Todd C. Mowry is supported by a Research Grant
from the Natural Sciences and Engineering Research Council of Canada. Michael D.
Smith is supported by the National Science Foundation under a Young Investigator
Grant No. CCR-9457779.
+PAGE+

Informing Memory Operations: Providing Memory
Performance Feedback in Modern Processors
Mark Horowitz Margaret Martonosi Todd C. Mowry Michael D. Smith
Computer Systems Department of Department of Electrical Division of
Laboratory Electrical Engineering and Computer Engineering Applied Sciences
Stanford University Princeton University University of Toronto Harvard University
horowitz@ee.stanford.edu martonosi@princeton.edu tcm@eecg.toronto.edu smith@eecs.harvard.edu
Abstract
Memory latency is an important bottleneck in system performance
that cannot be adequately solved by hardware alone. Several promising software techniques have been shown to address this problem
successfully in specific situations. However, the generality of these
software approaches has been limited because current architectures
do not provide a fine-grained, low-overhead mechanism for
observing and reacting to memory behavior directly. To fill this
need, we propose a new class of memory operations called informing memory operations, which essentially consist of a memory
operation combined (either implicitly or explicitly) with a conditional branch-and-link operation that is taken only if the reference
suffers a cache miss. We describe two different implementations of
informing memory operationsone based on a cache-outcome
condition code and another based on low-overhead trapsand find
that modern in-order-issue and out-of-order-issue superscalar processors already contain the bulk of the necessary hardware support.
We describe how a number of software-based memory optimizations can exploit informing memory operations to enhance performance, and look at cache coherence with fine-grained access
control as a case study. Our performance results demonstrate that
the runtime overhead of invoking the informing mechanism on the
Alpha 21164 and MIPS R10000 processors is generally small
enough to provide considerable exibility to hardware and software designers, and that the cache coherence application has
improved performance compared to other current solutions. We
believe that the inclusion of informing memory operations in
future processors may spur even more innovative performance
optimizations.
1 Introduction

Submitted to the Future Generation Computer Systems special issue on Data Mining.
Using Neural Networks for Data Mining
Mark W. Craven
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213-3891
mark.craven@cs.cmu.edu
Jude W. Shavlik
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706-1685
shavlik@cs.wisc.edu
Abstract
Neural networks have been successfully applied in a wide range of supervised and unsupervised learning applications. Neural-network methods are not commonly used for data-mining
tasks, however, because they often produce incomprehensible models and require long training
times. In this article, we describe neural-network learning algorithms that are able to produce
comprehensible models, and that do not require excessive training times. Specifically, we discuss
two classes of approaches for data mining with neural networks. The first type of approach,
often called rule extraction, involves extracting symbolic models from trained neural networks.
The second approach is to directly learn simple, easy-to-understand networks. We argue that,
given the current state of the art, neural-network methods deserve a place in the tool boxes of
data-mining specialists.
Keywords: machine learning, neural networks, rule extraction, comprehensible
models, decision trees, perceptrons
1 Introduction

A Simple Algorithm for Finding the
Maximum Recoverable System State in
Optimistic Rollback Recovery Methods
David B. Johnson
Peter J. Keleher
Willy Zwaenepoel
Rice COMP TR90-125
July 1990
Department of Computer Science
Rice University
P.O. Box 1892
Houston, Texas 77251-1892
(713) 527-4834
This research was supported in part by the National Science Foundation under grants CCR-8716914 and
CDA-8619893, and by the Office of Naval Research under contract ONR N00014-88-K-0140.
+PAGE+

A Prototype Reading Coach that Listens
Jack Mostow, Steven F. Roth, Alexander G. Hauptmann, and Matthew Kane
Project LISTEN,   215 Cyert Hall,   Carnegie Mellon University Robotics Institute
4910 Forbes Avenue, Pittsburgh, PA 15213-3890
mostow@cs.cmu.edu
COPYRIGHT NOTICE
This publication and its companion video and transcript (Mostow et al, 1994c) are copyrighted:
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane. (August 1994). A Prototype Reading Coach that Listens. Proceedings
of the Twelfth National Conference on Artificial Intelligence (AAAI-94). Seattle, WA, American Association for
Artificial Intelligence, Recipient of the AAAI-94 Outstanding Paper Award.
J. Mostow, S. Roth, A. Hauptmann, M. Kane, A. Swift, L. Chase, and B. Weide. (August 1994). A Reading Coach that
Listens (6-minute video). Video Track of the Twelfth National Conference on Artificial Intelligence (AAAI94). Seattle,
WA, American Association for Artificial Intelligence.
J. Mostow, S. Roth, A. Hauptmann, M. Kane, A. Swift, L. Chase, and B. Weide. (August 1994). A reading coach that
listens: (edited) video transcript. Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI94).
Seattle, WA.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee.
newpage
The following notice appears on page xiv of the proceedings:
AAAI-94
Outstanding Paper Award
A Prototype Reading Coach that Listens
Jack Mostow, Steven F. Roth, Alexander G. Hauptmann, and Matthew Kane
This year, AAAI's National Conference on Artifical Intelligence honors a paper that exemplifies high standards in
technical contribution and exposition. Papers were nominated for the Outstanding Paper Award by members of the program
committee during the NCAI review process. These nominations were then reviewed once again by a smaller subset of the
program committee to select the winning paper. Care was taken during the review process to ensure that our final decisions
were based on the opinions of impartial readers who are free from personal biases and conflicts of interest.
+PAGE+

Towards a Principled Representation
of Discourse Plans
R. Michael Young
Johanna D. Moore
Martha E. Pollack
ISP Technical Report 94-2
May 1994
Abstract
We argue that discourse plans must capture the intended causal and decompositional relations between communicative actions. We present a planning algorithm, DPOCL, that builds plan structures that properly capture these relations, and show
how these structures are used to solve the problems that plagued previous discourse planners, and allow a system to participate
effectively and flexibly in an ongoing dialogue.
A version of this paper appears in the Proceedings of the Sixteenth Annual Meeting
of the Cognitive Science Society, Atlanta, GA, 1994.
+PAGE+

Simulating Soft Shadows
with Graphics Hardware
Paul S. Heckbert and Michael Herf
January 15, 1997
CMU-CS-97-104
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
email: ph@cs.cmu.edu, herf+@cmu.edu
World Wide Web: http://www.cs.cmu.edu/~ph
This paper was written in April 1996. An abbreviated version appeared in [Michael Herf and Paul S. Heckbert, Fast
Soft Shadows, Visual Proceedings, SIGGRAPH 96, Aug. 1996, p. 145].
Abstract
This paper describes an algorithm for simulating soft shadows at interactive rates using graphics hardware. On current graphics
workstations, the technique can calculate the soft shadows cast by moving, complex objects onto multiple planar surfaces in
about a second. In a static, diffuse scene, these high quality shadows can then be displayed at 30 Hz, independent of the number
and size of the light sources.
For a diffuse scene, the method precomputes a radiance texture that captures the shadows and other brightness variations on
each polygon. The texture for each polygon is computed by creating registered projections of the scene onto the polygon from
multiple sample points on each light source, and averaging the resulting hard shadow images to compute a soft shadow image.
After this precomputation, soft shadows in a static scene can be displayed in real-time with simple texture mapping of the
radiance textures. All pixel operations employed by the algorithm are supported in hardware by existing graphics workstations.
The technique can be generalized for the simulation of shadows on specular surfaces.
This work was supported by NSF Young Investigator award CCR-9357763. The views and conclusions contained in this document are those of the authors and
should not be interpreted as representing the official policies, either expressed or implied, of NSF or the U.S. government.
+PAGE+

NIFDY: A Low Overhead, High Throughput Network Interface
Timothy Callahan and Seth Copen Goldstein
ftimothyc,sethgg@cs.berkeley.edu
Computer Science Division
University of California-Berkeley
Abstract
In this paper we present NIFDY, a network interface that uses admission control to reduce congestion and ensures that packets are
received by a processor in the order in which they were sent, even
if the underlying network delivers the packets out of order. The
basic idea behind NIFDY is that each processor is allowed to have at
most one outstanding packet to any other processor unless the destination processor has granted the sender the right to send multiple
unacknowledged packets. Further, there is a low upper limit on the
number of outstanding packets to all processors.
We present results from simulations of a variety of networks
(meshes, tori, butterflies, and fat trees) and traffic patterns to verify NIFDY's efficacy. Our simulations show that NIFDY increases
throughput and decreases overhead. The utility of NIFDY increases
as a network's bisection bandwidth decreases. When combined
with the increased payload allowed by in-order delivery NIFDY increases total bandwidth delivered for all networks. The resources
needed to implement NIFDY are small and constant with respect to
network size.
1 Introduction

Implementing Bit-addressing with Specialization
Scott Draves
School of Computer Science
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
General media-processing programs are easily expressed with bit-addressing and variable-sized bit-fields. But the natural implementation of bit-addressing relies on dynamic shift offsets and repeated
loads, resulting in slow execution. If the code is specialized to the
alignment of the data against word boundaries, the offsets become
static and many repeated loads can be removed. We show how introducing modular arithmetic into an automatic compiler generator
enables the transformation of a program that uses bit-addressing
into a synthesizer of fast specialized programs.
In partial-evaluation jargon we say: modular arithmetic is supported by extending the binding time lattice used by the static analysis in a polyvariant compiler generator. The new binding time
Cyclic functions like a partially static integer.
A software cache combined with a fast, optimistic sharing analysis built into the compilers eliminates repeated loads and stores.
The utility of the transformation is demonstrated with a collection
of examples and benchmark data. The examples include vector
arithmetic, audio synthesis, image processing, and a base-64 codec.
1 Introduction

Exploiting Global Input/Output Access Pattern Classification
Tara M. Madhyastha Daniel A. Reed
ftara,reedg@cs.uiuc.edu
Department of Computer Science
University of Illinois
Urbana, Illinois 61801
1 Introduction

In: Proceedings of EURASIP Workshop, February 15-17, 1990, Portugal
INVERSION IN TIME
Sebastian Thrun Alexander Linden
Gesellschaft fur Mathematik und Datenverarbeitung mbH
D-5205 St. Augustin, West Germany
Abstract
Inversion of multilayer synchronous networks is a method which tries to answer questions
like "What kind of input will give a desired output?" or "Is it possible to get a desired
output (under special input/output constraints)?".
We will describe two methods of inverting a connectionist network. Firstly, we extend
inversion via backpropagation (Linden/Kindermann [4], Williams [11]) to recurrent (El-man [1], Jordan [3], Mozer [5], Williams/Zipser [10]), time-delayed (Waibel at al. [9])
and discrete versions of continuous networks (Pineda [7], Pearlmutter [6]). The result
of inversion is an input vector. The corresponding output vector is equal to the target
vector except a small remainder. The knowledge of those attractors may help to understand the function and the generalization qualities of connectionist systems of this
kind.
Secondly, we introduce a new inversion method for proving the non-existence of an
input combination under special constraints, e.g. in a subspace of the input space. This
method works by iterative exclusion of invalid activation values. It might be a helpful
way to judge the properties of a trained network.
We conclude with simulation results of three different tasks: XOR, morse signal decoding
and handwritten digit recognition.
Keywords: connectionist systems, backpropagation, inversion, recurrent neural networks, digit recognition
1 Introduction

SUPRENUM: Perspectives and Performance
Oliver A. McBryan
Dept. of Computer Science
University of Colorado
Boulder, CO 80309.
Email: mcbryan@cs.colorado.edu
Abstract
We describe our impressions of the SUPRENUM project and of its primary
supercomputer result, the Suprenum-1 prototype. We comment on the significance
of the architecture, its role among contemporary systems and its relevance to
current systems. We similarly discuss the SUPRENUM software and its impact on
distributed systems. Finally we discuss the successes and failures observed
throughout this exciting project and relate these to the organizational decisions on
which SUPRENUM was based.
As an illustration of Suprenum-1 capabilities, we describe the
implementation of a fluid dynamical benchmark on the 256 node Suprenum-1
parallel computer. The benchmark, the Shallow Water Equations, is frequently used
as a model for both oceanographic and atmospheric circulation. We describe the
steps involved in implementing the algorithm on the Suprenum-1 and we provide
details of performance obtained. For such regular grid-based algorithms the system
delivers a very impressive fraction (25%) of its theoretical peak rate of 5 Gflops.
Keywords: SUPRENUM, MPP, MIMD, parallel, supercomputer, performance,
atmospheric, shallow water, architecture, software, message passing.
* Research supported in part by NSF Grand Challenges Applications Group grant ASC-9217394 and by
NASA HPCC Group Grant NAG5-2218.
To appear in Parallel Computing, October 1994.
+PAGE+

Metamorphosis Networks:
An Alternative to Constructive Methods
Brian V. Bonnlander Michael C. Mozer
Department of Computer Science &
Institute of Cognitive Science
University of Colorado
Boulder, CO 80309-0430
Abstract
Given a set of training examples, determining the appropriate number of free parameters is a challenging problem. Constructive
learning algorithms attempt to solve this problem automatically by
adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms|called metamorphosis algorithms|in which the number of units is fixed, but
the number of free parameters gradually increases during learning.
The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the
network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and
interpolation of sparse training data.
1 INTRODUCTION

The Sybil Database Integration and Evolution Environment: An Overview
Roger King, Michael Novak, Christian Och, Richard Osborne
Department of Computer Science
University of Colorado
Campus Box 430
Boulder, CO 80309-0430
roger@cs.colorado.edu, rick@cs.colorado.edu
Fernando Velez
Unidata Inc.
1099 18th Street
Suite 2200
Denver, CO 80202-1925
Abstract
Sybil is a database integration and evolution environment for supporting large, heterogeneous applications.
We are interested in using Sybil to support the data integration and evolution needs of applications that are using legacy databases and are looking to integrate with
more modern database systems. The Sybil approach is
based on loosely coupling databases or other persistent
tools into lightweight alliances tailored for specific applications. Such alliances are built via four sorts of constructs: heterogeneous views, inter-database constraints,
inter-database propagations, and integration supported
by domain specific information.
Sybil Overview

Schema Evolution in Object Databases: Measuring
the Performance of Immediate and Deferred Updates
Fabrizio Ferrandina Thorsten Meyer Roberto Zicari
Johann Wolfgang Goethe-Universitat Frankfurt
Fachbereich Informatik (20)
Datenbanken und Informationssysteme (DBIS)
Robert-Mayer-Str. 11-15
D-60325 Frankfurt am Main, Germany
fferrandi,meyer,zicarig@dbis.informatik.uni-frankfurt.de
http://www.dbis.informatik.uni-frankfurt.de
Abstract
When the schema of an object database system is modified, the database needs to be changed
in such a way that the schema and the database remain consistent with each other. This paper
uses the OO1 benchmark [2], appropriately modified, to compare the two most used approaches for
transforming the database, namely the immediate and the deferred database transformation [4].
1 Immediate vs. Lazy Database Updates

Goal-Directed Classification using Linear Machine
Decision Trees
Bruce A. Draper Carla E. Brodley Paul E. Utgoff
Department of Computer Science
University of Massachusetts
Amherst, MA., USA. 01003
bdraper@cs.umass.edu
September 17, 1993
Abstract
Recent work in feature-based classification has focused on non-parametric techniques that can classify instances even when the underlying feature distributions are
unknown. The inference algorithms for training these techniques, however, are designed
to maximize the accuracy of the classifier, with all errors weighted equally. In many
applications, certain errors are far more costly than others, and the need arises for
non-parametric classification techniques that can be trained to optimize task-specific
cost functions. This paper reviews the Linear Machine Decision Tree (LMDT) algorithm for inducing multivariate decision trees, and shows how LMDT can be altered to
induce decision trees that minimize arbitrary misclassification cost functions (MCFs).
Demonstrations of pixel classification in outdoor scenes show how MCFs can optimize
the performance of embedded classifiers within the context of larger image understand
ing systems.
Keywords: Decision Trees, Non-Parametric Classification, Pattern Recognition, Object Recognition, Computer Vision, Machine Learning.
1 Introduction

Cellular Encoding Applied to Neurocontrol
Darrell Whitley, Frederic Gruau and Larry Pyeatt
Department of Computer Science
Colorado State University
Fort Collins, Colorado 80523 USA
whitley,gruau,pyeatt@cs.colostate.edu +L +
Abstract
Neural networks are trained for balancing 1
and 2 poles attached to a cart on a fixed
track. For one variant of the single pole system, only pole angle and cart position variables are supplied as inputs; the network
must learn to compute velocities. All of the
problems are solved using a fixed architecture
and using a new version of cellular encoding that evolves an application specific architecture with real-valued weights. The learning times and generalization capabilities are
compared for neural networks developed using both methods. After a post processing
simplification, topologies produced by cellular encoding were very simple and could be
analyzed. Architectures with no hidden units
were produced for the single pole and the two
pole problem when velocity information is
supplied as an input. Moreover, these linear
solutions display good generalization. For all
the control problems, cellular encoding can
automatically generate architectures whose
complexity and structure reflect the features
of the problem to solve.
1 Introduction

Efficient Indexing for Object Recognition Using Large Networks
Mark R. Stevens Charles W. Anderson J. Ross Beveridge
Department of Computer Science
Colorado State University
Fort Collins, CO 80523
fstevensm,anderson,rossg@cs.colostate.edu
Abstract
Template matching is an effective means of locating vehicles in outdoor scenes, but it tends to be
a computationally expensive. To reduce processing time, we use large neural networks to predict, or
index, a small subset of templates that are likely to match each window in an image. Results on actual
LADAR range images show that limiting the templates to those selected by the neural networks reduces
the computation time by a factor of 5 without sacrificing the accuracy of the results.
1 Introduction

Measured Performance of a Wireless LAN
Dan Duchamp and Neil F. Reynolds
Computer Science Department
Columbia University
500 W. 120th St.
New York, NY 10027
fdjd,nfrg@cs.columbia.edu
Abstract
We have studied the performance of a high-speed
commercial spread-spectrum wireless LAN that uses the
CSMA/CA multiple-access strategy. Employing synthetic workloads, we measured packet capture success
more so than signal propagation characteristics. Specifically, we measured throughput, packet loss rates, range,
and patterns of errors within packets. We conclude
that CSMA/CA is quite successful in allocating bandwidth under stress, but that packet capture rate degrades very quickly once the LAN's effective range is
exceeded. Hence, network maintainers should plan the
layout of wireless networks at least as carefully as they
plan wired networks.
1 Introduction

Internet Telephony: A (Partial) Research Agenda
Jonathan Rosenberg
Bell Laboratories and Columbia U.
Rm. 4C-526
101 Crawfords Corner Rd.
Holmdel, NJ 07733
jdrosen@bell-labs.com
TEL: +1 908 949-6418
October 17, 1997
1 Introduction

Predictive Dynamic Load Balancing of Parallel Hash-Joins over
Heterogeneous Processors in the Presence of Data Skew
Hasanat M. Dewan Mauricio Hernandez
Kui W. Mok Salvatore J. Stolfo
Department of Computer Science, Columbia University,   New York, NY 10027
CUCS-026-94
(This is an extended version of the paper that appeared in the Proceedings of the
3rd International Conference on Parallel and Distributed Information Systems.)
Abstract
In this paper, we present new algorithms to balance
the computation of parallel hash joins over heterogeneous processors in the presence of data skew and external loads. Heterogeneity in our model consists of
disparate computing elements, as well as general purpose computing ensembles that are subject to external loading (e.g., a LAN connected workstation cluster). Data skew manifests itself as significant non-uniformities in the distribution of attribute values of
underlying relations that are involved in a join.
We develop cost models and predictive dynamic
load balancing protocols to detect imbalance during
the computation of a single large join. New predictive bucket scheduling algorithms are presented that
smooth out the load over the entire ensemble by reallocating buckets whenever imbalance is detected. Our
algorithms can account for imbalance due to data skew
as well as heterogeneity in the computing environment.
Significant performance gains are reported for a wide
range of test cases on a prototype implementation of
the system.
1 Introduction

Mining Audit Data to Build Intrusion Detection Models
Wenke Lee Salvatore J. Stolfo
Kui W. Mok
Computer Science Department
Columbia University
500 West 120th Street, New York, NY 10027
fwenke,sal,mokg@cs.columbia.edu
Abstract
In this paper we discuss a data mining framework for constructing intrusion detection models.
The key ideas are to mine system audit data for consistent and useful patterns of program and
user behavior, and use the set of relevant system features presented in the patterns to compute
(inductively learned) classifiers that can recognize anomalies and known intrusions. Our past experiments showed that classifiers can be used to detect intrusions, provided that sufficient audit
data is available for training and the right set of system features are selected. We propose to use
the association rules and frequent episodes computed from audit data as the basis for guiding the
audit data gathering and feature selection processes. We modify these two basic algorithms to use
axis attribute(s) as a form of item constraints to compute only the relevant (useful) patterns, and
an iterative level-wise approximate mining procedure to uncover the low frequency (but important)
patterns. We report our experiments in using these algorithms on real-world audit data.
Keywords: Intrusion detection, audit data, classification, association rules, frequent episodes.
Contact Author:   Wenke Lee,   wenke@cs.columbia.edu,   (212) 939-7078.
This research is supported in part by grants from DARPA (F30602-96-1-0311) and NSF (IRI-96-32225 and CDA-96-25374).
+PAGE+

Scalability of Hierarchical Meta-Learning
on Partitioned Data
Philip K. Chan
Computer Science
Florida Institute of Technology
Melbourne, FL 32901
pkc@cs.fit.edu
FAX: (407) 984-8461
Salvatore J. Stolfo
Department of Computer Science
Columbia University
New York, NY 10027
sal@cs.columbia.edu
(212) 939-7080
May 8, 1997
Abstract
In this paper we study the issue of how to scale machine learning algorithms, that
typically are designed to deal with main-memory based datasets, to efficiently learn
models from large distributed databases. We have explored an approach called meta-learning that is related to the traditional approaches of data reduction commonly
employed in distributed database query processing systems. We explore the scalability
of learning arbiter and combiner trees from partitioned data. Arbiter and combiner
trees integrate classifiers trained in parallel from small disjoint subsets. Previous work
demonstrated the efficacy of these meta-learning architectures in terms of accuracy
of the computed meta-classifiers. Here we discuss the computational performance
of constructing arbiter and combiner trees in terms of speedup and scalability as a
function of database size and number of partitions. The performance of serial learning
algorithms is evaluated. We then analyze the performance of the algorithms used to
construct combiner and arbiter trees in parallel. Our empirical results validate these
analyses and indicate that the techniques can effectively scale up to large datasets with
millions of records using cheap commodity hardware.
Keywords: speedup, scalability, arbiter and combiner trees, meta-learning, parallel/distributed
processing, inductive learning
This work was partially funded by grants from NSF (IRI-96-32225 & CDA-96-25374), ARPA (F30602
96-1-0311), and NYSSTF (423115-445).
+PAGE+

Learning Distributions from Random Walks
Funda Ergun S Ravi Kumar
Department of Computer Science
Cornell University
Ithaca, NY 14853.
Ronitt Rubinfeld
Abstract
We introduce a new model of distributions generated by random walks on graphs. This model
suggests a variety of learning problems, using the definitions and models of distribution
learning defined in [6]. Our framework is general enough to model previously studied distribution learning problems, as well as to suggest
new applications. We describe special cases of
the general problem, and investigate their relative difficulty. We present algorithms to solve
the learning problem under various conditions.
1 INTRODUCTION

Compiling for distributed memory architectures
Anne Rogers and Keshav Pingali
June 10, 1991
+PAGE+

Embedded Machine Learning Systems for
Natural Language Processing: A General
Framework
Claire Cardie
Department of Computer Science, Cornell University,   Ithaca NY 14853, USA
In Wermter, S. and Riloff, E. and Scheler, Gabriele (eds.), Connectionist, Statistical and
Symbolic Approaches to Learning for Natural Language Processing, Lecture Notes in
Artificial Intelligence, 315-328, Springer, 1996.
Abstract. This paper presents Kenmore, a general framework for knowledge acquisition for natural language processing (NLP) systems. To ease
the acquisition of knowledge in new domains, Kenmore exploits an online corpus using robust sentence analysis and embedded symbolic machine learning techniques while requiring only minimal human intervention. By treating all problems in ambiguity resolution as classification
tasks, the framework uniformly addresses a range of subproblems in sentence analysis, each of which traditionally had required a separate computational mechanism. In a series of experiments, we demonstrate the
successful use of Kenmore for learning solutions to several problems in
lexical and structural ambiguity resolution. We argue that the learning
and knowledge acquisition components should be embedded components
of the NLP system in that (1) learning should take place within the
larger natural language understanding system as it processes text, and
(2) the learning components should be evaluated in the context of prac
tical language-processing tasks.
1 Introduction

Comparing Mostly-Copying and Mark-Sweep Conservative Collection
Frederick Smith
Cornell University
fms@cs.cornell.edu
Greg Morrisett
Cornell University
jgm@cs.cornell.edu
Abstract
Many high-level language compilers generate C code and
then invoke a C compiler for code generation. To date, most
of these compilers link the resulting code against a conservative mark-sweep garbage collector in order to reclaim unused
memory. We introduce a new collector, MCC, based on an
extension of mostly-copying collection.
We analyze the various design decisions made in MCC
and provide a performance comparison to the most widely
used conservative mark-sweep collector (the Boehm-Demers-Weiser collector). Our results show that a good mostly-copying collector can outperform a mature highly-optimized
mark-sweep collector when physical memory is large relative
to the live data. A surprising result of our analysis is that
cache behavior can have a greater impact on overall performance than either collector time, or allocation code.
1 Overview

On-Line Search in a Simple Polygon
Jon M. Kleinberg
Abstract
We consider a number of search and exploration problems, from the perspective of
robot navigation in a simple polygon. These problems are "on-line" in the sense that
the robot does not have access to the map of the polygon; it must make decisions as it
proceeds, based only on what it has seen so far. For the problem of exploring a simple
rectilinear polygon (under the L 1 norm), Deng, Kameda, and Papadimitriou give a
2-competitive deterministic algorithm; we present a randomized exploration algorithm
which is 5=4-competitive. Using similar techniques, we are able to give an algorithm
for searching an arbitrary, unknown rectilinear polygon. No constant competitive ratio
is attainable in this case, but our algorithm is within a constant factor of optimal in
the worst case; in a sense, it is a generalization of some of the strategies of Baeza-Yates, Culberson, and Rawlins to a much more general class of search spaces. Finally,
we examine a type of polygon for which competitive search is possible | the class of
"streets" considered by Klein, who gave a 1 + 3
2 -competitive algorithm for the search
problem in this case. We present a simple algorithm with a competitive ratio of at
most
q
p
8 (~ 2:61); in rectilinear streets it achieves the optimal competitive ratio
of
2.
This work was supported in part by the Sloan Fellowship and NSF PYI award of Eva Tardos. Author
is supported by an ONR Graduate Fellowship
Laboratory for Computer Science, MIT, Cambridge MA 02139 USA.
+PAGE+

Submitted to the special Algorithmica issue of Algorithmic Foundations of Robotics
Visibility-Based Planning of Sensor Control Strategies
Amy J. Briggs Bruce R. Donald
Department of Mathematics Department of Computer Science
and Computer Science   Upson Hall
Middlebury College Cornell University
Middlebury, VT 05753, USA Ithaca, NY 14853, USA
briggs@middlebury.edu brd@cs.cornell.edu
Keywords: Visibility-based planning, error detection and recovery, sensor con
figuration, camera control, surveillance
Abstract
We consider the problem of planning sensor control strategies that enable a
sensor to be automatically configured for robot tasks. In this paper we present
robust and efficient algorithms for computing the regions from which a sensor
has unobstructed or partially obstructed views of a target in a goal. We apply
these algorithms to the Error Detection and Recovery problem of recognizing
whether a goal or failure region has been achieved. Based on these methods and
strategies for visually-cued camera control, we have built a robot surveillance
system in which one mobile robot navigates to a viewing position from which
it has an unobstructed view of a goal region, and then uses visual recognition
to detect when a specific target has entered the region.
Support for this work was provided in part by the National Science Foundation under grants
No. IRI-8802390, IRI-9000532, IRI-9201699, and by a Presidential Young Investigator award to
Bruce Donald, and in part by the Air Force Office of Sponsored Research, the Mathematical Sciences Institute, Intel Corporation, and AT&T Bell laboratories. The first author was additionally
supported by an AT&T Bell Laboratories Graduate Fellowship sponsored by the AT&T Foundation.
+PAGE+

Experimental Study of Minimum Cut Algorithms
Chandra S. Chekuri
Computer Science Department
Stanford University
Stanford, CA 94305
chekuri@theory.stanford.edu
Andrew V. Goldberg
NEC Research Institute
Princeton, NJ 08540
avg@research.nj.nec.com
David R. Karger
Laboratory for Computer Science
MIT
Cambridge, MA 02139
karger@theory.lcs.mit.edu
Matthew S. Levine
Laboratory for Computer Science
MIT
Cambridge, MA 02139
mslevine@theory.lcs.mit.edu
Cliff Stein x
Department of Computer Science
Dartmouth College
Hanover, NH, 03755
cliff@cs.dartmouth.edu.
October 1996
Abstract
Recently, several new algorithms have been developed for the minimum cut problem.
These algorithms are very different from the earlier ones and from each other and substantially improve worst-case time bounds for the problem. We conduct experimental evaluation
the relative performance of these algorithms. In the process, we develop heuristics and data
structures that substantially improve practical performance of the algorithms. We also develop problem families for testing minimum cut algorithms. Our work leads to a better
understanding of practical performance of the minimum cut algorithms and produces very
efficient codes for the problem.
Supported by NSF Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foun
dation, Shell Foundation, and Xerox Corporation.
Research partly supported by ARPA contract N00014-95-1-1246.
Research partly supported by a grant from the World Wide Web Consortium. Some of this work was done
while visiting the second author at NEC.
x Research partly supported by NSF Award CCR-9308701, a Walter Burke Research Initiation Award and a
Dartmouth College Research Initiation Award. Some of this work was done while visiting the second author at
NEC, and while visiting Stanford University.
+PAGE+

Dartmouth College Computer Science Technical Report
PCSTR96-294
(Revised September 1996)
Performing Out-of-Core FFTs on Parallel Disk Systems
Thomas H. Cormen
David M. Nicol
Dartmouth College
Department of Computer Science
Abstract
The Fast Fourier Transform (FFT) plays a key role in many areas of computational science
and engineering. Although most one-dimensional FFT problems can be entirely solved entirely in
main memory, some important classes of applications require out-of-core techniques. For these,
use of parallel I/O systems can improve performance considerably. This paper shows how to
perform one-dimensional FFTs using a parallel disk system with independent disk accesses. We
present both analytical and experimental results for performing out-of-core FFTs in two ways:
using traditional virtual memory with demand paging, and using a provably asymptotically
optimal algorithm for the Parallel Disk Model (PDM) of Vitter and Shriver. When run on a
DEC 2100 server with a large memory and eight parallel disks, the optimal algorithm for the
PDM runs up to 144.7 times faster than in-core methods under demand paging. Moreover, even
including I/O costs, the normalized times for the optimal PDM algorithm are competitive, or
better than, those for in-core methods even when they run entirely in memory.
1 Introduction

Agent Tcl: Alpha Release 1.1
Robert S. Gray
Department of Computer Science
Dartmouth College
Hanover, NH 03755
E-mail: robert.s.gray@dartmouth.edu
December 1, 1995
Abstract
Agent Tcl is a transportable agent system. The agents are written in an extended version of the
Tool Command Lanuage (Tcl). Each agent can suspend its execution at an arbitrary point, transport
to another machine and resume execution on the new machine. This migration is accomplished with
the agent jump command. agent jump captures the current state of the Tcl script and transfers this
state to the destination machine. The state is restored on the new machine and the Tcl script continues
its execution from the command immediately after the agent jump. In addition to migration, agents
can send messages to each other and can establish direct connections. A direct connection is more
efficient than message passing for bulk data transfer. Finally, agents can use the Tk toolkit to create
graphical user interfaces on their current machine. Agent Tcl is implemented as two components. The
first component is an extended Tcl interpreter. The second component is a server which runs on each
machine. The server accepts incoming agents, messages and connection requests and keeps track of the
agents that are running on its machine. An alpha release of Agent Tcl is available for public use. This
documentation describes how to obtain and compile the source code, how to run the server and how to
write transportable agents.
Supported by AFOSR contract F49620-93-1-0266 and ONR contract N00014-95-1-1204
+PAGE+

DIMACS Series in Discrete Mathematics
and Theoretical Computer Science
Volume 00, 19xx
Some applications of generalized FFTs
Daniel N. Rockmore
Abstract. Generalized FFTs are efficient algorithms for computing a Fourier
transform of a function defined on finite group, or a bandlimited function defined on a compact group. The development of such algorithms has been accompanied and motivated by a growing number of both potential and realized
applications. This paper will attempt to survey some of these applications.
Appendices include some more detailed examples.
1. A brief history

A Hyperconcentrator Switch
for Routing Bit-Serial Messages
Thomas H. Cormen
Charles E. Leiserson
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, Massachusetts 02139
Supported in part by the Defense Advanced Research Projects Agency under Contracts N00014-80-C-0622 and N00014-87-K-0825 and by a National Science Foundation
Fellowship.
Supported in part by the Defense Advanced Research Projects Agency under Contracts
N00014-80-C-0622 and N00014-87-K-0825 and by an NSF Presidential Young Investigator Award with matching funds provided by AT&T Bell Laboratories, IBM Corporation,
and Xerox Corporation.
+PAGE+

Document Image Compression Via Pattern
Matching
A Thesis
Submitted to the Faculty
in partial fulfillment of the requirements for the
degree of
Doctor of Philosophy
in
Computer Science
by
Qin Zhang
DARTMOUTH COLLEGE
Hanover, New Hampshire
9 September 1997
Examining Committee:
(chairman) John Danskin
Henry Baird
Dennis Healy
Geoff Davis
Neal Young
Roger D. Sloboda
Dean of Graduate Studies
+PAGE+

Probabilistic Analysis for Combinatorial
Functions of Moving Points
Li Zhang Harish Devarajan Julien Basch Piotr Indyk
Computer Science Department
Stanford University
Stanford, CA94305
flizhang,harish,jbasch,indykg@cs.stanford.edu
September 9, 1997
Abstract
Given a set of n points, what is the description complexity of their convex hull? In our world, this question
is understood with an implicit "in the worst case", and the answer is n bd=2c where d is the dimension of
the underlying space. This is not entirely satisfactory, as this description complexity can vary tremendously
depending on the positions of the points. Another approach is to look at the expected description complexity
when the points are drawn from a given distribution. This type of analysis, initiated by Renyi and Sulanke
[RS63] and pursued by others gets its value from the fact that this expectation is in general much smaller
than in the worst case, and, more importantly, in that it often allows one to design algorithms that have
expected running times against which worst case aware algorithms cannot compete. For instance, the convex
hull of n points drawn independently uniformly at random from a d-dimensional hypercube has expected
complexity O(log d1 n), and can be computed in expected linear time.
In parallel, in the past decade, a number of papers have considered a setting where points are allowed
to move along low degree algebraic trajectories. Different questions have been asked in this context. In
particular, Atallah [Ata85], studied the number of times the combinatorial description of the convex hull
or closest pair can change, in the worst case ("dynamic computational geometry"). More recently, Basch,
Guibas, and Hershberger [BGH97] have designed kinetic data structures to maintain these attributes in an
online setting, measuring the quality of a kinetic data structure by the ratio of the worst case number of
changes to the configuration of interest, to the worst case number of changes to the data structure itself, for
low degree algebraic motions. However, an experimental study undertaken in [BGSZ97] to assess the quality
of these data structures in practice shows that the worst case analysis can hide vastly different results in
terms of expectation when the point positions and speeds are drawn at random from some distributions. It
is this study that motivated the present paper.
In this communication, we report several results on the expected number of changes to various combinatorial structures for the case when points are drawn from the uniform distribution on the unit square. These
results can be generalized for any dimension d &gt; 2.
This paper appeared in the proceedings of the 13th Symposium of Computational Geometry (1997) as a communication
Supported in part by National Science Foundation grant CCR-9623851 and by US Army MURI grant DAAH04-96-1-0007.
Supported in part by the Okawa Foundation.
x Supported in part by ARO MURI Grant DAAH04-96-1-0007 and by NSF Award CCR-9357849, with matching funds
from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation.
+PAGE+

Geometric Matching under Noise: Combinatorial Bounds and
Algorithms
Piotr Indyk Rajeev Motwani Suresh Venkatasubramanian
Department of Computer Science
Stanford University
Stanford, CA 94305
findyk,rajeev,sureshg@cs.stanford.edu
Abstract
The geometric point set matching problem in 2 and 3 dimensions is a well-studied problem with application to areas such as computer vision and pattern recognition, computational chemistry and other fields
such as cartography and computer animation. The basic problems can be formulated as follows. Given
some choice of a space of transformations and a similarity measure d(P; Q) for two point sets P and Q in
Euclidean space.
Problem 1 (Pattern Matching (PM)) Given point sets P and Q, where jP j = k and jQj = n with
k n, and an * &gt; 0, find a transformation T for which d(T (P ); Q) * or return none if no such T exists.
Problem 2 (Largest Common Point-set (LCP)) Given point sets P and Q, where jP j = m and jQj =
n, K &gt; 0 and * &gt; 0, find a transformation T and a set P 0 P of size jP 0 j K such that jd(T (P 0 ); Q)j *
or return none if no such T and P 0 exist.
We restrict T to the space of rigid Euclidean translations and rotations. The similarity measures of interest
to us are: the exact metric 1 d E (P; Q) (binary-valued, requiring each point in P to be mapped to a point in
Q); the Hausdorff metric d H (P; Q) (maximum over all points in P of the distance to the nearest point in
Q); and, the matching metric d M (bottleneck matching distance).
A detailed study of this set of problems was initiated by Alt, Mehlhorn, Wagener, and Welzl [AMWW88].
In this seminal work, they propose a suite of algorithms for the exact and matching metrics. More recent work
led to improved bounds for pattern matching and LCP under the exact metric. The problem of estimating
the minimum Hausdorff distance between two point sets in 2 and 3 dimensions has been studied extensively.
Clearly, any real application of such algorithms has to deal with the presence of noise in data, requiring
the algorithms to perform matching within the limits of some reasonable noise model. In fact, our interest
in these problems arose during an attempt to implement such algorithms for applications in computer vision
and in computational biology and chemistry, particularly rational drug design [FKL + 97]. We discovered that
noise present in real data rendered most known algorithms inoperative. In hindsight, this is not surprising
for, as noted in the survey by Alt and Guibas [AG96], these algorithms are likely to be "difficult to implement
and numerically unstable due to the necessary computation of intersections of complex algebraic surfaces."
Worse still, they have unacceptably high running times: for example, even in R 2 , LCP under the matching
metric requires O(n 8 ) time, although under additional restrictions on noise regions the running times can be
This research is supported in part by a grant from Pfizer Central Research.
Supported by NSF Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell
Foundation, and Xerox Corporation.
Supported by an Alfred P. Sloan Research Fellowship, an IBM Faculty Partnership Award, an ARO MURI Grant DAAH04-96-1-0007, and NSF Young Investigator Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foun
dation, Shell Foundation, and Xerox Corporation.
1 We use the term "metric" only for convenience; formally, none of the above measures satisfy all the properties of a metric.

Lexicographic Bit Allocation for MPEG Video
Dzung T. Hoang +L +
Sony Semiconductor of America
3300 Zanker Road, MS SJ3C3
San Jose, CA 95134
dth@ricochet.net
Elliot L. Linzer
C-Cube Microsystems
One Water Street, 2nd Floor
White Plains, NY 10601
elliot.linzer@c-cube.com
Jeffrey Scott Vitter
Duke University
Box 90129
Durham, NC 27708-0129
jsv@cs.duke.edu
+PAGE+

Scalable Mining for Classification Rules
in Relational Databases
Min Wang Bala Iyer Jeffrey Scott Vitter
Dept. of Computer Science Database Technology Institute Dept. of Computer Science
Duke University IBM Santa Teresa Lab Duke University
Durham, NC 27708 San Jose, CA 95161 Durham, NC 27708
Abstract
Classification is a key function of many "business
intelligence" toolkits and a fundamental building block
in data mining. Immense data may be needed to train
a classifier for good accuracy. The state-of-art classifiers [21, 25] need an in-memory data structure of
size O(N ), where N is the size of the training data,
to achieve efficiency. For large data sets, such a data
structure will not fit in the internal memory. The best
previously known classifier does a quadratic number of
I/Os for large N .
In this paper, we propose a novel classification algorithm (classifier) called MIND (MINing in
Databases). MIND can be phrased in such a way that
its implementation is very easy using the extended relational calculus SQL, and this in turn allows the classifier to be built into a relational database system directly. MIND is truly scalable with respect to I/O efficiency, which is important since scalability is a key
requirement for any data mining algorithm.
We built a prototype of MIND in the relational
database manager DB2 and benchmarked its performance. We describe the working prototype and report
the measured performance with respect to the previous
method of choice. MIND scales not only with the size
of the datasets but also with the number of processors
on an IBM SP2 computer system. Even on uniproces-sors, MIND scales well beyond the dataset sizes previously published for classifiers. We also give some
insights that may have an impact on the evolution of
the extended relational calculus SQL.
1 Introduction

A Unified Analysis of Value-Function-Based
Reinforcement-Learning Algorithms
Csaba Szepesvari
Research Group on Artificial Intelligence
"Jozsef Attila" University
Szeged 6720, Aradi vrt tere 1.
Hungary
szepes@sol.cc.u-szeged.hu
Michael L. Littman
Department of Computer Science
Duke University
Durham, NC 27708-0129
mlittman@cs.duke.edu
December 3, 1997
Abstract
Reinforcement learning is the problem of generating optimal behavior in a sequential decision-making environment given the opportunity of
interacting with it. Many algorithms for solving reinforcement-learning
problems work by computing improved estimates of the optimal value
function. We extend prior analyses of reinforcement-learning algorithms
and present a powerful new theorem that can provide a unified analysis of
value-function-based reinforcement-learning algorithms. The usefulness
of the theorem lies in how it allows the asynchronous convergence of a
complex reinforcement-learning algorithm to be proven by verifying that
a simpler synchronous algorithm converges. We illustrate the application
of the theorem by analyzing the convergence of Q-learning, model-based
reinforcement learning, Q-learning with multi-state updates, Q-learning
for Markov games, and risk-sensitive reinforcement learning.
1 Introduction

M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 279   (Replaces TR-228)
Appears in the IEEE Transactions on Image Processing Special Issue: Image Sequence Compression,
vol 3, no. 5, p. 625-638, September 1994. Revised: May, 1994
Representing Moving Images with Layers
John Y. A. Wang AND Edward H. Adelson
Abstract
We describe a system for representing moving
images with sets of overlapping layers. Each
layer contains an intensity map that defines the
additive values of each pixel, along with an alpha
map that serves as a mask indicating the transparency. The layers are ordered in depth and
they occlude each other in accord with the rules
of compositing. Velocity maps define how the
layers are to be warped over time. The layered
representation is more flexible than standard image transforms and can capture many important
properties of natural image sequences. We describe some methods for decomposing image sequences into layers using motion analysis, and we
discuss how the representation may be used for
image coding and other applications.
Keywords| Image coding, motion analysis, image
segmentation, image representation, robust estimation.
1 Introduction

An Extensible End-to-End Protocol and Framework
K. L. Calvert
R. H. Kravets
R. D. Krupczak
College of Computing
Georgia Institute of Technology
Atlanta, Georgia, USA
fcalvert,robink,rdkg@cc.gatech.edu
Abstract
We describe a framework for composing end-to-end protocol functions. The framework comprises:
a generic model of protocol processing; a metaheader protocol supporting per-packet configuration
of protocol function and efficient demultiplexing of incoming data units; and an extensible set of
modular protocol functions. This paper describes the pieces of the framework and motivates some
of the design decisions.
1 Introduction and Motivations

A Protocol for Efficient Transfer of Data over Fiber/Cable Systems
Dolors Sala John O. Limb
School of Electrical College of Computing
and Computer Engineering
Georgia Institute of Technology
Atlanta, GA 30332-0280
E-mail: (dolors,limb)@cc.gatech.edu
Abstract
A revolution is occurring in the scope and range
of information, communication and education services
that will be made available to schools, libraries, town-halls, clinics and, most importantly, residences. These
services will be provided initially, primarily over hybrid fiber-cable systems, either by telephone companies
or cable companies. The old cable plant is being upgraded and used in totally new ways.
The topology and physical characteristics of the upstream channel present new challenges for efficient
channel access. We present a media access protocol
that efficiently transfers data on this channel. A primary goal in the design was to keep the portion of the
protocol resident in the station as simple as possible.
Thus we use centralized control located in the cable
head-end and minimize intelligence in the station. We
refer to this protocol as Centralized Priority Reservation or CPR. A station wishing to transmit sends a request to the head-end using a contention channel. The
head-end acknowledges the request and then schedules
the request, informing the station by means of a grant
message when to transmit.
The protocol performs well under heavy load. Performance is affected little by the number of stations,
the speed of the system and the physical length of the
system.
1 Introduction

PARALLEL AND DISTRIBUTED SIMULATION
Richard M. Fujimoto
College of Computing
Georgia Institute of Technology
Atlanta, Georgia 30332-0280, U.S.A.
ABSTRACT
Research and development efforts in the parallel and
distributed simulation field over the last 15 years
has progressed, largely independently, in two separate camps: the largely academic high performance
Parallel And Distributed (discrete event) Simulation (PADS) community, and the DoD-centered Distributed Interactive Simulation (DIS) community.
This tutorial gives an overview and comparison of
work in these two areas, emphasizing issues related to
distributed execution where these fields have the most
overlap. Differences in the fundamental assumptions
routinely used within each community are contrasted,
followed by overviews of work in each community.
1 INTRODUCTION

Storage Systems for Movies-on-Demand Video Servers
Ann L. Chervenak
April 7, 1995
1 Abstract
In this paper, we evaluate storage system alternatives for movies-on-demand video servers. We begin by
characterizing the movies-on-demand workload. Then we study disk farms in which one movie is stored per
disk. This is a simple scheme, but it wastes substantial disk bandwidth, since disks holding less popular
movies are under-utilized; also, good performance requires that movies be replicated to reflect the user
request pattern. Next, we examine disk farms in which movies are striped across disks, and find that
striped video servers offer close to full utilization of the disks by achieving better load balancing. Finally, we
evaluate the use of storage hierarchies for video service that include a tertiary library along with a disk farm.
Unfortunately, we show that the performance of neither magnetic tape libraries nor optical disk jukeboxes
as part of a storage hierarchy is adequate to service the predicted distribution of movie accesses. We suggest
changes to tertiary libraries that would make them better-suited to these applications.
2 Introduction

Evaluating Blocking Probability in Generalized
Connectors
Ellen Witte Zegura
Abstract| Generalized connectors provide the capability to connect a single input to one or more outputs. Such networks play an important role in supporting any application that involves the distribution of information from one source to many destinations or many sources to many destinations. We present the first analytic model for evaluating blocking probability in generalized connectors. The model allows flexibility in specifying traffic fanout characteristics and network routing algorithms. Equations are derived for computing blocking probability for the important class of series-parallel networks. We investigate the accuracy of the equations by comparing the blocking probability computed using the equations to results from simulation.
1 Introduction

Reverse Engineering with a CASE Tool
Bret Johnson
Research advisors: Spencer Rugaber and Rich LeBlanc
October 6, 1994
Abstract
We examine using a CASE tool, Interactive Development Environment's Software through Pictures (StP), to support reverse engineering. We generate structure charts in StP from the automated analysis
of C source code. The advantages of this approach are that one can use
the CASE tool's support for drawing, linking, and modifying pictorial
notations for program design in order to make it easier to construct a
reverse engineering tool. Additionally, one can then use the design rep
resentations with the CASE tool to do reengineering for maintenance.
1 Introduction

Department of Computer Science
Series of Publications C
Report C-1997-15
Discovery of frequent episodes in event sequences
Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo
University of Helsinki
Finland
+PAGE+

Finding Interesting Rules from Large Sets of Discovered Association Rules
Mika Klemettinen Heikki Mannila Pirjo Ronkainen Hannu Toivonen A. Inkeri Verkamo
Department of Computer Science
University of Helsinki
P.O. Box 26, FIN-00014 University of Helsinki, Finland
mannila@cs.helsinki.fi
Abstract
Association rules, introduced by Agrawal, Imielinski, and
Swami, are rules of the form "for 90 % of the rows of the
relation, if the row has value 1 in the columns in set W ,
then it has 1 also in column B". Efficient methods exist for
discovering association rules from large collections of data.
The number of discovered rules can, however, be so large
that browsing the rule set and finding interesting rules from
it can be quite difficult for the user. We show how a simple
formalism of rule templates makes it possible to easily describe the structure of interesting rules. We also give examples of visualization of rules, and show how a visualization
tool interfaces with rule templates.
1 Introduction

A structured document database system
Pekka Kilpelainen Greger Linden Heikki Mannila
Erja Nikunen
University of Helsinki
Abstract
We describe a database system for writing, editing, and querying structured documents. The structure of text is described using a context-free
grammar. The operations are implemented using a powerful query language. The system supports the use of user-defined multiple views of the
documents: one view can contain all the structure explicitly, while another
can contain only part of the document and have only part of the structure visible. This makes the system flexible for different editing tasks.
The system is implemented in C using a relational database system.
1 Introduction

Lempel-Ziv Index for q-Grams
Juha Karkkainen and Erkki Sutinen
Department of Computer Science,   P.O. Box 26 (Teollisuuskatu 23),
FIN-00014   University of Helsinki,   Finland
fJuha.Karkkainen,Erkki.Sutineng@cs.Helsinki.FI
Abstract. We present a new sublinear-size index structure for q-grams.
A q-gram index of the text is used in many approximate pattern matching
algorithms. All earlier q-gram indexes have at least linear size. The new
method takes advantage of repetitions in the text found by Lempel-Ziv
parsing.
1 Introduction

??, ??, 1-10 (??)
c ?? Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
Unifying Two-View and Three-View Geometry
SHAI AVIDAN, AMNON SHASHUA
favidan,shashuag@cs.huji.ac.il
Institute of Computer Science, The Hebrew University,   Jerusalem 91904, Israel
Received ??. Revised ??.
Abstract. The core of multiple-view geometry is governed by the fundamental matrix and the trilinear
tensor. In this paper we unify both representations by first re-deriving the fundamental matrix as a rank
deficient tensor, and secondly by deriving a unified set of operators that are transparent to the number of
views. As a result, we show that the basic building block of the geometry of multiple views is the trilinear
tensor of three views and that this tensor specializes to the fundamental matrix (in it's tensor form) in
the case of two views. The properties of the tensor (geometric interpretation, contraction properties,
etc.) are independent of the number of views (two or three). As a byproduct, every two-view algorithm
can be considered as a degenerate three-view algorithm and three-view algorithms can work with either
two or three images, all using one standard set of tensor operations. To highlight the usefulness of this
paradigm we provide two practical applications. First we present a novel view synthesis algorithm that
starts with the fundamental matrix (in its tensor form) and seamlessly move to the general trilinear
tensor, all using one set of tensor operations. The second application is a camera stabilization algorithm,
originally introduced for three views, now working with two views without modification.
1. Introduction

Using Queue Time Predictions for Processor Allocation
Allen B. Downey
University of California at Berkeley
San Diego Supercomputer Center
Abstract
When a moldable job is submitted to a space-sharing
parallel computer, it must choose whether to begin execution on a small, available cluster or wait in queue for
more processors to become available. To make this decision, it must predict how long it will have to wait for
the larger cluster. We propose statistical techniques for
predicting these queue times, and develop an allocation
strategy that uses these predictions. We present a workload model based on observed workloads at the San Diego
Supercomputer Center and the Cornell Theory Center,
and use this model to drive simulations of various allocation strategies. We find that prediction-based allocation
not only improves the turnaround time of individual jobs;
it also improves the utilization of the system as a whole.
1 Introduction

Measure, Stochasticity, and the Density
of Hard Languages
TR 92-13
Jack H. Lutz and Elvira Mayordomo
May 1992
Iowa State University of Science and Technology
Department of Computer Science
226 Atanasoff
Ames, IA 50011
+PAGE+

An Executable Semantics for a
Formalized Data Flow Diagram
Specification Language
TR93-27
Tim Wahls, Albert L. Baker, and Gary T. Leavens
November 15, 1993
Iowa State University of Science and Technology
Department of Computer Science
226 Atanasoff
Ames, IA 50011
+PAGE+

Coordination and Control Structures and Processes:
Possibilities for Connectionist Networks (CN)
Vasant Honavar & Leonard Uhr
Computer Sciences Department
University of Wisconsin-Madison
Abstract
The absence of powerful control structures and processes that synchronize, coordinate, switch between, choose among, regulate, direct, modulate interactions between, and
combine distinct yet interdependent modules of large connectionist networks (CN) is
probably one of the most important reasons why such networks have not yet succeeded at
handling difficult tasks (e.g. complex object recognition and description, complex
problem-solving, planning).
In this paper we examine how CN built from large numbers of relatively simple
neuron-like units can be given the ability to handle problems that in typical multi-computer networks and artificial intelligence programs along with all other types of
programs are always handled using extremely elaborate and precisely worked out central control (coordination, synchronization, switching, etc.). We point out the several
mechanisms for central control of this un-brain-like sort that CN already have built into
them albeit in hidden, often overlooked, ways.
We examine the kinds of control mechanisms found in computers, programs, fetal
development, cellular function and the immune system, evolution, social organizations,
and especially brains, that might be of use in CN. Particularly intriguing suggestions are
found in the pacemakers, oscillators, and other local sources of the brain's complex partial synchronies; the diffuse, global effects of slow electrical waves and neurohormones;
the developmental program that guides fetal development; communication and coordination within and among living cells; the working of the immune system; the evolutionary
processes that operate on large populations of organisms; and the great variety of partially competing partially cooperating controls found in small groups, organizations, and
larger societies. All these systems are rich in control but typically control that emerges
from complex interactions of many local and diffuse sources. We explore how several
different kinds of plausible control mechanisms might be incorporated into CN, and
assess their potential benefits with respect to their cost.
Introduction

1 FEATURE SUBSET SELECTION USING A
GENETIC ALGORITHM
Jihoon Yang and Vasant Honavar
Artificial Intelligence Research Group
Department of Computer Science
226 Atanasoff Hall
Iowa State University
Ames, IA 50011
U.S.A.
yangjhonavar-@cs.iastate.edu
Abstract: Practical pattern classification and knowledge discovery problems require selection of a
subset of attributes or features (from a much larger set) to represent the patterns to be classified.
This is due to the fact that the performance of the classifier (usually induced by some learning
algorithm) and the cost of classification are sensitive to the choice of the features used to construct
the classifier. Exhaustive evaluation of possible feature subsets is usually infeasible in practice because
of the large amount of computational effort required. Genetic algorithms, which belong to a class of
randomized heuristic search techniques, offer an attractive approach to find near-optimal solutions
to such optimization problems. This paper presents an approach to feature subset selection using a
genetic algorithm. Some advantages of this approach include the ability to accommodate multiple
criteria such as accuracy and cost of classification into the feature selection process and to find feature
subsets that perform well for particular choices of the inductive learning algorithm used to construct
the pattern classifier. Our experiments with several benchmark real-world pattern classification
problems demonstrate the feasibility of this approach to feature subset selection in the automated
design of neural networks for pattern classification and knowledge discovery.
1.1 INTRODUCTION

EFFICIENT COMPILATION AND PROFILE-DRIVEN
DYNAMIC RECOMPILATION IN SCHEME
Robert G. Burger
Submitted to the faculty of the University Graduate School
in partial fulfillment of the requirements
for the degree
Doctor of Philosophy
in the   Department of Computer Science,
Indiana University
March 1997
+PAGE+

Compiler Construction Using Scheme
Erik Hilsdale J. Michael Ashley
R. Kent Dybvig Daniel P. Friedman
Indiana University Computer Science Department
Lindley Hall 215
Bloomington, Indiana 47405
fehilsdal,jashley,dyb,dfried g@cs.indiana.edu
Abstract
This paper describes a course in compiler design that focuses on the
Scheme implementation of a Scheme compiler that generates native assembly code for a real architecture. The course is suitable for advanced
undergraduate and beginning graduate students. It is intended both to
provide a general knowledge about compiler design and implementation
and to serve as a springboard to more advanced courses. Although this
paper concentrates on the implementation of a compiler, an outline for an
advanced topics course that builds upon the compiler is also presented.
1 Introduction

Using Goals and Experience to Guide Abduction
David B. Leake
leake@cs.indiana.edu
Technical Report #359
Department of Computer Science, Indiana University
Lindley Hall 215, Bloomington, IN 47405
Abstract
Standard methods for abductive understanding are neutral to prior experience and current goals.
Candidate explanations are built from scratch by backwards chaining, without considering how
similar situations were previously explained, and selection of the candidate to accept is based on its
likelihood, without considering the information needs beyond routine understanding. Problems arise
when applying these methods to everyday understanding: The vast range of possible explanations
makes it difficult to control the cost of explanation construction and to assure that the explanations
generated will actually be useful.
We argue that these problems can be overcome by using goals and experience to guide both
explanation generation and evaluation. Our work is within the framework of case-based explanation, which builds explanations by retrieving and adapting prior explanations stored in memory.
We substantiate our model by describing mechanisms that enable it to effectively generate good
explanations. First, we demonstrate that there exists a theory of anomaly and explanation that can
guide retrieval of relevant explanations. Second, we present a plausibility evaluation process that
efficiently detects conflicts and confirmations of an explanation's assumptions by prior patterns,
making it possible to focus explanation adaptation when retrieved explanations are implausible.
Third, we present methods for judging whether explanations provide the information needed to satisfy explainer goals beyond routine understanding. By reflecting experience and goals in the search
for explanations, case-based explanation provides a practical mechanism for guiding search towards
explanations that are both plausible and useful.
1 The work described here was supported in part by the Defense Advanced Research Projects Agency,

Toward the Rigorous Use of
Diagrams in Reasoning about
Hardware
Steven D. Johnson   sjohnson@indiana.edu   Department of Computer Science Indiana University
Jon Barwise
barwise@indiana.edu
Departments of Philosophy, Mathematics, and Computer Science
Indiana University
Gerard T. Allwein
Visual Inference Laboratory
Indiana University
Indiana University Logic Group
Preprint No. IULG-93-23
May 1993
+PAGE+

Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, Morgan Kaufmann, San Francisco, 1997.
Learning to Integrate Multiple Knowledge Sources
for Case-Based Reasoning
David B. Leake, Andrew Kinley, and David Wilson
Computer Science Department
Lindley Hall 215,   Indiana University
Bloomington, IN 47405, U.S.A.
fleake, akinley, davwilsg@cs.indiana.edu
Abstract
The case-based reasoning process depends on
multiple overlapping knowledge sources, each
of which provides an opportunity for learning. Exploiting these opportunities requires
not only determining the learning mechanisms
to use for each individual knowledge source,
but also how the different learning mechanisms interact and their combined utility. This
paper presents a case study examining the
relative contributions and costs involved in
learning processes for three different knowledge sources|cases, case adaptation knowledge, and similarity information|in a case-based planner. It demonstrates the importance
of interactions between different learning processes and identifies a promising method for integrating multiple learning methods to improve
case-based reasoning.
1 Introduction

A Ray Tracing Method for Illumination Calculation in
Diffuse-Specular Scenes
Peter Shirley
Department of Computer Science
University of Illinois
1304 West Springfield Avenue
Urbana, Illinois 61801
USA
Abstract
Several ways of improving the realism of the results
of traditional ray tracing are presented. The essential physical quantities of spectral radiant power and
spectral radiance and their use in lighting calculations
are discussed. Global illumination terms are derived
by employing illumination ray tracing for calculation of
quickly changing indirect lighting components, and ra-diosity ray tracing for slowly changing indirect lighting
components. Direct lighting is calculated during the
viewing phase allowing the use of bump maps. Finally,
a method is introduced that reduces the total number
of shadow rays to no more than the total number of
viewing rays for a given picture.
Keywords: Bump Mapping, Illumination, Radiosity,
Radiance, Ray Tracing, Realism, Stratified Sampling,
Texture Mapping.
1 Introduction

3-D Stereo Using Photometric Ratios
Lawrence B. Wolff
Elli Angelopoulou
Computer Vision Laboratory
Department of Computer Science
The Johns Hopkins University
Baltimore, MD 21218
ABSTRACT
We present a novel robust methodology for corresponding a dense set of points on an
object surface from photometric values, for 3-D stereo computation of depth. The methodology utilizes multiple stereo pairs of images, each stereo pair taken of exactly the same
scene but under different illumination. With just 2 stereo pairs of images taken respectively for 2 different illumination conditions, a stereo pair of ratio images can be produced; one for the ratio of left images, and one for the ratio of right images. We
demonstrate how the photometric ratios composing these images can be used for accurate
correspondence of object points. Object points having the same photometric ratio with
respect to 2 different illumination conditions comprise a well-defined equivalence class of
physical constraints defined by local surface orientation relative to illumination conditions. We formally show that for diffuse reection the photometric ratio is invariant to
varying camera characteristics, surface albedo, and viewpoint and that therefore the same
photometric ratio in both images of a stereo pair implies the same equivalence class of
physical constraints. Corresponding photometric ratios along epipolar lines in a stereo pair
of images under different illumination conditions is therefore a robust correspondence of
equivalent physical constraints, and determination of depth from stereo can be performed
without explicitly knowing what these physical constraints being corresponded actually
are. This implies a very practical shape-from-stereo methodology applicable to perspective views and not requiring any knowledge whatsoever of illumination conditions. This is
particularly practical for determination of 3-D shape on smooth featureless surfaces which
has previously been hard to perform using stereo. We demonstrate experimental 3-D shape
determination from a dense set of points using our stereo technique on smooth objects of
known ground truth shape that are accurate to well within 1% depth accuracy.
+PAGE+

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 17, NO. 6, JUNE 1995 599
Best-Case Results for Nearest Neighbor
Learning
Steven Salzberg, Arthur Delcher, David Heath, and Simon Kasif
Abstract| In this paper we propose a theoretical model
for analysis of classification methods, in which the teacher
knows the classification algorithm and chooses examples in
the best way possible. We apply this model using the nearest-neighbor learning algorithm, and develop upper and lower
bounds on sample complexity for several different concept
classes. For some concept classes, the sample complexity
turns out to be exponential even using this best-case model,
which implies that the concept class is inherently difficult
for the nearest-neighbor algorithm. We identify several geometric properties that make learning certain concepts relatively easy. Finally we discuss the relation of our work
to helpful teacher models, its application to decision-tree
learning algorithms, and some of its implications for current experimental work.
Keywords| machine learning, nearest-neighbor, geometric
concepts.
I. Introduction

Deterministic Sorting
in Nearly Logarithmic Time
on the Hypercube
and Related Computers
Robert Cypher
IBM Almaden Research Center
650 Harry Rd.
San Jose, CA 95120
C. Greg Plaxton
MIT Laboratory for Computer Science
545 Technology Square
Cambridge, MA 02139
November 29, 1995
Abstract
This paper presents a deterministic sorting algorithm, called Sharesort, that sorts n
records on an n-processor hypercube, shu*e-exchange, or cube-connected cycles in
O(log n (log log n) 2 ) time in the worst case. The algorithm requires only a constant
amount of storage at each processor. The fastest previous deterministic algorithm for
this problem was Batcher's bitonic sort, which runs in O(log 2 n) time.
Supported by an NSERC postdoctoral fellowship, and DARPA contracts N00014-87-K-825 and N00014
89-J-1988.
+PAGE+

Appears in Machine Learning: Proceedings of the Tenth International Conference,
P. E. Utgoff (editor), Morgan Kaufmann, San Mateo, CA, 1993
Learning Symbolic Rules Using Artificial Neural Networks
Mark W. Craven and Jude W. Shavlik
Computer Sciences Department
University of Wisconsin
1210 West Dayton St.
Madison, WI 53706
email: fcraven, shavlikg@cs.wisc.edu
Abstract
A distinct advantage of symbolic learning
algorithms over artificial neural networks is
that typically the concept representations
they form are more easily understood by humans. One approach to understanding the
representations formed by neural networks is
to extract symbolic rules from trained networks. In this paper we describe and investigate an approach for extracting rules from
networks that uses (1) the NofM extraction algorithm, and (2) the network training
method of soft weight-sharing. Previously,
the NofM algorithm had been successfully
applied only to knowledge-based neural networks. Our experiments demonstrate that
our extracted rules generalize better than
rules learned using the C4.5 system. In addition to being accurate, our extracted rules
are also reasonably comprehensible.
1 INTRODUCTION

The Royal Tree Problem, a Benchmark for Single and
Multi-population Genetic Programming
appears in "Advances in Genetic Programming II", MIT Press, Pete Angeline and
Kim Kinnear, editors
Bill Punch, Doug Zongker, and Erik Goodman
We report on work done to develop a benchmark problem for genetic programming, both
as a difficult problem to test GP abilities and as a platform for tuning GP parameters.
This benchmark, the royal tree, is a function that accounts for tree shape as part of its
evaluation function, thus it controls for a parameter not often found in the GP literature.
It also is a progressive function, allowing the user to set the difficulty of the problem
attempted. We not only describe the function, but also report on results of using island
parallelism for solving GP problems. The results obtained are somewhat surprising, as it
appears that a single large population outperforms a group of smaller populations under
all the conditions tested.
15.1 Introduction

A Perspective on Word Sense Disambiguation Methods
and Their Evaluation
Philip Resnik
Dept. of Linguistics/UMIACS
University of Maryland
College Park, MD 20742
resnik@umiacs.umd.edu
David Yarowsky
Dept. of Computer Science/CLSP
Johns Hopkins University
Baltimore, MD 21218
yarowsky@cs.jhu.edu
Abstract
In this position paper, we make several
observations about the state of the art in
automatic word sense disambiguation. Motivated by these observations, we offer several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the
definition of sense inventories.
1 Introduction

Inductive Constraint Logic
and the Mutagenesis Problem
Wim Van Laer Hendrik Blockeel
Luc De Raedt
Department of Computer Science, Katholieke Universiteit Leuven
Celestijnenlaan 200A, B-3001 Heverlee, Belgium
Email:fWimV,Hendrik,LucDRg@cs.kuleuven.ac.be
Abstract
A novel approach to learning first order logic formulae from positive and negative examples is incorporated in a system named ICL (Inductive Constraint
Logic). In ICL, examples are viewed as interpretations which are true or false
for the target theory, whereas in present inductive logic programming systems,
examples are true and false ground facts (or clauses). Furthermore, ICL uses a
clausal representation, which corresponds to a conjunctive normal form where
each conjunct forms a constraint on positive examples, whereas classical learning
techniques have concentrated on concept representations in disjunctive normal
form.
We present some experiments with this new system on the mutagenesis problem. These experiments illustrate some of the differences with other systems,
and indicate that our approach should work at least as well as the more classical
approaches.
1 Introduction

Model-guided Segmentation of Corpus Callosum in MR Images
Arvid Lundervold 1 , Nicolae Duta 2 , Torfinn Taxt 1 & Anil K. Jain 2
1 Section for Medical Image Analysis and Informatics, Department of Physiology
University of Bergen,   Arstadveien 19, N-5009 Bergen, Norway
2 Department of Computer Science and Engineering, Michigan State University
East Lansing, MI 48824-1226, USA
Abstract
Magnetic resonance imaging (MRI) of the brain, followed by automated segmentation of the corpus callosum
(CC) in midsagittal sections have important applications
in both clinical neurology and neurocognitive research
since the size and shape of the CC are shown to be correlated to sex, age, neurodegenerative diseases and various lateralized behavior in man. Moreover, whole head,
multispectral 3D MRI recordings enable voxel-based tissue classification and estimation of total brain volumes,
in addition to CC morphometric parameters. We propose
a new algorithm that uses both multispectral MRI measurements (intensity values) and prior information about
shape (CC template) to segment CC in midsagittal slices
with very little user interaction. The algorithm has been
tested on a sample of 10 subjects scanned with multispec-tral 3D MRI, collected for a study of dyslexia, with very
good agreement between the manually traced (true) CC
outline and the detected CC outline. We conclude that
the proposed method for CC segmentation is promising for
clinical use when multispectral MR images are recorded.
1 Introduction

Statistical Characteristics and Multiplexing of MPEG Streams
Marwan Krunz , Ron Sass , and Herman Hughes
Department of Electrical Engineering
Department of Computer Science
Michigan State University
East Lansing, MI 48824
Abstract
This paper presents a study of the statistical characteristics and multiplexing of Variable-Bit-Rate (VBR)
MPEG-coded video streams. Our results are based on
23 minutes of video obtained from the entertainment
movie, The Wizard of Oz. The experimental setup
which was used to capture, digitize, and compress the
video stream is described. Although the study is conducted at the frame level (as opposed to the slice level),
it is observed that the inter-frame correlation structure for the frame-size sequence involves complicated
forms of pseudo-periodicity that are mainly affected
by the compression pattern of the sequence. A simple model for an MPEG traffic source is developed in
which frames are generated according to the compression pattern of the original captured video stream. The
number of cells per frame is fitted by a lognormal distribution. Simulations are used to study the performance of an ATM multiplexer for MPEG streams.
1 Introduction

Aditi-Prolog language manual +L +
James Harland
David B. Kemp
Tim S. Leask
Kotagiri Ramamohanarao
John A. Shepherd
Zoltan Somogyi
Peter J. Stuckey
Jayen Vaghani
Abstract
Aditi is a deductive database system under development at the Collaborative Information
Technology Research Institute by researchers from the University of Melbourne. The main
language in which users interact with Aditi is Aditi-Prolog. This document is a reference
manual for Aditi-Prolog.
1 Introduction

An Agent-Based Approach
for Robot Vision System
Technical Report 95/34
Tak Keung CHENG
Leslie KITCHEN
Zhi-Qiang LIU
James COOPER
+PAGE+

Agents for Citation Finding
on the World Wide Web
Yi Han, Seng Wai Loke, Leon Sterling
Technical Report 96/40
Department of Computer Science
The University of Melbourne
Parkville, Victoria 3052
Australia
+PAGE+

Termination Analysis for Mercury
Chris Speirs, Zoltan Somogyi and Harald Stndergaard
Department of Computer Science
The University of Melbourne
Parkville, Victoria 3052, Australia
Abstract
Since the late eighties, much progress has been made in the theory of termination analysis for
logic programs. However, from a practical point of view, the significance of much of the work
on termination is hard to judge, since experimental evaluations rarely get published. Here we
describe and evaluate a termination analyzer for Mercury, a strongly typed and moded logic-
functional programming language. Mercury's high degree of referential transparency and the
guaranteed availability of reliable mode information simplify the termination analysis of Mer-
cury compared with that of other logic programming languages. We describe our termination
analyzer, which uses a variant of a method developed by Plumer. It deals with full Mercury,
including modules, declarative input/output, the foreign language interface, and higher-order
features. In spite of these obstacles, it produces high-quality termination information, comparable to the results recently obtained by Lindenstrauss and Sagiv. Most important, in stark
contrast with Lindenstrauss and Sagiv's experimental results, our analyzer has a negligible
impact on the running time of the compiler of which it is part, even for large programs. This
means that the Mercury compiler can produce valuable termination information at no real
cost to the programmer.
1 Introduction

Mixed Semidefinite-Quadratic-Linear Programs
Jean-Pierre Haeberly Madhu V. Nayakkankuppam
Michael L. Overton
September 23, 1998
Abstract
We consider mixed semidefinite-quadratic-linear programs. These are
linear optimization problems with three kinds of cone constraints, namely:
the semidefinite cone, the quadratic cone and the nonnegative orthant. We
outline a primal-dual path following method to solve these problems and
highlight the main features of SDPpack, a Matlab package which solves
such programs. We give some examples where such mixed programs arise,
and provide numerical results on benchmark problems.
1 Introduction

Methods for Handling Faults and Asynchrony
in Parallel Computation
Z. M. Kedem
1. Introduction and Motivation

Toward the Automation of the Card-Playing
Component of Bridge
Ming-Sheng Chang
Department of Computer Science
Courant Institute of Mathematical Sciences
New York University
Abstract
In comparision with other games, particularly chess, the research in computer bridge is
immature, and the best bridge-playing programs are mediocre. We propose to study the
automation of the card-playing segment of bridge (omitting bidding), using a number of
different techniques. In this paper we first give an introduction to the state of computer
bridge. Next, we propose two possible architectures for solving double-dummy bridge
(i.e., a simplified bridge game with perfect information): The first is based on the
combination of And-OR search and heuristic evaluation. The second forms a global plan
by merging subplans for each individual suit. Next, to deal with uncertain information in
real bridge, we present a new mechanism that combines the concepts of both minimax
search and possible worlds. Finally we give a brief description of further work toward
automating card-playing in real bridge.
1 Introduction

The Development of the C Language
Dennis M. Ritchie
AT&T Bell Laboratories
Murray Hill, NJ 07974 USA
dmr@research.att.com
ABSTRACT
The C programming language was devised in the early 1970s as a system
implementation language for the nascent Unix operating system. Derived from
the typeless language BCPL, it evolved a type structure; created on a tiny
machine as a tool to improve a meager programming environment, it has become
one of the dominant languages of today. This paper studies its evolution.
Introduction

Hierarchical Explanation-Based Reinforcement Learning
Prasad Tadepalli and Thomas G. Dietterich
Computer Science Department
Oregon State University
Corvallis,Oregon 97331-3202
ftadepalli,tgdg@research.cs.orst.edu
Abstract
Explanation-Based Reinforcement Learning
(EBRL) was introduced by Dietterich and
Flann as a way of combining the ability of
Reinforcement Learning (RL) to learn optimal plans with the generalization ability
of Explanation-Based Learning (EBL) (Di-etterich & Flann, 1995). We extend this
work to domains where the agent must order and achieve a sequence of subgoals in
an optimal fashion. Hierarchical EBRL can
effectively learn optimal policies in some of
these sequential task domains even when the
subgoals weakly interact with each other.
We also show that when a planner that can
achieve the individual subgoals is available,
our method converges even faster.
1 Introduction

DISTRIBUTED CONTROL IN OPTICAL WDM NETWORKS
X. Yuan R. Gupta R. Melhem
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
ABSTRACT
This paper describes and evaluates distributed
wavelength reservation protocols for all-optical WDM
networks. These protocols are essential for applying
WDM techniques to large scale all-optical networks.
The protocols ensure that the wavelengths on the links
along a path are reserved before communication takes
place. A message is transmitted using the reserved
wavelengths and remains in the optical domain utill
it reaches the destination. Based upon the timing at
which the reservation is performed, the protocols are
classified into two categories: forward reservation protocols and backward reservation protocols. Although
forward reservation protocols are simpler, our performance study shows that backward reservation protocols provide better performance.
INTRODUCTION

Contingency Selection in Plan Generation
Nilufer Onder
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
nilufer@cs.pitt.edu
Martha E. Pollack
Department of Computer Science
and Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
pollack@cs.pitt.edu
Abstract
A key question in conditional planning is: how many,
and which of the possible execution failures should be
planned for? One cannot, in general, plan for all the
possible failures because the search space is too large.
One cannot ignore all the possible failures, or one will
fail to produce sufficiently flexible plans. In this paper,
we describe an approach to conditional planning that
attempts to identify the contingencies that contribute
the most to a plan's overall utility. Plan generation
proceeds by handling the most important contingencies first, extending the plan to include actions that
will be taken in case the contingency fails. We discuss
the representational issues that must be addressed in
order to implement such an algorithm, and present an
example which illustrates our approach.
Introduction

Working Sets, Cache Sizes, and Node Granularity Issues
for Large-Scale Multiprocessors
Edward Rothberg Jaswinder Pal Singh and Anoop Gupta
Intel Supercomputer Systems Division Computer Systems Laboratory
14924 N.W. Greenbrier Parkway   Stanford University
Beaverton, OR 97006 Stanford, CA 94305
Abstract
The distribution of resources among processors, memory and
caches is a crucial question faced by designers of large-scale
parallel machines. If a machine is to solve problems with a
certain data set size, should it be built with a large number of
processors each with a small amount of memory, or a smaller
number of processors each with a large amount of memory?
How much cache memory should be provided per processor for
cost-effectiveness? And how do these decisions change as larger
problems are run on larger machines?
In this paper, we explore the above questions based on the
characteristics of five important classes of large-scale parallel scientific applications. We first show that all the applications have a hierarchy of well-defined per-processor working
sets, whose size, performance impact and scaling characteristics
can help determine how large different levels of a multiprocessor's cache hierarchy should be. Then, we use these working sets together with certain other important characteristics of
the applications|such as communication to computation ratios,
concurrency, and load balancing behavior|to reflect upon the
broader question of the granularity of processing nodes in high-performance multiprocessors.
We find that very small caches whose sizes do not increase
with the problem or machine size are adequate for all but two of
the application classes. Even in the two exceptions, the working
sets scale quite slowly with problem size, and the cache sizes
needed for problems that will be run in the foreseeable future
are small. We also find that relatively fine-grained machines,
with large numbers of processors and quite small amounts of
memory per processor, are appropriate for all the applications.
1 Introduction

Modeling Communication in Parallel Algorithms:
A Fruitful Interaction between Theory and Systems?
Jaswinder Pal Singh * , Edward Rothberg , and Anoop Gupta *
* Computer Systems Laboratory Intel Supercomputer Systems
Stanford University   14924 NW Greenbrier Pkwy, CO6-09
Stanford, CA 94305 Beaverton, OR 97006
Abstract
Recently, several theoretical models of parallel architectures have been proposed to replace the PRAM as the model
that is presented to an algorithm designer. A primary focus of
the new models is to include the cost of interprocessor communication, which is increasingly important in modern parallel
architectures. We argue that modeling the communication costs
in the architecture or system is only one part of the problem.
The other, and usually much more difficult, part is modeling
the communication properties of the algorithm itself, which
provides necessary inputs into the architectural model to determine overall complexity. In this context, we make three main
points in this paper: (i) It is incomplete to describe communication without regard to its relationship with replication. We
propose a description of the communication-replication relationship in terms of the working set hierarchy of an algorithm.
(ii) Both inherent communication and the communication-replication relationship can be very difficult to model in irregular, dynamic computations that are crucial in many real-world
applications. We present some examples that demonstrate this
difficulty. (iii) We believe that substantial leverage can be
obtained in this effort from the computer systems community,
which can provide a hierarchy of simulation and profiling
toolsfrom abstract to detailedtailored to the needs of the
algorithm designers. We propose an initial set of simulation
tools, and we discuss possible future refinements to this set.
1 Introduction

Technical Report TR-514-96.
Irregular Applications under Software Shared Memory
Liviu Iftode, Jaswinder Pal Singh and Kai Li
Department of Computer Science
Princeton University
Princeton, NJ 08544
liv,jps,li@cs.princeton.edu
Abstract
Shared Virtual Memory (SVM) provides an inexpensive way to support the popular shared address
space programming model on networks of workstations or personal computers. Despite recent advances
in SVM systems, their performance for all but coarse-grained or regular applications is not well understood.
Nor is there an understanding of whether and how
fine-grained, irregular programs should be written differently for SVM, with its large granularities of communication and coherence, than for the more familiar
hardware coherent at cache line granularity. In this
paper we try to understand the performance and programming issues for emerging, irregular applications
on SVM systems. We examine performance on both
an aggressive all-software system as well as one with a
little hardware support in the network interface. We
also present approaches to improve the performance
of irregular applications at both the programming and
the system level. As a result of our experiences, we
identify a set of guidelines and techniques that pertain
specifically to programming SVM systems, beyond the
guidelines commonly used for programming hardware-coherent systems as well. We also present a further
relaxation of the memory consistency model, called
scope consistency, which is particularly effective for
such applications.
1 Introduction

Network Services for
Multi-User Virtual Environments
Thomas A. Funkhouser
AT&T Bell Laboratories
Murray Hill, NJ
Abstract
This paper describes network services to support
large multi-user virtual environments. A client-server design is proposed in which multiple servers
coordinate execution, manage communication, offload processing, and provide persistent storage for
their clients. Using this design, it is possible to support real-time features, such as collision detection,
voice bridging, persistent updates, physical simulation, and autonomous agents, that would be difficult to implement for large virtual environments
with a peer-to-peer design. The paper includes a
description of services being implemented in RING,
a client-server system for interaction between many
users in large virtual environments.
1 Introduction

Database and Display Algorithms for
Interactive Visualization of Architectural Models
by
Thomas Allen Funkhouser
B.S. (Stanford University) 1983
M.S. (University of California at Los Angeles) 1989
A dissertation submitted in partial satisfaction of the
requirements for the degree of
Doctor of Philosophy
in
Computer Science
in the
GRADUATE DIVISION
of the
UNIVERSITY of CALIFORNIA at BERKELEY
Committee in charge:
Professor Carlo H. Sequin , Chair
Professor Lawrence Rowe
Professor Jean Pierre Protzen
1993
+PAGE+

Simulating the Madness of Crowds:
Price Bubbles in an Auction-Mediated Robot Market
Ken Steiglitz and Daniel Shapiro
Dept. of Computer Science, Princeton University
Princeton, NJ 08544
May 5, 1997
Abstract
We simulate a multiagent market with production, consumption, and exchange
mediated by a sealed-bid double auction. Marked price bubbles and subsequent
crashes occur when value-based (fundamentals-driven) and trend-based traders are
both present, and the market equilibrium price is ramped up exogenously. Similarly,
negative price bubbles and recoveries occur when the equilibrium price is ramped
down. Because the simulated market is auction-mediated, we can observe the operations of traders during these events, and study the interactions that produce and
resolve bubbles. Some preliminary circuit-breaker experiments are described, in which
bubbles are interrupted during their formation.
1 Introduction

DIMACS Technical Report 98-10
February 1998
Patience is a Virtue: The Effect of Delay on
Competitiveness for Admission Control
by
Michael H. Goldwasser 1
Department of Computer Science
Princeton University
Princeton, NJ 08544
wass@cs.princeton.edu
1 Permanent Member
DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs-Research,
Bell Labs and Bellcore.
DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999;
and also receives support from the New Jersey Commission on Science and Technology.
+PAGE+

An Evolutionary Approach to
Combinatorial Optimization Problems
Sami Khuri
Department of Mathematics & Computer Science
San Jose State University
One Washington Square
San Jose, CA 95192-0103, U.S.A.
khuri@sjsumcs.sjsu.edu
Thomas Back and Jorg Heitkotter
Department of Computer Science
University of Dortmund
Systems Analysis Research Group, LSXI
D-44221 Dortmund, Germany
fbaeck,jokeg@ls11.informatik.uni-dortmund.de
Copyright c 1993 ACM Press. All rights reserved.
To appear in the proceedings of CSC'94
Phoenix Arizona, March 8-10, 1994.
Abstract: The paper reports on the application of genetic
algorithms, probabilistic search algorithms based on the
model of organic evolution, to NP-complete combinatorial
optimization problems. In particular, the subset sum, maximum cut, and minimum tardy task problems are considered.
Except for the fitness function, no problem-specific changes
of the genetic algorithm are required in order to achieve results of high quality even for the problem instances of size
100 used in the paper. For constrained problems, such as the
subset sum and the minimum tardy task, the constraints are
taken into account by incorporating a graded penalty term
into the fitness function. Even for large instances of these
highly multimodal optimization problems, an iterated application of the genetic algorithm is observed to find the global
optimum within a number of runs. As the genetic algorithm
samples only a tiny fraction of the search space, these results
are quite encouraging.
1 Introduction

Investigating
Genetic Algorithms
for Scheduling
Hsiao-Lan Fang
MSc Dissertation
Department of Artificial Intelligence
University of Edinburgh
1992
+PAGE+

Navigation in Three Dimensional Spaces
CS-590Z
Carlos Gonzalez Ochoa Aleman
May 23, 1994
Abstract
Current graphic hardware have helped to develop scientific visualization tools,
but this progress has not level with the magnitude of data genereted in some areas needing to be visualized. Techniques to navigate data have been developed,
including new hardware and algorithms to improve the rendering speed and quality.
This paper will describe the issues of navigation, current display and interaction
technology, and algorithms. At the end a set of problems yet to be solved will be
discussed
1 Introduction

A Color-based Technique for Measuring Visible Loss for Use
in Image Data Communication 1
Melliyal Annamalai, Aurobindo Sundaram and Bharat Bhargava
Department of Computer Sciences
Purdue University
W. Lafayette, IN 47906, USA
fmelli,auro,bbg@cs.purdue.edu
Abstract
The concept of the global information infrastructure and specifically that of the World
Wide Web (WWW) has led to users accessing data of different media including images
and video data over a wide area network. These data objects have sizes the order of
megabytes and communication time is very large. The data size can be reduced without
losing information by applying loss-inducing techniques and this will lead to reduction in
communication time. Several loss-inducing techniques have been developed and each image
is treated differently by each technique. In some cases an acceptable quality of the image
is obtained and in some cases it is not. In this paper we develop a color-based technique
to quantify the data loss when a loss-inducing technique is applied to an image. This will
result in estimating whether the resulting image is indistinguishable from the original with
respect to the human eye. We illustrate its use to classify images according to the loss they
can tolerate. This avoids redundant communication of a high quality image when a lower
quality image can satisfy the application resulting in the conservation and better usage
of network resources. We present the technique, the communication time saved, and an
experimental evaluation to prove the validity of the technique.
1 Introduction

AFEC: An Adaptive Forward Error-Correction
Protocol and Its Analysis
Kihong Park
Department of Computer Sciences
Purdue University
West Lafayette, IN 47907
park@cs.purdue.edu
CSD-TR 97-038
Abstract
This paper presents an adaptive protocol for packet-level forward error-correction in dynamic networks. The objective is to facilitate best-effort real-time applications whose timing
constraints rule out the use of retransmission-based ARQ schemes. The degree of redundancy
is adjusted as a function of network state, decreasing when the network is well-behaved and
increasing when it is not. The control problem is nontrivial due to the fact that increased redundancy, beyond a certain level, backfires resulting in self-induced congestion which impedes
the timely recovery of information at the receiver.
In the first part of the paper, we present a comprehensive analysis of the control problem
associated with dynamic forward error-correction, concentrating on a particular protocol called
Adaptive Forward Error-Correction (AFEC). We show that instabilities can arise from two
distinct sources|desired operating point location and network delay|and we give solutions to
handle them. The first causal factor is intimately tied to optimality, making its achievement
potentially perilous in the context of QoS-greedy applications.
The second part of the paper presents simulation results that confirm the qualitative dynamics predicted by the analysis. We quantitatively estimate the redundancy-recovery rate function
which relates redundancy to the quality of service rendered at the receiver. We show under what
conditions the curve's shape is unimodal and to what degree. We compare the performance of
AFEC against a static FEC protocol in which the redundancy factor is fixed. We show that
AFEC exhibits superior performance when the network is subject to structural changes that
persist for nonnegligible durations. Under short-range dependent traffic conditions, AFEC is
able to closely match the performance of optimum static FEC but not exceed it.
+PAGE+

PYTHIA: A Knowledge Based System
for Intelligent Scientific Computing
Sanjiva Weerawarana, Elias N. Houstis, John R. Rice, Anupam Joshi
Purdue University
and
Catherine E. Houstis
University of Crete
Categories and Subject Descriptors: I.2.1 [Artificial Intelligence]: Applications and Expert
Systems; G.1.8 [Numerical Analysis]: Partial Differential Equations
General Terms: Knowledge Based Systems
Additional Key Words and Phrases: Computational Intelligence, Knowledge Based Systems, Partial Differential Equations, Performance Evaluation, Problem Solving Environments
1. ABSTRACT
Domain specific Problem Solving Environments (PSEs) are the key new ingredients that will aid in the widespread use of Computational Science & Engineering
(CS&E) systems. Each PSE consists of a well defined library that supports the
numerical and symbolic solution of certain mathematical model(s) characterizing a
specific discipline, together with an easy to use software environment. This environment should ideally interact with the user in a language "natural" to the associated
discipline, and provide a high level abstraction of the underlying, computationally
complex, model. However, it appears that almost all extant PSEs assume that
the user is familiar with the specific functionality/applicability of the PSE. Their
primary design objective is to support some form of high level programming with
predefined state-of-the-art algorithmic infrastructure. As the functionality of these
systems increases, the user is expected to make complex decisions in the parametric space of the algorithmic infrastructure supported by the PSE. In this paper
we describe a knowledge based system, PYTHIA, to automate this decision making process and aid in providing a high level abstraction to the user. Specifically,
PYTHIA addresses the problem of (parameter, algorithm) pair selection within a
scientific computing domain assuming some minimum user specified computational
objectives and some characteristics of the given problem. PYTHIA's framework
and methodology is general and applicable to any class of scientific problems and
Work supported in part by AFOSR award 91-F49620, NSF awards CCR 86-19817, CCR 92-02536,
and ASC 9404859.
Authors' addresses:   S. Weerawarana, E.N. Houstis, J.R. Rice and A. Joshi:   Department of Computer Sciences, Purdue University,   West Lafayette, IN 47907, USA;   C.E. Houstis:   Department of
Computer Science, University of Crete,   Heraklion, Greece.
+PAGE+

On the Collapse of the q-Gram Filtration
E. SUTINEN
University of Helsinki,   Finland
W. SZPANKOWSKI
Purdue University,   U.S.A.
Abstract
In the approximate pattern matching problem, the text area to be searched
for an occurrence of a pattern can be pruned by applying a filtration condition. A q-gram based filtration condition defines potential text areas in terms
of pattern q-grams, i.e., strings of length q. A text area will be checked by
an accurate method only if the set of the q-grams in the text area satisfies a
certain condition. One hopes that the filtration limits the number of checks
to a minimum, thus making the algorithm quite efficient. However, computer experiments show that the filtration method works fine for cases when
the allowed error level k is relatively small compared to the pattern length,
but loses its efficiency quite sharply with an increasing k. This is a phase
transition phenomenon that is quite often observed in nature. In this paper,
we present a theoretical explanation for this phenomenon which will excuse
us to introduce advanced mathematical analysis based on certain languages,
correlation polynomials, generating functions and complex analysis. It is our
view that nothing can be more exciting and rewarding than finding a theoretical justification for an abrupt manifestation of nature.
Keywords: Algorithm Analysis, Approximate Pattern Matching.
1 Introduction

Authorship Analysis: Identifying The
Author of a Program 1
Ivan Krsul
The COAST Project
Department of Computer Sciences
Purdue University
West Lafayette, IN 47907-1398
krsul@cs.purdue.edu
May 3, 1994
Technical Report CSD-TR-94-030
1 This paper was originally written as a Master's thesis at Purdue University.
+PAGE+

Efficient Disk Allocation for Fast Similarity Searching
Sunil Prabhakar Divyakant Agrawal Amr El Abbadi
Department of Computer Science
University of California
Santa Barbara
CA 93106, U.S.A.
fsunilp,agrawal,amrg@cs.ucsb.edu
Abstract
As databases increasingly integrate non-textual information
it is becoming necessary to support efficient similarity searching in addition to range searching. Recently, declustering
techniques have been proposed for improving the performance of similarity searches through parallel I/O. In this
paper, we propose a new scheme which provides good declus-tering for similarity searching. In particular, it does global
declustering as opposed to local declustering, exploits the
availability of extra disks and does not limit the partitioning of the data space. Our technique is based upon the
Cyclic declustering schemes which were developed for range
and partial match queries. We establish, in general, that
Cyclic declustering techniques outperform previously proposed techniques.
Appeared in Proc. 10th ACM Symposium on Parallel Algorithms and Architectures (SPAA '98), Puerto Vallarta, Mexico
1 Introduction

A Coarse-Grained Parallel
QR-Factorization Algorithm for
Sparse Least Squares Problems
Tz. Ostromsky P. C. Hansen Z. Zlatev
Abstract
A sparse QR-factorization algorithm SPARQR for coarse-grained parallel
computations is described. The coefficient matrix, which is assumed to be
general sparse, is reordered in an attempt to bring as many zero elements in
the lower left corner as possible. The reordered matrix is then partitioned into
block rows, and Givens plane rotations are applied in each block-row. These are
independent tasks and can be done in parallel. Row and column permutations
are carried out within the diagonal blocks in an attempt to preserve better the
sparsity of the matrix.
The algorithm can be used for solving least squares problems either directly
or combined with an iterative method (preconditioned conjugate gradients are
used). Small non-zero elements can optionally be dropped in the latter case.
This leads to a better preservation of the sparsity and, therefore, to a faster
factorization. The price which has to be paid is some loss of accuracy. The
iterative method is used to regain the accuracy lost during the factorization.
Numerical results from several experiments with matrices from the well-known Harwell-Boeing collection as well as with some larger sparse matrices are
presented in this work. An SGI Power Challenge computer with 16 processors
has been used in the experiments.
Keywords: coarse-grained parallelism, least squares problem, QR-factorization,
general sparse matrix, drop-tolerance, reordering, partitioning, block algorithm.
Purdue University, Department of Computer Science,
West Lafayette, IN 47907, USA
e-mail: tto@cs.purdue.edu
Institute of Mathematical Modelling, Technical University of Denmark,
Bldg. 305, DK-2800 Lyngby, Denmark
e-mail: pch@imm.dtu.dk
National Environmental Research Institute,   Frederiksborgvej 399, DK-4000 Roskilde, Denmark
e-mail: luzz@sun2.dmu.dk
+PAGE+

Efficient Finite-State Approximation of Context
Free Grammars
C.M. Rood
Computer Laboratory
University of Cambridge
cmr1001@cl.cam.ac.uk
Abstract. This paper introduces a novel method for constructing finite-state machines recognising context free (CF)
grammars. The method utilizes the idea of a path through
a finite-state machine (FSM). Certain paths form the basis
for an unfolding process which is applied to the LR(0) characteristic finite-state machine (CFSM) corresponding to the
grammar. The next section discusses the approximation algorithm, including this unfolding process, in more detail, and
section 3 introduces the concept of an unfolding criterion.
Section 4 proves the soundness of the approximation method,
and its exactness to arbitrary, fixed recursive depths. Section
5 presents initial computational figures resulting from an implementation of the method. A variation of the method that
is more computationally feasible is discussed. The final section compares the method with existing research on finite-state approximation of CF grammars, and presents preliminary conclusions regarding the method.
1 THE APPROXIMATION

Evaluating the Performance of Software Distributed Shared Memory
as a Target for Parallelizing Compilers
Alan L. Cox , Sandhya Dwarkadas , Honghui Lu and Willy Zwaenepoel
Rice University
Houston, TX 77005-1892
falc, hhl, willyg@cs.rice.edu
University of Rochester
Rochester, NY14627-0226
sandhya@cs.rochester.edu
Abstract
In this paper, we evaluate the use of software distributed
shared memory (DSM) on a message passing machine as
the target for a parallelizing compiler. We compare this approach to compiler-generated message passing, hand-coded
software DSM, and hand-coded message passing. For this
comparison, we use six applications: four that are regular
and two that are irregular.
Our results are gathered on an 8-node IBM SP/2 using the TreadMarks software DSM system. We use the APR
shared-memory (SPF) compiler to generate the shared memory programs, and the APR XHPF compiler to generate message passing programs. The hand-coded message passing
programs run with the IBM PVMe optimized message passing library. On the regular programs, both the compiler-generated and the hand-coded message passing outperform
the SPF/TreadMarks combination: the compiler-generated
message passing by 5.5% to 40%, and the hand-coded
message passing by 7.5% to 49%. On the irregular programs, the SPF/TreadMarks combination outperforms the
compiler-generated message passing by 38% and 89%, and
only slightly underperforms the hand-coded message passing, differing by 4.4% and 16%. We also identify the factors
that account for the performance differences, estimate their
relative importance, and describe methods to improve the
performance.
1. Introduction

Carlsberg: A Distributed Execution Environment
Providing Coherent Shared Memory and
Integrated Message Passing
A Position/Work-in-Progress Paper presented at
Nordic Workshop on Programming Environment Research, NWPER'94,
Lund, Sweden, June, 1994
Povl T. Koch Robert J. Fowler
Department of Computer Science, University of Copenhagen (DIKU)
Universitetsparken 1, 2100 Copenhagen, Denmark
Tel: +45 35 32 14 18 Fax: +45 35 32 14 01   E-mail: koch,fowler@diku.dk
Abstract
The Carlsberg prototype is a distributed operating system designed to provide efficient support for distributed-parallel applications on a cluster of high-performance workstations. A unique feature of Carlsberg is the integration of
coherent shared memory, multithreading, and message passing in one system.
In this paper we discuss the motivation for the Carlsberg system and we present
aspects of its design.
1 Introduction

8th SIAM Conference on Parallel Processing for Scientific Computing, Minneapolis, MN, March 1997.
On the Accuracy of Anderson's Fast N-body Algorithm
Yu Charlie Hu S. Lennart Johnsson
Abstract
We present an empirical study of the accuracy-cost tradeoffs of Anderson's method.
The various parameters that control the degree of approximation of the computational
elements and the separateness of interacting computational elements govern both the
arithmetic complexity and the accuracy of the method. Our experiment shows that for
a given error requirement, using a near-field containing only nearest neighbor boxes
and a hierarchy depth that minimizes the number of arithmetic operations minimizes
the total number of arithmetic operations.
1 Introduction

Reprinted from International Journal in Computer Simulation, Vol. 1, pp. 31-58 (1991)
The Efficient Simulation of Parallel Computer Systems
R. G. Covington
Jet Propulsion Laboratory,   125-233
4800 Oak Grove Drive
Pasadena, CA 91109
S. Dwarkadas, J. R. Jump, and J. B. Sinclair
Department of Electrical and Computer Engineering
Rice University
Houston, Texas 77251
S. Madala +L +
Coherent Systems, Inc.
1000 Bay Area Blvd., Suite 206
Houston, TX 77058
Abstract: An ongoing research project involves the design and evaluation of a software system
for simulating parallel computers. A major goal in the development of this system was to avoid the
high overhead associated with the conventional instruction-level simulation of sequential
computers, but to retain the accuracy of that technique derived from its use of the execution of real
programs. The resulting system is program-driven, but the overhead is significantly reduced by
profiling the program to get timing estimates for its basic blocks, which are then used at run time to
generate process execution times dynamically while avoiding a detailed emulation of each
instruction's execution. A number of experiments dealing with message-passing computer
systems have been performed in order to determine the level of accuracy that can be expected from
its performance predictions and to measure its overhead.
Index Terms - Architecture models, efficiency, parallel computers, parallel programs,
performance, simulation, testbed, validation.
This research was supported by Texas Instruments Grant No. TI720845RJ, NSF/ONR Grant No. N00014-87-K
0324, SDSU/SDIO Contract No. N66001-85-D-0203, NASA Grant No. NAG 0-208, and a Shell Doctoral
Fellowship.
+PAGE+

RICE UNIVERSITY
Synchronization, Coherence, and Consistency for
High Performance Shared-Memory
Multiprocessing
by
Sandhya Dwarkadas
A Thesis Submitted
in Partial Fulfillment of the
Requirements for the Degree
Doctor of Philosophy
Approved, Thesis Committee:
J Robert Jump, Co-Chairman
Professor
Electrical and Computer Engineering
James B. Sinclair, Co-Chairman
Associate Professor
Electrical and Computer Engineering
John K. Bennett
Assistant Professor
Electrical and Computer Engineering
Ken Kennedy
Professor
Computer Science
Houston, Texas
September, 1992
+PAGE+

Theoretical Foundations of Association Rules
Mohammed J. Zaki and Mitsunori Ogihara
Computer Science Department, University of Rochester,   Rochester NY 14627
fzaki,ogiharag@cs.rochester.edu
Abstract
In this paper we describe a formal framework for the problem of mining association rules. The theoretical foundation is based on the field of formal concept analysis. A concept is composed of closed subsets of attributes (itemsets)
and objects (transactions). We show that all frequent itemsets are uniquely determined by the frequent concepts. We
further show how this lattice-theoretic framework can be used to find a small rule generating set, from which one can
infer all other association rules.
1 Introduction

On GDM's: Geometrically Deformed Models for the Extraction of Closed
Shapes from Volume Data
By
James Vradenburg Miller
A Thesis Submitted to the Graduate
Faculty of   Rensselaer Polytechnic Institute
in Partial Fulfillment of the
Requirements for the Degree of
Master of Science
Approved:
Dr. Michael J. Wozny
Thesis Adviser
Rensselaer Polytechnic Institute
Troy, New York
December 1990
+PAGE+

Modeling and the Adaptive Solution of
CVD Fiber-Coating Processes
S. Adjerid, J. E. Flaherty, J. B. Hudson, and M. S. Shephard
Scientific Computation Research Center
Rensselaer Polytechnic Institute
Troy, New York 12180, USA
Abstract
We develop a mathematical model for the coating of ceramic fibers by chemical vapor
deposition (CVD) in cylindrical hot- or cold-walled reactors. The model couples the
Navier-Stokes equations for the mixture of a carrier gas and precursor species, an energy
equation for the gaseous mixture, a convection-diffusion system for the reacting precursor
species, a fiber coating model, and a fiber heat conduction equation. The system is heated
by a combination of conduction, convection, and radiation.
The partial differential system resulting from this model is solved by adaptive finite element software using automatic mesh refinement and coarsening on a quadtree-structured
mesh. The results of parameter studies indicate the effectiveness of using adaptive solution techniques with CVD applications and also suggest some guidance for improving
the process.
1 Introduction

Automated Design Optimization for the P2 and P8
Hypersonic Inlets
Vijay Shukla , Andrew Gelsey , Mark Schwabacher
Donald Smith , Doyle D. Knight
Rutgers, the State University of New Jersey
New Brunswick, NJ 08903
February 20, 1997
To appear in Journal of Aircraft, Vol 34, No. 2, March-April 1997
Copyright c by Vijay Shukla, Andrew Gelsey, Mark Schwabacher, Donald Smith and Doyle D. Knight.
Abstract
An automated design methodology incorporating industry-standard Navier-Stokes
codes and a gradient-based optimizer has been developed. This system is used to redesign the well-known NASA P2 and P8 hypersonic inlets. First, the Navier-Stokes
simulations of the original P2 and P8 inlet designs are validated using numerical convergence studies and comparison with wind-tunnel experimental data for the original
inlets published by NASA in the early 1970s. Second, the P2 and P8 inlets are redesigned with the objective of canceling the cowl shock (and, in the case of the P8
inlet, the additional cowl-generated compression) at the centerbody by appropriate
contouring of the centerbody boundary. The original inlets were intended to achieve
these same objectives, but detailed experimental measurements indicated that a substantial reflected shock system was present. The choice of the objective function, which
is used to drive the optimization, has a significant impact on the final design. Several
different formulations for the objective function have been employed, and improvements of 60% to 90% in the objective function have been achieved. This automated
design system represents one of the first successful combinations of numerical optimization methods with Reynolds-averaged Navier-Stokes fluid dynamics simulation for high
speed inlets, and demonstrates a new area in which High Performance Computing may
have considerable impact on problems of military and industrial significance.
Postdoctoral Research Associate, Dept. of Mechanical and Aerospace Engineering. AIAA Member.
Assistant Professor, Computer Science Dept. AIAA Member.
Graduate Student, Computer Science Dept.
Assistant Professor, Computer Science Dept.
Professor, Dept. of Mechanical and Aerospace Engineering. AIAA Associate Fellow.
+PAGE+

To be presented at the AAAI/ICML 1998 Workshop on Predicting the Future: AI Approaches to Time-Series Analysis 1
Predicting Sequences of User Actions
Brian D. Davison and Haym Hirsh
Department of Computer Science
Rutgers, The State University of New Jersey
New Brunswick, NJ 08903 USA
fdavison,hirshg@cs.rutgers.edu
Abstract
People display regularities in almost everything they do. This
paper proposes characteristics of an idealized algorithm that,
when applied to sequences of user actions, would allow a user
interface to adapt over time to an individual's pattern of use.
We describe a simple predictive method with these characteristics and show its predictive accuracy on a large dataset of
UNIX commands to be at least as good as others that have
been considered, while using fewer computational and memory resources.
Motivation

Towards a Cost-Effective Parallel Data Mining Approach
Zoltan Jarai, Aashu Virmani, Liviu Iftode
Department of Computer Science
Rutgers University
Piscataway, NJ 08854
fjarai,avirmanig@paul.rutgers.edu, iftode@cs.rutgers.edu
Abstract
Massive rule induction has recently emerged as one of the
powerful data mining techniques. The problem is known to
be exponential in the size of the attributes, and given its ever
increasing use, can greatly benefit from parallelization.
In this paper, we study cost-effective approaches to paral-lelize rule generation algorithms. In particular, we consider
the propositional rule generation algorithm of the Discovery
Board system, and present our design and implementation of
a parallel algorithm for the same task. We then present some
early performance results of our parallelization scheme on
hardware and software distributed shared memory multiprocessors.
1 Introduction

Intelligent Model Selection for Hillclimbing Search in
Computer-Aided Design
Thomas Ellman John Keane Mark Schwabacher
Department of Computer Science, Hill Center for Mathematical Sciences
Rutgers University,   New Brunswick, NJ 08903
fellman,keane,schwabacg@cs.rutgers.edu
Abstract
Models of physical systems can differ according to
computational cost, accuracy and precision, among
other things. Depending on the problem solving
task at hand, different models will be appropriate. Several investigators have recently developed
methods of automatically selecting among multiple models of physical systems. Our research is
novel in that we are developing model selection
techniques specifically suited to computer-aided de
sign. Our approach is based on the idea that artifact performance models for computer-aided design
should be chosen in light of the design decisions
they are required to support. We have developed
a technique called "Gradient Magnitude Model Selection" (GMMS), which embodies this principle.
GMMS operates in the context of a hillclimbing
search process. It selects the simplest model that
meets the needs of the hillclimbing algorithm in
which it operates. We are using the domain of sailing yacht design as a testbed for this research. We
have implemented GMMS and used it in hillclimb-ing search to decide between a computationally expensive potential-flow program and an algebraic
approximation to analyze the performance of sailing yachts. Experimental tests show that GMMS
makes the design process faster than it would be if
the most expensive model were used for all design
evaluations. GMMS achieves this performance improvement with little or no sacrifice in the quality
of the resulting design.
1. Introduction

Why Should Architectural Principles be
Enforced?
Naftaly H. Minsky
minsky@cs.rutgers.edu
Department of Computer Science
Rutgers University
New Brunswick, NJ, 08903 USA
August 12, 1998
Abstract
There is an emerging consensus that an explicit architectural model
would be invaluable for large evolving software systems, providing them
with a framework within which such a system can be reasoned about and
maintained. But the great promise of architectural models has not been
fulfilled so far, due to a gap between the model and the system it purports
to describe. It is our contention that this gap is best bridged if the model
is not just stated, but is enforced.
This gives rise to a concept enforced architectural model |or, a law |
which is explored in this paper. We argue that this model has two major beneficial consequences: First, by bridging the above mentioned gap
between an architectural model and the actual system, an enforced architectural model provides a truly reliable framework within which a system
can be reasoned about and maintained. Second, our model provides software developers with a carefully circumscribed flexibility in molding the
law of a project, during its evolutionary lifetime|while maintaining certain architectural principles as invariant of evolution.
Keywords: architectural model, law-governed software, evolution, in
variants of evolution, firewalls, protection.
Work supported in part by NSF grants No. CCR-9308773
+PAGE+

Imposing The Law of Demeter and Its Variations
Partha pratim Pal
partha@cs.rutgers.edu
Naftaly H. Minsky
minsky@cs.rutgers.edu
Department of Computer Science
Rutgers University
New Brunswick, NJ 08903
February 28, 1996
Abstract
The Law of Demeter [4] is accepted as a useful design principle that
promotes tightly encapsulated classes and reduced coupling. Principles
like this are routinely adopted in real-life projects, however neither the
programming languages nor the existing environments provide enough
support for effective realization of these principles. It is our thesis
that broad structural principles should be formally specified, strictly
enforced, and relaxed whenever relaxation is in order. In this paper
we show how this can be done under our darwin-E environment using,
the Law of Demeter as an illustration.
keywords: Law of Demeter, Law-Governed Architecture, Software
Engineering Principles, Enforcement.
1 Introduction

Learning Novel Domains Through Curiosity and Conjecture
Paul D. Scott & Shaul Markovitch
Center for Machine Intelligence
2001,Commonwealth Blvd.,
Ann Arbor, Michigan 48105
Abstract
This paper describes DIDO, a system we have
developed to carry out exploratory learning of
unfamiliar domains without assistance from an
external teacher. The program incorporates novel
approaches to experience generation and representation
generation. The experience generator uses a heuristic
based on Shannon's uncertainty function to find
informative examples. The representation generator
makes conjectures on the basis of small amounts of
evidence and retracts them if they prove to be wrong
or useless. A number of experiments are described
which demonstrate that the system can distribute its
learning resources to steadily acquire a good
representation of the whole of a domain, and that the
system can readily acquire both disjunctive and
conjunctive concepts even in the presence of noise.
1. Introduction

Distributed parallel shooting for BVODEs
K.L. Chow and W.H. Enright
Department of Computer Science, University of Toronto
E-mail : chow@cs.toronto.edu and enright@cs.toronto.edu
Abstract
Many important scientific problems can be formulated as systems of ordinary differential equations with two-point boundary value constraints (BVODE). Multiple shooting
is one of the most widely used numerical techniques for solving BVODE problems.
In this work, we present a new distributed parallel numerical algorithm for BVODEs
which is based on multiple shooting. We investigate the numerical stability of this
new distributed algorithm and identify difficulties that can arise. We propose a new
parallel iterative refinement scheme to cope with some specific numerical difficulties
identified in our investigation. Computational experience is presented to demonstrate
the potential effectiveness of our approach.
Keywords: Distributed parallel algorithm, boundary value problems, multiple shooting
1 Introduction

Practical PAC Learning
Dale Schuurmans
Department of Computer Science
University of Toronto
Toronto, Ontario M5S 1A4, Canada
dale@cs.toronto.edu
Russell Greiner
Siemens Corporate Research
Princeton, NJ 08540, USA
greiner@scr.siemens.com
Appears in
Proceedings of the Fourteenth International Conference on Artificial Intelligence (IJCAI-95),
Montreal, August 1995.
Abstract
We present new strategies for "probably approximately correct" (pac) learning that use
fewer training examples than previous approaches. The idea is to observe training examples one-at-a-time and decide "on-line" when to
return a hypothesis, rather than collect a large
fixed-size training sample. This yields sequential learning procedures that pac-learn by observing a small random number of examples.
We provide theoretical bounds on the expected
training sample size of our procedure | but establish its efficiency primarily by a series of experiments which show sequential learning actually uses many times fewer training examples in
practice. These results demonstrate that pac-learning can be far more efficiently achieved in
practice than previously thought.
1 Introduction

Continuous sigmoidal belief networks
trained using slice sampling
Brendan J. Frey
Department of Computer Science, University of Toronto
6 King's College Road, Toronto, Canada M5S 1A4
Abstract
Real-valued random hidden variables can be useful for modelling
latent structure that explains correlations among observed variables. I propose a simple unit that adds zero-mean Gaussian noise
to its input before passing it through a sigmoidal squashing function. Such units can produce a variety of useful behaviors, ranging
from deterministic to binary stochastic to continuous stochastic. I
show how "slice sampling" (Neal 1996) can be used for inference
and learning in top-down networks of these units and demonstrate
learning on two simple problems.
1 Introduction

Learning Useful Horn Approximations
Russell Greiner
755 College Road East
Siemens Corporate Research
Princeton, NJ 08540
greiner@learning.siemens.com
Dale Schuurmans
Department of Computer Science
University of Toronto
Toronto, Ontario M5S 1A4
dale@cs.toronto.edu
Abstract
While the task of answering queries from an
arbitrary propositional theory is intractable in
general, it can typically be performed efficiently
if the theory is Horn. This suggests that it
may be more efficient to answer queries using a "Horn approximation"; i.e., a horn theory that is semantically similar to the original
theory. The utility of any such approximation
depends on how often it produces answers to
the queries that the system actually encounters;
we therefore seek an approximation whose expected "coverage" is maximal. Unfortunately,
there are several obstacles to achieving this goal
in practice: (i) The optimal approximation depends on the query distribution, which is typically not known a priori; (ii) identifying the optimal approximation is intractable, even given
the query distribution; and (iii) the optimal approximation might be too large to guarantee
tractable inference. This paper presents an approach that overcomes (or side-steps) each of
these obstacles. We define a learning process,
AdComp, that uses observed queries to estimate the query distribution "online", and then
uses these estimates to hill-climb, efficiently,
in the space of size-bounded Horn approximations, until reaching one that is, with provably
high probability, effectively at a local optimum.
Appears in the
Proceedings of the Third International Conference on Knowledge Representation and Reasoning,
25-29 October 1992, Cambridge, MA.
1 Introduction

Powers of Trees
Paul E. Kearney and Derek G. Corneil
Department of Computer Science, University of Toronto,
Toronto ON, CANADA M5S 1A4
kearney@cs.toronto.edu
dgc@cs.toronto.edu
Abstract. We present the first polynomial algorithm for recognizing tree powers. A graph G
is a tree power if there is a tree T and a positive integer k such that T k ~ = G where x and are
adjacent in T k if and only if d T (x; ) k. We also show that a natural extension of tree power
recognition is NP-complete, namely, given a graph G and a positive integer r, determine if
there is a tree power within r edges of G.
1 Introduction

Coding Segments Inside and Outside the Chromosome
Thomas Haynes
Department of Computer Science
Wichita State University
Wichita, KS 67260
E-mail: haynes@cs.twsu.edu
Phone: (316) 978-3925
Abstract
Coding segments are those sub-segments of the chromosome which contribute either positively or
negatively to the fitness evaluation of the chromosome. We extract coding segments from chromosomes
and we investigate the sharing of coding segments both inside and outside of the chromosome. We
find duplication of coding segments inside the chromosomes provides a back-up mechanism for the
search heuristics. We further find local search in a collective memory of coding segments outside of the
chromosome, collective adaptation, enables the search heuristic to represent partial solutions which are
larger than realistic chromosomes lengths and to express the solution outside of the chromosome.
+PAGE+

Learning to Select Useful Landmarks
Russell Greiner
Siemens Corporate Research
Princeton, NJ 08540
greiner@learning.siemens.com
Ramana Isukapalli
Department of Computer Science
Rutgers University
ramana@cs.rutgers.edu
Appears in the
Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94),
Seattle, Washington, July 1994.
Abstract
To navigate effectively, an autonomous agent must be
able to quickly and accurately determine its current
location. Given an initial estimate of its position (perhaps based on dead-reckoning) and an image taken of
a known environment, our agent first attempts to locate a set of landmarks (real-world objects at known
locations), then uses their angular separation to obtain an improved estimate of its current position. Unfortunately, some landmarks may not be visible, or
worse, may be confused with other landmarks, resulting in both time wasted in searching for invisible landmarks, and in further errors in the agent's estimate of
its position. To address these problems, we propose a
method that uses previous experiences to learn a selection function that, given the set of landmarks that
might be visible, returns the subset which can reliably
be found correctly, and so provide an accurate registration of the agent's position. We use statistical techniques to prove that the learned selection function is,
with high probability, effectively at a local optimal in
the space of such functions. This report also presents
empirical evidence, using real-world data, that demonstrate the effectiveness of our approach.
1. Introduction

Support for Implementation of Evolutionary
Concurrent Systems in Concurrent
Programming Languages
Raju Pandey 1 and J. C. Browne 2
1 Computer Science Department, University of California,   Davis, CA 95616    2 Department of Computer Sciences, The University of Texas,   Austin, TX 78712
Abstract. In many concurrent programming languages, concurrent programs are difficult to extend and modify: small changes in a concurrent
program may require re-implementations of a large number of its components. In this paper a novel concurrent program composition mechanism
is presented in which implementations of computations and synchronizations are completely separated. Separation of implementations facilitates
extensions and modifications of programs by allowing one to change implementations of both computations and synchronizations. The paper
also describes a concurrent programming model and a programming lan
guage that support the proposed approach.
1 Introduction

A VARIABLE TIME STEP METHOD FOR AN AGE-DEPENDENT
POPULATION MODEL WITH NONLINEAR DIFFUSION
BRUCE P. AYATI
Abstract. We propose a method for solving a model of age-dependent population diusion
with random dispersal. This method, unlike previous methods, allows for variable time steps and
independent age and time discretizations. We use a moving age discretization that transforms the
problem to a system of parabolic equations. The system is then solved by backward dierences in
time and a Galerkin approximation in space; the equations that need to be solved at each step treat
each age group separately. A priori L 2 error estimates are obtained by an energy analysis. These
estimates are superconvergent in the age variable. We present a postprocessing technique which
capitalizes on the superconvergence.
Key words. population dynamics, age-dependence, nonlinear diusion, variable time steps,
superconvergence, postprocessing.
AMS subject classitcations. 35Q80, 65M06, 65M15, 65M60, 92D25.
1. Introduction. In this paper, we consider a numerical method for solving an

ISSN 1360-1725
The Test Matrix Toolbox for Matlab (Version 3.0)
N. J. Higham
Numerical Analysis Report No. 276
September 1995
Manchester Centre for Computational Mathematics
Numerical Analysis Reports
DEPARTMENTS OF MATHEMATICS
Reports available from:
Department of Mathematics
University of Manchester
Manchester M13 9PL
England
And over the World-Wide Web from URLs
http://www.ma.man.ac.uk/MCCM/MCCM.html
ftp://ftp.ma.man.ac.uk/pub/narep
+PAGE+

User-Defined Aggregates
for Logical Data Languages
Haixun Wang and Carlo Zaniolo
Computer Science Department
University of California at Los Angeles
Los Angeles, CA 90095
hxwang@cs.ucla.edu zaniolo@cs.ucla.edu
Abstract. A new wave of data-intensive and knowledge-based applications|such as data
mining and decision support|require the introduction of complex application-specific aggregate functions. In this paper, we propose extensions for deductive database systems to
support these new applications. We develop constructs, formal semantics, and implementation techniques for user-defined aggregates, and describe their realization in an extended
LDL++ system recently built at UCLA. With these extensions, the system can support online aggregation, roll-ups for data cubing, temporal aggregates for time-series, iceberg queries,
and other recently proposed operators used in decision support and data mining procedures.
We then discuss the application of this technology to other DBMSs, and in particular to the
SQL3 specifications that support the notion of user-defined aggregates. We show that SQL3
suffers from limitations that severely restrict its use in new applications; thus we propose
simple extensions similar to those used for LDL++ to overcome such limitations.
1 Introduction

Greedy Algorithms in
Greedy with Choice and Negation
Sergio Greco Carlo Zaniolo
Dip: Elettr: Informatica Sist: Computer Science Dept:
Universita della Calabria Univ: of California at Los Angeles
87030 Rende; Italy LosAngeles; CA 90024
greco@si:deis:unical:it zaniolo@cs:ucla:edu
Abstract
In the design of algorithms, the greedy paradigm provides a powerful tool for solving
efficiently classical computational problems, within the framework of procedural languages. However, expressing these algorithms within the declarative framework of logic-based languages has proved to be a difficult research challenge. In this paper, we extend the framework of Datalog-like languages to obtain simple declarative formulations
for such problems, and propose effective implementation techniques to ensure computational complexities comparable to those of procedural formulations. These advances are
achieved through the use of the choice construct, that has semantics reducible to that of
programs with negation under stable model semantics. Then we extend the fixpoint-based
semantics of choice programs with preference annotations to guide search strategies and
simple logic-based formulations of classical greedy algorithms.
1 Introduction

Semantics-based Failure Recovery in Distributed Systems
with Optimistic Message Logging
Hong Va Leong Divyakant Agrawal
Department of Computer Science
University of California
Santa Barbara, CA 93106
Abstract
Recovery from failures is important in distributed computing. A common technique to support
recovery is asynchronous checkpointing, coupled with optimistic message logging. These schemes have
low overheads during failure-free operations and can provide an acceptable degree of fault-tolerance.
Central to these protocols is the determination of a maximal consistent global state, which is recoverable.
Message semantics is not exploited in most existing recovery protocols to determine the recoverable state.
We propose to identify messages that are not influential in the computation through message semantics.
These messages can be logically removed from the computation without changing its meaning or result.
In this paper, we illustrate with examples how the removal of these messages improves the theoretical
maximal consistent global state. Taking semantics into account, recovery protocols are then developed
to realize the idea. The semantics in object-oriented databases is adapted to special processes acting as
servers for further improvements. This technique can also be applied to ensure a more timely commitment
for output in a distributed computation.
Keywords: message semantics, commutativity, recovery, optimistic message logging, asynchronous
checkpointing
An abridged version of this paper appears in the Proceedings of the 14th International Conference on Distributed Computing
Systems as Using Message Semantics to Reduce Rollback in Optimistic Message Logging Recovery Schemes.
This research is supported in part by the NSF under grant number IRI-9117094.
+PAGE+

Run-time Compilation for Parallel Sparse Matrix Computations
Cong Fu and Tao Yang
Department of Computer Science
University of California,
Santa Barbara, CA 93106.
http://www.cs.ucsb.edu/f~cfu,~tyangg
Abstract
Run-time compilation techniques have been shown effective
for automating the parallelization of loops with unstructured
indirect data accessing patterns. However, it is still an open
problem to efficiently parallelize sparse matrix factorizations
commonly used in iterative numerical problems. The difficulty is that a factorization process contains irregularly-interleaved communication and computation with varying
granularities and it is hard to obtain scalable performance
on distributed memory machines. In this paper, we present
an inspector/executor approach for parallelizing such applications by embodying automatic graph scheduling techniques to optimize interleaved communication and computation. We describe a run-time system called RAPID that
provides a set of library functions for specifying irregular
data objects and tasks that access these objects. The system
extracts a task dependence graph from data access patterns,
and executes tasks efficiently on a distributed memory machine. We discuss a set of optimization strategies used in
this system and demonstrate the application of this system
in parallelizing sparse Cholesky and LU factorizations.
1 Introduction

TRCS 97-02
A New Framework for Image Invariants using Basis Expansion
Yuan-Fang Wang
Department of Computer Science
University of California
Santa Barbara, CA 93106
E-mail: yfwang@cs.ucsb.edu
Web: http://www.cs.ucsb.edu/ yfwang
Abstract
We propose a general framework for computing invariant features from images. The proposed
approach is based on a simple concept of basis expansion. It is widely applicable to many popular
basis representations, such as wavelets [4, 5, 24, 25], short-time Fourier analysis [15, 30], and
splines [2, 6, 33]. Exploiting formulations that use both global and local information about
shape and color, the new approach is neither strictly global nor local. It has the advantage of
tolerating a certain degree of occlusion (unlike global analysis) and does not require estimating
high-order derivatives in computing invariants (unlike local analysis), whence is more robust.
Furthermore, it enables a quasi-localized, hierarchical shape analysis which is not possible with
other known invariant techniques. Unlike most current research on image invariants which
concentrates on either geometry or illumination invariants, the proposed framework is very
general and produces invariants which are insensitive to rigid motion, general affine transform,
changes of parameterization and scene illumination, and perspective transform.
1 Introduction

A SUIF Java Compiler
Holger M. Kienle
kienle@cs.uscb.edu
Technical Report TRCS98-18
August, 1998
+PAGE+

Jade: A High-Level, Machine-Independent Language for Parallel
Programming
Martin C. Rinard, Daniel J. Scales and Monica S. Lam
Computer Systems Laboratory
Stanford University,   CA 94305
1 Introduction

EXPLOITING COMMUTING OPERATIONS IN PARALLELIZING
SERIAL PROGRAMS
PEDRO DINIZ AND MARTIN RINARD
DEPARTMENT OF COMPUTER SCIENCE
UNIVERSITY OF CALIFORNIA, SANTA BARBARA
SANTA BARBARA, CA 93106
Abstract. Two operations commute if the result of their execution is independent of the order in which they execute. Commuting operations can be executed
concurrently provided they execute atomically on the objects they access. Statically
recognizing commuting operations is of great interest because they increase the amount
of concurrency a compiler can exploit. In this document we introduce commutativity
analysis anew technique for automatically parallelizing serial programs. We then
conduct a feasibility study of existing scientific applications as to the existence and
exploitability of commuting operations. We study the commuting operations present
in one such application the Barnes-Hut hierarchical N-body algorithm. We then
parallelize this application using knowledge of commuting operations and present performance results of the parallel code for a shared-memory multiprocessor.
1. Introduction.

A Concurrency Mechanism For Sequential Eiffel
Murat Karaorman John Bruno
Department of Computer Science
University of California
Santa Barbara, CA 93106
Email: murat@cs.ucsb.edu, bruno@cs.ucsb.edu
Abstract
This paper describes a set of classes designed to
facilitate concurrent programming using the sequential object-oriented language EIFFEL. The design and
implementation presented here is the application of a
more general Concurrency Model we have built to introduce concurrency to sequential OOPLs. The model
views concurrency as a well-defined, inheritable property of objects specified in the class CONCURRENCY,
and provides a methodology using inheritance to write
concurrent object-orient applications. Key ideas involved in the methodology are: active objects, extensibility of protocols, synchronization programming,
data-driven synchronization with asynchronous message passing.
The novel feature of our work is in its describing
concurrency in the context of sequential programming
and using a object-oriented design methodology to describe and implement it.
We illustrate the usefulness and expressiveness of
the concurrency mechanism by presenting examples
and analyzing them. The ability to express powerful
synchronization constraints as reusable software components emerges as a strong point of our implementation.
This research has been supported by the Lawrence Liver-more National Labs; grant no: ISCR/LLNL 89-22,90-22
1 INTRODUCTION

Scintilla: Cluster Computing with SCI
Max Ibel, Klaus E. Schauser, Chris J. Scheiman, and Michael Schmitt
Department of Computer Science
University of California, Santa Barbara
Santa Barbara, CA 93106
fibel,schauser,chriss,schmittmg@cs.ucsb.edu
http://www.cs.ucsb.edu/research/scintilla
Abstract
The Scintilla project at UCSB studies SCI-based cluster
computing. The Scalable Coherent Interface (SCI) is a
recent communication standard for cluster interconnects.
We focus on non-coherent SCI, using our cluster setup
of SBus-based and PCI-based workstations connected via
Dolphin SCI adapters. Our motivation for choosing SCI as
network fabric is the very low latency and high bandwidth.
We study how to map a variety of programming models efficiently onto the SCI hardware, focusing on message
passing and global address space support, implementing
Active Messages and Split-C. We present implementation
trade-offs, present performance measurements and compare the PCI and SBus adapters.
We found that the user-level load/store programming interface of SCI is very convenient to use, achieves low latencies, and is fully virtualized, simultaneously supporting
multiple parallel programs and communication channels.
On the other hand, neither of the programming models
studied maps directly to SCI. Issues such as notification,
atomic operations, and virtual address space limitations
represent major implementation challenges, which we address with a combination of compiler and run-time support. Overall, we found the SCI network a good substrate
for high-performance cluster computing.
1 Introduction

Run-time Techniques for Exploiting Irregular Task
Parallelism on Distributed Memory Architectures
Cong Fu and Tao Yang
Department of Computer Science
University of California
Santa Barbara, CA 93106
fcfu,tyangg@cs.ucsb.edu
Abstract
Automatic scheduling for directed acyclic graphs (DAG) and its applications for coarse-grained irregular problems such as large n-body simulation have been studied in the literature.
However solving irregular problems with mixed granularities such as sparse matrix factorization
is challenging since it requires efficient run-time support to execute a DAG schedule. In this
paper, we investigate run-time optimization techniques for executing general asynchronous
DAG schedules on distributed memory machines. Our solution tightly integrates the run-time
scheme with a fast communication mechanism to eliminate unnecessary overhead in message
buffering and copying. We discuss a consistency model incorporating the above optimizations,
and taking advantage of task dependence properties to ensure the correctness of execution.
We demonstrate the applications of this scheme in sparse factorizations and triangular solver
for which actual speedups are hard to obtain. Our experiments on Meiko CS-2 show that the
automatically scheduled code has achieved scalable performance for these problems and the
run-time overhead is small compared to the total execution time.
1 Introduction

UNIVERSITY of CALIFORNIA
Santa Barbara
Design and Implementation of a System to Support Integration of
Autonomous Database Systems
A thesis submitted in partial satisfaction of the
requirements for the degree of
Master of Science
in
Computer Science
by
Tze Kwan Lau
Committee in charge:
Professor Jianwen Su, Chair
Professor Oscar Ibarra
Professor Amr El Abbadi
December 1998
+PAGE+

The Institution of order-sorted equational logic
Grigore Ro~su
University of Bucharest, Faculty of Mathematics,
Department of Computer Science
Str. Academiei 14, R70109, ROMANIA
Abstract
The paper provides an organisation of order-sorted equational logic as
an Institution.
1 Introduction

Appears in Journal of the ACM, Vol. 39, No. 1, January 1992, pp. 214-233. Prelimanary version
in Proceedings of the 20th Annual Symposium on Theory of Computing, ACM, 1988.
How to Sign Given Any Trapdoor Permutation
Mihir Bellare Silvio Micali
January 1992
Abstract
We present a digital signature scheme which is based on the existence of any trapdoor
permutation. Our scheme is secure in the strongest possible natural sense: namely, it is secure
against existential forgery under adaptive chosen message attack.
Department of Computer Science & Engineering,   Mail Code 0114,   University of California at San Diego,   9500
Gilman Drive, La Jolla, CA 92093.   E-mail: mihir@cs.ucsd.edu.   Work done while author was at MIT, supported in
part by NSF grant CCR-87-19689.
MIT Laboratory for Computer Science,   545 Technology Square, Cambridge, MA 02139.   Supported in part by
NSF grant DCR-84-13577 and ARO grant DAALO3-86-K-0171.
+PAGE+

The Relative Complexity of NP Search Problems
Paul Beame
Computer Science and Engineering
University of Washington
beame@cs.washington.edu
Stephen Cook
Computer Science Dept.
University of Toronto
sacook@cs.toronto.edu
Jeff Edmonds
I.C.S.I.
Berkeley, CA 94704-1198
edmonds@icsi.berkeley.edu
Russell Impagliazzo x
Computer Science and Engineering
UC, San Diego
9500 Gilman Drive
La Jolla, CA 92093-0114
russell@cs.ucsd.edu
Toniann Pitassi -
Mathematics and Computer Science
University of Pittsburgh
toni@cs.pitt.edu
Abstract
Papadimitriou introduced several classes of NP search problems based on combinatorial principles which guarantee the
existence of solutions to the problems. Many interesting
search problems not known to be solvable in polynomial
time are contained in these classes, and a number of them
are complete problems. We consider the question of the relative complexity of these search problem classes. We prove
several separations which show that in a generic relativized
world, the search classes are distinct and there is a standard
search problem in each of them that is not computation-ally equivalent to any decision problem. (Naturally, absolute separations would imply that P 6= NP.) Our separation
proofs have interesting combinatorial content and go to the
heart of the combinatorial principles on which the classes are
based. We derive one result via new lower bounds on the
degrees of polynomials asserted to exist by Hilbert's Null-stellensatz over finite fields.
Research supported by NSF grants CCR-8858799 and CCR-9303017
Research supported by an NSERC operating grant and the Information Technology Research Centre
Supported by an NSF postdoctoral fellowship and by a Canadian
NSERC postdoctoral fellowship
x Research Supported by NSF YI Award CCR-92-570979, Sloan
Research Fellowship BR-3311, grant #93025 of the joint US-Czechoslovak Science and Technology Program, and USA-Israel BSF
Grant 92-00043
Research supported by an NSF postdoctoral fellowship and by
NSF Grant CCR-9457782
1 Introduction

TR - UCSD - CS96-484
Mapping Parallel Applications to
Distributed Heterogeneous Systems
Silvia M. Figueira 1 and Francine Berman 2
Department of Computer Science and Engineering
University of California, San Diego
-silvia,berman-@cs.ucsd.edu
Abstract
Fast networks have made it possible to coordinate distributed heterogeneous CPU,
memory and storage resources to provide a powerful platform for executing high-performance applications. However, the performance of parallel applications on such
systems is highly dependent on the mapping of application tasks to machines. In this
paper, we propose a mapping strategy for applications formed by multiple tasks targeted
to heterogeneous platforms. We first define a mapping model, the match-tree, which
reects the data movement and conversion costs of distributed algorithms and allows for
alternative implementations of individual tasks on different machines. We then define the
find-mapping and split-partition algorithms, based on the match-tree model, to
determine the best allocation of tasks to resources in heterogeneous systems. We
illustrate the use of these algorithms with a sample distributed application.
1 Introduction

Almost All Regular Graphs are
Hamiltonian
R. W. Robinson
Computer Science Department
University of Georgia
Athens, GA 30602, U.S.A.
N. C. Wormald
Department of Mathematics
University of Melbourne
Parkville, VIC 3052, Australia
Abstract
In a previous paper the authors showed that almost all labelled
cubic graphs are hamiltonian. In the present paper, this result is
used to show that almost all r-regular graphs are hamiltonian for any
fixed r 3, by an analysis of the distribution of 1-factors in random
regular graphs. Moreover, almost all such graphs are r-edge-colourable
if they have an even number of vertices. Similarly, almost all r-regular
bipartite graphs are hamiltonian and r-edge-colourable for fixed r 3.
Research supported by the Australian Research Council
+PAGE+

Learning to Schedule Straight-Line Code
J. Eliot B. Moss
Dept. of Comp. Sci.
Univ. of Mass.
Amherst, MA 01003
Paul E. Utgoff
Dept. of Comp. Sci.
Univ. of Mass.
Amherst, MA 01003
John Cavazos
Dept. of Comp. Sci.
Univ. of Mass.
Amherst, MA 01003
Doina Precup
Dept. of Comp. Sci.
Univ. of Mass.
Amherst, MA 01003
Darko Stefanovi c
Dept. of Comp. Sci.
Univ. of Mass.
Amherst, MA 01003
Carla Brodley
Sch. of Elec. and Comp. Eng.
Purdue University
W. Lafayette, IN 47907
David Scheeff
Sch. of Elec. and Comp. Eng.
Purdue University
W. Lafayette, IN 47907
Abstract
Execution speed of programs on modern computer architectures is sensitive, by a factor of two or more, to the order in which instructions
are presented to the processor. To realize potential execution efficiency,
it is now customary for an optimizing compiler to employ a heuristic
algorithm for instruction scheduling. These algorithms are painstakingly
hand-crafted, which is expenseive and time-consuming. We show how
to cast the instruction scheduling problem as a learning task, so that one
obtains the heuristic scheduling algorithm automatically. Our focus is the
narrower problem of scheduling straight-line code, also known as a basic
block of instructions. Our empirical results show that just a few features
are adequate for quite good performance at this task for a real modern
processor, and that any of several supervised learning methods perform
nearly optimally with respect to the features used.
Category: Applications (compiler optimization)
Original: This work has not been submitted elsewhere.
Presentation: We prefer oral presentation.
Contact author: Eliot Moss
+PAGE+

Multi-time Models for Temporally Abstract
Planning
Doina Precup, Richard S. Sutton
University of Massachusetts
Amherst, MA 01003
fdprecupjrichg@cs.umass.edu
Abstract
Planning and learning at multiple levels of temporal abstraction is a key
problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov
decision processes and reinforcement learning. Current model-based reinforcement learning is based on one-step models that cannot represent
common-sense higher-level actions, such as going to lunch, grasping an
object, or flying to Denver. This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction
setting to include actions, control, and planning. We introduce a more
general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship
to the Bellman equations. This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a
gridworld planning task.
The need for hierarchical

Intra-Option Learning about Temporally Abstract Actions
Richard S. Sutton
Department of Computer Science
University of Massachusetts
Amherst, MA 01003-4610
rich@cs.umass.edu
Doina Precup
Department of Computer Science
University of Massachusetts
Amherst, MA 01003-4610
dprecup@cs.umass.edu
Satinder Singh
Department of Computer Science
University of Colorado
Boulder, CO 80309-0430
baveja@cs.colorado.edu
Abstract
Several researchers have proposed modeling
temporally abstract actions in reinforcement
learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of
options can be learned using methods designed
for semi-Markov decision processes (SMDPs).
However, all these methods require an option to
be executed to termination. In this paper we explore methods that learn about an option from
small fragments of experience consistent with
that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience
within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously
about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the
consequences of options. We present computational examples in which these new methods
learn much faster than SMDP methods and learn
effectively when SMDP methods cannot learn at
all. We also sketch a convergence proof for intra
option value learning.
1 Introduction

A Method Providing Identity Privacy to Mobile Users during
Authentication
Didier Samfat, Refik Molva
Institut Eurecom
2229 Route des Cr^etes
BP 193 - Sophia Antipolis FRANCE
fsamfat, molvag@eurecom.fr
Abstract
The increasing development of mobile networks
raises new security requirements and concerns. In addition to the basic need of authentication, confidentiality and key distribution services, a new problem involving privacy is the unauthorized tracking of users'
migration. In other words, accessing any information
related to the mobile user's location data without his
consent, is a serious violation of his privacy. Moreover, if no care is taken, the disclosure of the mobile
user real identity may appear during the authentication process. The basic solution to this problem is the
use of aliases which insure non-traceability by hiding
the user's real identity and also his relationship with
domain authorities. In this paper we provide a classification of the different degrees of non-traceability and
present a new efficient method for the computation of
aliases. This technique can be used during authentication of mobile users and thus avoids the drawbacks of
existing solutions such as GSM and CDPD.
1 Introduction

Composite Model Checking with Type Specific
Symbolic Encodings
Tevfik Bultan Richard Gerber
Department of Computer Science
University of Maryland,   College Park, MD 20742, USA
Abstract
We present a new symbolic model checking technique, which analyzes temporal properties in multi-typed transition systems. Specifically, the method uses multiple type-specific data encodings to represent
system states, and it carries out fixpoint computations via the corresponding type-specific symbolic
operations. In essence, different symbolic encodings are unified into one composite model checker. Any
type-specific language can be included in this framework provided that the language is closed under
Boolean connectives, propositions can be checked for satisfiability, and relational images can be computed.
Our technique relies on conjunctive partitioning of transition relations of atomic events based on variable
types involved, which allows independent computation of one-step pre- and post-conditions for each
variable type.
In this paper we demonstrate the effectiveness of our method on a nontrivial data-transfer protocol,
which contains a mixture of integer and Boolean-valued variables. The protocol operates over an unreliable channel that can lose, duplicate or reorder messages. Moreover, the protocol's send and receive
window sizes are not specified in advance; rather, they are represented as symbolic constants. The resulting system was automatically verified using our composite model checking approach, in concert with
a conservative approximation technique.
This research is supported in part by ONR grant N00014-94-10228 and NSF CCR-9619808.
+PAGE+

Mobile-End Transport Protocol:
An Alternative to TCP/IP Over Wireless Links
Kuang-Yeh Wang
Department of Computer Science
University of Maryland
College Park, MD 20742
kwang@cs.umd.edu
Satish K. Tripathi
Bourns College of Engineering
University of California
Riverside, CA 92521-0425
tripathi@engr.ucr.edu
Abstract
Unlike wired links, wireless network bandwidth is highly limited
and the channel usually suffers from frequent and bursty loss due
to its vulnerability to various kinds of interference. At the same
time, the traditional TCP with its congestion control is well known
for its poor performance over wireless links. This paper proposes
a new protocol that (1) replaces TCP/IP over the wireless link by
a simpler protocol with smaller headers, if the link is the last hop
along a data path, (2) shifts functions needed to communicate with
an Internet host using TCP/IP from the mobile host to the base
station, so that the distinct wireless link is hidden from the outside Internet, and (3) exploits link-layer acknowledgments and re-transmissions to quickly recover losses over the wireless link. Our
simulation results show a substantial performance improvement
achieved by the new protocol.
1 Introduction

Ratio Rules:
A New Paradigm for Fast, Quantifiable Data Mining
Flip Korn, Alexandros Labrinidis, Yannis Kotidis
Department of Computer Science
University of Maryland
College Park, MD 20742
fflip,labrinid,kotidisg@cs.umd.edu
Christos Faloutsos
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213
christos@cs.cmu.edu
Abstract
Association Rule Mining algorithms operate
on a data matrix (e.g., customers fi products)
to derive association rules [2, 23]. We propose a new paradigm, namely, Ratio Rules,
which are quantifiable in that we can measure
the "goodness" of a set of discovered rules.
We propose to use the "guessing error" as a
measure of the "goodness", that is, the root-mean-square error of the reconstructed values
of the cells of the given matrix, when we pretend that they are unknown. Another contribution is a novel method to guess missing/hidden values from the Ratio Rules that
our method derives. For example, if somebody bought $10 of milk and $3 of bread, our
rules can "guess" the amount spent on, say,
butter. Thus, we can perform a variety of important tasks such as forecasting, answering
"what-if" scenarios, detecting outliers, and visualizing the data. Moreover, we show how to
compute Ratio Rules in a single pass over the
dataset with small memory requirements (a
few small matrices), in contrast to traditional
association rule mining methods that require
multiple passes and/or large memory. Experiments
Work performed while at the University of Maryland. This
research was partially funded by the Institute for Systems Research (ISR), and by the National Science Foundation under
Grants No. EEC-94-02384, IRI-9205273 and IRI-9625428.
Permission to copy without fee all or part of this material is
granted provided that the copies are not made or distributed for
direct commercial advantage, the VLDB copyright notice and
the title of the publication and its date appear, and notice is
given that copying is by permission of the Very Large Data Base
Endowment. To copy otherwise, or to republish, requires a fee
and/or special permission from the Endowment.
Proceedings of the 24th VLDB Conference
New York, USA, 1998
on several real datasets (e.g., basketball and baseball statistics, biological data)
demonstrate that the proposed method consistently achieves a "guessing error" of up to
5 times less than the straightforward competi
tor.
1 Introduction

To appear in Proc. AAAI'97
Dynamic Abstraction Planning
Robert P. Goldman, David J. Musliner, Kurt D. Krebsbach, Mark S. Boddy
Automated Reasoning Group
Honeywell Technology Center
3660 Technology Drive
Minneapolis, MN 55418
fgoldman,musliner,krebsbac,boddyg@htc.honeywell.com
Abstract
This paper describes Dynamic Abstraction Planning
(DAP), an abstraction planning technique that improves the efficiency of state-enumeration planners for
real-time embedded systems such as CIRCA. Abstraction is used to remove detail from the state representation, reducing both the size of the state space that
must be explored to produce a plan and the size of the
resulting plan. The intuition behind this approach is
simple: in some situations, certain world features are
important, while in other situations those same features are not important.
By automatically selecting the appropriate level of abstraction at each step during the planning process,
DAP can significantly reduce the size of the search
space. Furthermore, the planning process can supply
initial plans that preserve safety but might, on further
refinement, do a better job of goal achievement. DAP
can also terminate with an executable abstract plan,
which may be much smaller than the corresponding
plan expanded to precisely-defined states. Preliminary
results show dramatic improvements in planning speed
and scalability.
Introduction

Appears in Working Notes of the NASA Workshop on Planning and Scheduling for Space
Oxnard, CA, October 1997
CIRCA and the Cassini Saturn Orbit Insertion:
Solving a Prepositioning Problem
David J. Musliner and Robert P. Goldman
Automated Reasoning Group
Honeywell Technology Center
3660 Technology Drive
Minneapolis, MN 55418
fmusliner,goldmang@htc.honeywell.com
Introduction
